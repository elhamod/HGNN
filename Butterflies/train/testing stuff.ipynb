{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=1\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only way to do it is to launch jupyter this way:\n",
    "\n",
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath = \"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = \"CIFAR_phylogeny_tripletloss_new_archi_full\"\n",
    "i=0\n",
    "device = 0\n",
    "experiment_params = {\"image_path\": \"cifar-100-python\", \"suffix\": None, \"img_res\": 32, \"augmented\": True, \"batchSize\": 64, \"learning_rate\": 0.01, \"numOfTrials\": 2, \"fc_layers\": 1, \"pretrained\": True, \"epochs\": 30, \"patience\": 12, \"optimizer\": \"adabelief\", \"scheduler\": \"cosine\", \"weightdecay\": 0.0005, \"modelType\": \"BB\", \"lambda\": 10, \"tl_model\": \"preResNet\", \"link_layer\": \"avgpool\", \"adaptive_smoothing\": True, \"adaptive_lambda\": 0.1, \"adaptive_alpha\": 0.1, \"tripletEnabled\": True, \"tripletSamples\": 3, \"tripletSelector\": \"semihard\", \"tripletMargin\": 0.3, \"phylogeny_loss\": False, \"displayName\": \"CIFARpretrained4-Triplet-Cos\", \"noSpeciesBackprop\": False, \"phylogeny_loss_epsilon\": 0.03}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# experiment_params = {\"image_path\": \"cifar-100-python\", \"suffix\": None, \"img_res\": 32, \"augmented\": True, \"batchSize\": 64, \"learning_rate\": 0.001, \"numOfTrials\": 1, \"fc_layers\": 1, \"pretrained\": True, \"epochs\": 30, \"patience\": 12, \"optimizer\": \"adabelief\", \"scheduler\": \"plateau\", \"weightdecay\": 0.0005, \"modelType\": \"BB\", \"lambda\": 10, \"tl_model\": \"preResNet\", \"link_layer\": \"avgpool\", \"adaptive_smoothing\": True, \"adaptive_lambda\": 0.1, \"adaptive_alpha\": 0.1, \"tripletEnabled\": False, \"tripletSamples\": 3, \"tripletSelector\": \"semihard\", \"tripletMargin\": 0.2, \"phylogeny_loss\": False, \"displayName\": \"CIFARpretrained4-BB\", \"noSpeciesBackprop\": False, \"phylogeny_loss_epsilon\": 0.03}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train import CNN\n",
    "from HGNN.train.configParser import getModelName\n",
    "from configParser import ConfigParser, getModelName\n",
    "import os\n",
    "import torch\n",
    "from myhelpers.resnet_cifar2 import cifar100\n",
    "import numpy as np\n",
    "from myhelpers.preresnet_cifar import resnet as preresnet_cifar\n",
    "import random\n",
    "# def seed_everything(seed):\n",
    "#     random.seed(seed)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "# seed_everything(121)\n",
    "\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "experiment_params = config_parser.fixExperimentParams(experiment_params)\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "\n",
    "train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "architecture = {\n",
    "    \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "    \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "}\n",
    "\n",
    "# from myhelpers.resnet_cifar import cifar_resnet56\n",
    "\n",
    "# # modelName = getModelName(experiment_params, i)\n",
    "# print(modelName)\n",
    "# modelName = \"models/b07640aa2d0c1aacb31f7bcda75ac772ad8bf53db3dacc06d2280a98\"\n",
    "# trialName = os.path.join(experimentPathAndName, modelName)\n",
    "# # initModelPath = CNN.getInitModelFile(experimentPathAndName)\n",
    "# finalModelPath = CNN.getModelFile(trialName)\n",
    "# model.load_state_dict(torch.load(finalModelPath))\n",
    "\n",
    "# model = cifar_resnet56(pretrained='cifar100')\n",
    "model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "# model = cifar100(128, pretrained=True)\n",
    "# model = preresnet_cifar(dataset='cifar100', inpt_size=32, pretrained=True)\n",
    "# CNN.trainModel(train_loader, validation_loader, experiment_params, model, \"hello\", test_loader, device=device)\n",
    "# print(model)\n",
    "\n",
    "predlist, lbllist = CNN.getLoaderPredictionProbabilities(test_loader, model, experiment_params, device=device)\n",
    "predlist, lbllist = CNN.getPredictions(predlist, lbllist)\n",
    "if device is not None:\n",
    "    predlist = predlist.cpu()\n",
    "    lbllist = lbllist.cpu()   \n",
    "print(f1_score(lbllist, predlist, average='macro'))\n",
    "print(f1_score(lbllist, predlist, average='micro'))\n",
    "print(accuracy_score(lbllist, predlist))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     rand_input = torch.rand(2, 3, model.network_fine.img_res, model.network_fine.img_res)\n",
    "#     if model.network_fine.device is not None:\n",
    "#         rand_input = rand_input.cuda()\n",
    "#     out_ = model.network_fine(rand_input)\n",
    "\n",
    "model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "\n",
    "predlist, lbllist = CNN.getLoaderPredictionProbabilities(test_loader, model, experiment_params, device=device)\n",
    "predlist, lbllist = CNN.getPredictions(predlist, lbllist)\n",
    "if device is not None:\n",
    "    predlist = predlist.cpu()\n",
    "    lbllist = lbllist.cpu()   \n",
    "print(f1_score(lbllist, predlist, average='macro'))\n",
    "print(f1_score(lbllist, predlist, average='micro'))\n",
    "print(accuracy_score(lbllist, predlist))\n",
    "\n",
    "\n",
    "model = model.network_fine\n",
    "\n",
    "predlist, lbllist = CNN.getLoaderPredictionProbabilities(test_loader, model, experiment_params, device=device)\n",
    "predlist, lbllist = CNN.getPredictions(predlist, lbllist)\n",
    "if device is not None:\n",
    "    predlist = predlist.cpu()\n",
    "    lbllist = lbllist.cpu()   \n",
    "print(f1_score(lbllist, predlist, average='macro'))\n",
    "print(f1_score(lbllist, predlist, average='micro'))\n",
    "print(accuracy_score(lbllist, predlist))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = next(iter(train_loader))\n",
    "plt.imshow(np.transpose(images['image'][0].cpu().detach().numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "learning_rate = 0.00001\n",
    "scheduler_gamma = 0.1\n",
    "scheduler_patience = 4\n",
    "epochs = 40\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_patience, eta_min=learning_rate*scheduler_gamma)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(scheduler.get_last_lr())\n",
    "    scheduler.step() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "adaptive_alpha=0.5\n",
    "adaptive_lambda=0.1\n",
    "fine_loss=[torch.tensor(25)]\n",
    "other_losses ={\n",
    "    'layer2': torch.tensor(2),\n",
    "    'layer4': torch.tensor(2)\n",
    "}\n",
    "\n",
    "from myhelpers.adaptive_smoothing import get_lambdas\n",
    "\n",
    "for fine_loss_ in fine_loss:\n",
    "    print(get_lambdas(adaptive_alpha, adaptive_lambda, fine_loss_, other_losses =other_losses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "model = models.resnet18()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from HGNN.train import CNN\n",
    "\n",
    "experiment_params = {\"image_path\": \"cifar-100-python\", \n",
    "                     \"suffix\": None, \n",
    "                     \"img_res\": 32,\n",
    "                     \"augmented\": True, \n",
    "                     \"batchSize\": 64,\n",
    "                     \"learning_rate\": 0.01,\n",
    "                     \"numOfTrials\": 2,\n",
    "                     \"fc_layers\": 1, \"pretrained\": True, \"epochs\": 30, \"patience\": 12,\n",
    "                     \"optimizer\": \"adabelief\", \"scheduler\": \"cosine\", \"weightdecay\": 0.0005,\n",
    "                     \"modelType\": \"BB\", \"lambda\": 10, \"tl_model\": \"ResNet18\", \"link_layer\": \"avgpool\", \"adaptive_smoothing\": True, \"adaptive_lambda\": 0.1, \"adaptive_alpha\": 0.1, \"tripletEnabled\": True, \"tripletSamples\": 3, \"tripletSelector\": \"semihard\", \"tripletMargin\": 0.3, \"phylogeny_loss\": False, \"displayName\": \"CIFARpretrained4-Triplet-Cos\", \"noSpeciesBackprop\": False, \"phylogeny_loss_epsilon\": 0.03}\n",
    "\n",
    "architecture = {\n",
    "    \"fine\": 50,\n",
    "    \"coarse\" : None\n",
    "}\n",
    "\n",
    "\n",
    "model = CNN.create_model(architecture, experiment_params)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "embeddings = torch.tensor([[4., 3., 0.],[4., 5., 6.]])\n",
    "F.normalize(embeddings, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.tensor([[4.],[4.]])\n",
    "(embeddings-4).eq(torch.zeros_like(embeddings)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import choice\n",
    "\n",
    "a = torch.tensor([[1],[2]])\n",
    "\n",
    "print(torch.FloatTensor(list(map(lambda x : choice([i for i in range(0,3) if i not in [x]]), a))).view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxonomy import Taxonomy\n",
    "from HGNN.train.CSV_processor import CSV_processor\n",
    "\n",
    "fine_csv_scientificName_header = \"scientificName\"\n",
    "fileNameAndPath = \"/raid/elhamod/Fish/Curated4/Easy_30/cleaned_metadata.tre\"\n",
    "data_root = \"/raid/elhamod/Fish/Curated4/Easy_30/\"\n",
    "suffix=\"\"\n",
    "\n",
    "processor = CSV_processor(data_root, suffix, verbose=True)\n",
    "\n",
    "df_nodupes = processor.fine_csv[fine_csv_scientificName_header].drop_duplicates() # Will probably need more processing to deal with small letter...etc\n",
    "node_ids = df_nodupes.tolist()\n",
    "\n",
    "t = Taxonomy(node_ids, fileNameAndPath, verbose=True)\n",
    "\n",
    "# t.get_total_distance()\n",
    "species = node_ids[33]\n",
    "print('species',species)\n",
    "t.get_siblings_by_name(species, 0.5, get_ottids = False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to render the tree beautifully\n",
    "!pip install pyqt5\n",
    "# set flag for rendering\n",
    "import os\n",
    "os.environ['QT_QPA_PLATFORM']='offscreen'\n",
    "\n",
    "import ete3\n",
    "ts = ete3.TreeStyle()\n",
    "ts.show_branch_length=True\n",
    "\n",
    "t.tree.render('%%inline', tree_style=ts)\n",
    "# t.tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "\n",
    "def add(a):\n",
    "    a['1'] = 2\n",
    "    \n",
    "add(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [0,1,2]\n",
    "\n",
    "s[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "sisters_indices = [[1, 2],[3,1,2]]\n",
    "mlb = MultiLabelBinarizer(range(5))\n",
    "mlb.fit_transform(sisters_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "input = torch.randn((2,3), requires_grad=True)\n",
    "target = torch.tensor([[1.,0.,1.],[0.,1.,0.]])\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HGNN.train import CNN\n",
    "\n",
    "architecture\n",
    "model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "CNN.loadModel(model, savedModelName, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load('/raid/elhamod/Fish/experiments/Fish30-5run-testPhyloNN/models/9bad7cb5922c83f40dc03a781629256c776a0ad5b44039b9099f47d8/finalModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.network[4][1].conv2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phyloDistances=[0.75, 0.5, 0.25]\n",
    "phyloDistances.insert(0, 1)\n",
    "loss_name='1distance'\n",
    "distance_indx = [idx for idx, element in enumerate(phyloDistances) if loss_name == str(element).replace(\".\", \"\")+\"distance\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = phyloDistances[distance_indx]\n",
    "next_distance = phyloDistances[distance_indx+1] if distance_indx<len(phyloDistances)-1 else 0\n",
    "abs_total_dist = 512\n",
    "print('distance_indx', distance_indx, int(next_distance*abs_total_dist), int(distance*abs_total_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phyloDistances[0]*abs_total_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permuted\n",
      "torch.Size([5, 15, 3, 3])\n",
      "torch.Size([15, 5, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def deconv_orth_dist(kernel1, kernel2, stride = 2, padding = 1):\n",
    "    assert (kernel1.shape[0] == kernel2.shape[0]) or (kernel1.shape[1] == kernel2.shape[1]) , \"Kernels should be of compatible sizes\" + str(kernel1.shape) + \", \" + str(kernel2.shape)\n",
    "    if kernel1.shape[1] != kernel2.shape[1]:\n",
    "        kernel1 = kernel1.permute(1, 0, 2, 3)\n",
    "        kernel2 = kernel2.permute(1, 0, 2, 3)\n",
    "        print('permuted')\n",
    "    [o_c, i_c, w, h] = kernel1.shape\n",
    "    output = torch.conv2d(kernel1, kernel2, stride=stride, padding=padding)\n",
    "    print(kernel1.shape)\n",
    "#     print(target)\n",
    "#     print(output)\n",
    "    return torch.norm( output )\n",
    "\n",
    "kernel = torch.rand((15,5,3,3)).cuda()\n",
    "kernel2 = torch.rand((15,15,3,3)).cuda()\n",
    "deconv_orth_dist(kernel,kernel2,1, 1)\n",
    "print(kernel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f076184a2dfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
