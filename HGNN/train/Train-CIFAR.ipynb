{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath = \"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = \"CIFAR_phylogeny_tripletloss_SGDTest_staticlargelambda_updateArchi\"\n",
    "device = 1\n",
    "detailed_reporting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "experiment:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 1\n",
      "{'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': False, 'batchSize': 128, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 10, 'tl_model': 'CIFAR', 'link_layer': 'avgpool', 'adaptive_smoothing': False, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.001, 'tripletEnabled': True, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'CIFAR large lambda static updated architecture', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8b568b538f4f17b3ef14191f63e290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_phylogeny_tripletloss_SGDTest_staticlargelambda_updateArchi', 'modelName': 'models/a9ee2e3cdf53aa802cd1b304abfae30309014d126cf3ec35118d2f50', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '3793d1aac98cb6961773428741793544abfa53544522635b53889f30', 'trialHash': 'a9ee2e3cdf53aa802cd1b304abfae30309014d126cf3ec35118d2f50', 'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': False, 'batchSize': 128, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 10, 'tl_model': 'CIFAR', 'link_layer': 'avgpool', 'adaptive_smoothing': False, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.001, 'tripletEnabled': True, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'CIFAR large lambda static updated architecture', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">a9ee2e3cdf53aa802cd1b304abfae30309014d126cf3ec35118d2f50</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/7n4uiyvd\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/7n4uiyvd</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210510_190901-7n4uiyvd</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:28<?, ?it/s, min_val_loss=inf, train=0.959, val=0.962, val_loss=3.68]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [00:28<3:57:42, 28.58s/it, min_val_loss=inf, train=0.959, val=0.962, val_loss=3.68]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [02:53<3:57:42, 28.58s/it, min_val_loss=inf, train=0.072, val=0.0693, val_loss=4.58]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [02:53<8:46:30, 63.44s/it, min_val_loss=inf, train=0.072, val=0.0693, val_loss=4.58]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [05:45<8:46:30, 63.44s/it, min_val_loss=14.4, train=0.109, val=0.1, val_loss=4.56]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [05:45<13:15:28, 96.03s/it, min_val_loss=14.4, train=0.109, val=0.1, val_loss=4.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [10:31<13:15:28, 96.03s/it, min_val_loss=9.96, train=0.187, val=0.17, val_loss=4.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [10:31<21:04:26, 152.96s/it, min_val_loss=9.96, train=0.187, val=0.17, val_loss=4.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [15:20<21:04:26, 152.96s/it, min_val_loss=5.88, train=0.227, val=0.211, val_loss=4.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [15:20<26:39:12, 193.84s/it, min_val_loss=5.88, train=0.227, val=0.211, val_loss=4.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [20:05<26:39:12, 193.84s/it, min_val_loss=4.73, train=0.302, val=0.268, val_loss=4.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [20:05<30:21:55, 221.29s/it, min_val_loss=4.73, train=0.302, val=0.268, val_loss=4.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [24:53<30:21:55, 221.29s/it, min_val_loss=3.74, train=0.233, val=0.213, val_loss=4.5] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [24:53<33:00:56, 241.09s/it, min_val_loss=3.74, train=0.233, val=0.213, val_loss=4.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [29:39<33:00:56, 241.09s/it, min_val_loss=3.74, train=0.349, val=0.295, val_loss=4.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [29:39<34:47:57, 254.63s/it, min_val_loss=3.74, train=0.349, val=0.295, val_loss=4.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [34:26<34:47:57, 254.63s/it, min_val_loss=3.39, train=0.329, val=0.279, val_loss=4.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [34:26<36:04:00, 264.44s/it, min_val_loss=3.39, train=0.329, val=0.279, val_loss=4.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [39:15<36:04:00, 264.44s/it, min_val_loss=3.39, train=0.397, val=0.327, val_loss=4.4] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [39:15<36:58:37, 271.67s/it, min_val_loss=3.39, train=0.397, val=0.327, val_loss=4.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [42:06<36:58:37, 271.67s/it, min_val_loss=3.06, train=0.423, val=0.341, val_loss=4.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [42:06<32:49:34, 241.67s/it, min_val_loss=3.06, train=0.423, val=0.341, val_loss=4.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [44:48<32:49:34, 241.67s/it, min_val_loss=2.93, train=0.434, val=0.345, val_loss=4.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [44:48<29:29:49, 217.60s/it, min_val_loss=2.93, train=0.434, val=0.345, val_loss=4.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [47:26<29:29:49, 217.60s/it, min_val_loss=2.9, train=0.458, val=0.353, val_loss=4.37] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [47:26<27:00:39, 199.67s/it, min_val_loss=2.9, train=0.458, val=0.353, val_loss=4.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [50:02<27:00:39, 199.67s/it, min_val_loss=2.83, train=0.48, val=0.362, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [50:02<25:12:48, 186.77s/it, min_val_loss=2.83, train=0.48, val=0.362, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [52:46<25:12:48, 186.77s/it, min_val_loss=2.77, train=0.459, val=0.35, val_loss=4.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [52:46<24:14:39, 179.96s/it, min_val_loss=2.77, train=0.459, val=0.35, val_loss=4.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [55:30<24:14:39, 179.96s/it, min_val_loss=2.77, train=0.489, val=0.364, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [55:30<23:32:21, 175.08s/it, min_val_loss=2.77, train=0.489, val=0.364, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [58:13<23:32:21, 175.08s/it, min_val_loss=2.75, train=0.486, val=0.36, val_loss=4.36] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [58:13<23:00:47, 171.53s/it, min_val_loss=2.75, train=0.486, val=0.36, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [1:00:55<23:00:47, 171.53s/it, min_val_loss=2.75, train=0.507, val=0.365, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [1:00:55<22:34:07, 168.56s/it, min_val_loss=2.75, train=0.507, val=0.365, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [1:03:39<22:34:07, 168.56s/it, min_val_loss=2.74, train=0.509, val=0.367, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [1:03:39<22:19:37, 167.11s/it, min_val_loss=2.74, train=0.509, val=0.367, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [1:07:35<22:19:37, 167.11s/it, min_val_loss=2.72, train=0.517, val=0.368, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [1:07:35<25:04:00, 188.00s/it, min_val_loss=2.72, train=0.517, val=0.368, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [1:12:27<25:04:00, 188.00s/it, min_val_loss=2.72, train=0.518, val=0.367, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [1:12:27<29:09:51, 219.19s/it, min_val_loss=2.72, train=0.518, val=0.367, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [1:16:50<29:09:51, 219.19s/it, min_val_loss=2.72, train=0.528, val=0.369, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:16:50<30:49:27, 232.15s/it, min_val_loss=2.72, train=0.528, val=0.369, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:20:48<30:49:27, 232.15s/it, min_val_loss=2.71, train=0.53, val=0.368, val_loss=4.34] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:20:48<31:00:54, 234.08s/it, min_val_loss=2.71, train=0.53, val=0.368, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:24:59<31:00:54, 234.08s/it, min_val_loss=2.71, train=0.526, val=0.369, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [1:24:59<31:35:58, 238.99s/it, min_val_loss=2.71, train=0.526, val=0.369, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [1:29:13<31:35:58, 238.99s/it, min_val_loss=2.71, train=0.529, val=0.37, val_loss=4.34] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [1:29:13<32:07:10, 243.43s/it, min_val_loss=2.71, train=0.529, val=0.37, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [1:34:26<32:07:10, 243.43s/it, min_val_loss=2.7, train=0.54, val=0.37, val_loss=4.34]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [1:34:26<34:48:19, 264.35s/it, min_val_loss=2.7, train=0.54, val=0.37, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [1:40:35<34:48:19, 264.35s/it, min_val_loss=2.7, train=0.535, val=0.363, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [1:40:35<38:52:23, 295.86s/it, min_val_loss=2.7, train=0.535, val=0.363, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [1:46:16<38:52:23, 295.86s/it, min_val_loss=2.7, train=0.541, val=0.371, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [1:46:16<40:33:58, 309.40s/it, min_val_loss=2.7, train=0.541, val=0.371, val_loss=4.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [1:52:13<40:33:58, 309.40s/it, min_val_loss=2.69, train=0.547, val=0.37, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [1:52:13<42:21:14, 323.72s/it, min_val_loss=2.69, train=0.547, val=0.37, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [1:57:50<42:21:14, 323.72s/it, min_val_loss=2.69, train=0.552, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [1:57:50<42:46:56, 327.69s/it, min_val_loss=2.69, train=0.552, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [2:03:46<42:46:56, 327.69s/it, min_val_loss=2.68, train=0.556, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [2:03:46<43:47:20, 336.12s/it, min_val_loss=2.68, train=0.556, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [2:09:25<43:47:20, 336.12s/it, min_val_loss=2.68, train=0.552, val=0.373, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   6%|▋         | 32/500 [2:09:25<43:47:34, 336.87s/it, min_val_loss=2.68, train=0.552, val=0.373, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [2:15:21<43:47:34, 336.87s/it, min_val_loss=2.68, train=0.553, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [2:15:21<44:26:39, 342.61s/it, min_val_loss=2.68, train=0.553, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [2:21:00<44:26:39, 342.61s/it, min_val_loss=2.67, train=0.553, val=0.375, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [2:21:00<44:12:23, 341.51s/it, min_val_loss=2.67, train=0.553, val=0.375, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [2:26:54<44:12:23, 341.51s/it, min_val_loss=2.66, train=0.557, val=0.376, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 35/500 [2:26:54<44:37:05, 345.43s/it, min_val_loss=2.66, train=0.557, val=0.376, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 35/500 [2:32:40<44:37:05, 345.43s/it, min_val_loss=2.66, train=0.56, val=0.374, val_loss=4.33] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 36/500 [2:32:40<44:32:32, 345.59s/it, min_val_loss=2.66, train=0.56, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 36/500 [2:38:31<44:32:32, 345.59s/it, min_val_loss=2.66, train=0.559, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 37/500 [2:38:31<44:39:42, 347.26s/it, min_val_loss=2.66, train=0.559, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 37/500 [2:44:22<44:39:42, 347.26s/it, min_val_loss=2.66, train=0.556, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 38/500 [2:44:22<44:41:37, 348.26s/it, min_val_loss=2.66, train=0.556, val=0.374, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 38/500 [2:50:06<44:41:37, 348.26s/it, min_val_loss=2.66, train=0.56, val=0.372, val_loss=4.33] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 39/500 [2:50:06<44:25:56, 346.98s/it, min_val_loss=2.66, train=0.56, val=0.372, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 39/500 [2:55:59<44:25:56, 346.98s/it, min_val_loss=2.66, train=0.56, val=0.372, val_loss=4.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 40/500 [2:55:59<44:34:18, 348.82s/it, min_val_loss=2.66, train=0.56, val=0.372, val_loss=4.33]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   8%|▊         | 40/500 [2:56:11<33:46:09, 264.28s/it, min_val_loss=2.66, train=0.56, val=0.372, val_loss=4.33]\n",
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/ipykernel_launcher.py:110: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 61860<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210510_190901-7n4uiyvd/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210510_190901-7n4uiyvd/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>39</td></tr><tr><td>learning rate</td><td>2e-05</td></tr><tr><td>validation_fine_f1</td><td>0.37577</td></tr><tr><td>training_fine_f1</td><td>0.56011</td></tr><tr><td>test_fine_f1</td><td>0.36214</td></tr><tr><td>validation_loss</td><td>4.33169</td></tr><tr><td>_runtime</td><td>10565</td></tr><tr><td>_timestamp</td><td>1620698706</td></tr><tr><td>_step</td><td>12246</td></tr><tr><td>loss</td><td>3.74421</td></tr><tr><td>batch</td><td>12519</td></tr><tr><td>loss_fine</td><td>1.8017</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>loss_layer2</td><td>0.19425</td></tr><tr><td>lambda_layer2</td><td>10</td></tr><tr><td>nonzerotriplets_layer2</td><td>5</td></tr><tr><td>loss_layer3</td><td>0.0</td></tr><tr><td>lambda_layer3</td><td>10</td></tr><tr><td>nonzerotriplets_layer3</td><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>████▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_fine_f1</td><td>█▁▁▂▂▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>training_fine_f1</td><td>█▁▁▂▂▃▂▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>test_fine_f1</td><td>█▁▁▂▃▃▃▄▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>validation_loss</td><td>▁██▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>██▇▅▅▄▅▃▃▃▂▂▃▂▃▃▃▂▂▁▁▂▃▁▂▁▂▂▂▁▁▂▂▂▂▁▂▁▂▂</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▇▇▆▅▄▅▄▃▃▂▂▃▁▃▃▃▂▂▁▂▂▃▁▂▁▂▂▂▁▁▂▁▂▂▁▂▁▂▂</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer2</td><td>▅▆▃▁▂▅▅▇▆▆▄▄▆▅▆▇▇▆▇▆▅▇▆▇▇▆█▆▇▇▆▇▆▇▆▇▅▇▇▅</td></tr><tr><td>lambda_layer2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>█▄▇▄▄▂▃▃▁▂▂▂▃▃▃▃▂▃▄▃▂▂▂▂▂▃▁▄▃▄▂▁▂▂▁▂▃▂▃▁</td></tr><tr><td>loss_layer3</td><td>▆▇▇▄▇▇▆▅▅▁▆▇▅█▆█████▄▇▇█▆▇▇▇█▇▇██▇▇▇▆▆▇▇</td></tr><tr><td>lambda_layer3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer3</td><td>█▄▄▃▃▃▇▄▆▄▅▃▃▂▂▁▂▁▂▂▃▃▃▂▃▂▂▃▁▂▃▃▂▄▁▂▃▃▃▃</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">a9ee2e3cdf53aa802cd1b304abfae30309014d126cf3ec35118d2f50</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/7n4uiyvd\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/7n4uiyvd</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment: 100%|██████████| 1/1 [2:56:22<00:00, 10582.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    print('wandb not found')\n",
    "\n",
    "from myhelpers import config_plots, TrialStatistics\n",
    "from myhelpers.try_warning import try_running\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "config_plots.global_settings()\n",
    "\n",
    "experimetnsFileName = \"experiments.csv\"\n",
    "WANDB_message=\"wandb not working\"\n",
    "\n",
    "\n",
    "# For logging to server\n",
    "try_running(lambda : wandb.login(), WANDB_message)\n",
    "\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "# set cuda\n",
    "if device is not None:\n",
    "    print(\"using cuda\", device)\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# get experiment params\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "\n",
    "# init experiments file\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "\n",
    "paramsIterator = config_parser.getExperiments()  \n",
    "number_of_experiments = sum(1 for e in paramsIterator)  \n",
    "experiment_index = 0\n",
    "\n",
    "# Loop through experiments\n",
    "# with progressbar.ProgressBar(max_value=number_of_experiments) as bar:\n",
    "with tqdm(total=number_of_experiments, desc=\"experiment\") as bar:\n",
    "    for experiment_params in config_parser.getExperiments():\n",
    "        print(experiment_params)\n",
    "        experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "        # load images \n",
    "        if experiment_params['image_path'] == 'cifar-100-python':\n",
    "            datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        else:\n",
    "            datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "        train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "        architecture = {\n",
    "            \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "            \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "        }\n",
    "\n",
    "        # Loop through n trials\n",
    "        for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "            modelName = getModelName(experiment_params, i)\n",
    "            trialName = os.path.join(experimentPathAndName, modelName)\n",
    "            trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "            row_information = {\n",
    "                'experimentName': experimentName,\n",
    "                'modelName': modelName,\n",
    "                'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "                'experimentHash': experimentHash,\n",
    "                'trialHash': trialHash\n",
    "            }\n",
    "            row_information = {**row_information, **experiment_params} \n",
    "            print(row_information)\n",
    "\n",
    "            run = try_running(lambda : wandb.init(project='HGNN', group=experimentName+\"-\"+experimentHash, name=trialHash, config=row_information), WANDB_message) #, reinit=True\n",
    "\n",
    "            # Train/Load model\n",
    "            model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "#             print(model)\n",
    "\n",
    "#             try_running(lambda : wandb.watch(model, log=\"all\"), WANDB_message)\n",
    "\n",
    "            if os.path.exists(CNN.getModelFile(trialName)):\n",
    "                print(\"Model {0} found!\".format(trialName))\n",
    "            else:\n",
    "                initModelPath = CNN.getInitModelFile(experimentPathAndName)\n",
    "                if os.path.exists(initModelPath):\n",
    "                    model.load_state_dict(torch.load(initModelPath))\n",
    "                    print(\"Init Model {0} found!\".format(initModelPath))\n",
    "                CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=detailed_reporting)\n",
    "\n",
    "            # Add to experiments file\n",
    "            if os.path.exists(experimentsFileNameAndPath):\n",
    "                experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "            else:\n",
    "                experiments_df = pd.DataFrame()\n",
    "\n",
    "            record_exists = not (experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName]).empty if not experiments_df.empty else False\n",
    "            if record_exists:\n",
    "                experiments_df.drop(experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName].index, inplace = True) \n",
    "\n",
    "            experiments_df = experiments_df.append(pd.DataFrame(row_information, index=[0]), ignore_index = True)\n",
    "            experiments_df.to_csv(experimentsFileNameAndPath, header=True, index=False)\n",
    "\n",
    "            try_running(lambda : run.finish(), WANDB_message)\n",
    "\n",
    "        bar.update()\n",
    "\n",
    "        experiment_index = experiment_index + 1\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "#     import argparse\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--cuda', required=True, type=int)\n",
    "#     parser.add_argument('--experiments', required=True)\n",
    "#     parser.add_argument('--data', required=True)\n",
    "#     parser.add_argument('--name', required=True)\n",
    "#     parser.add_argument('--detailed', required=False, action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "#     main(experimentName=args.name, experimentsPath=args.experiments, dataPath=args.data, device=args.cuda, detailed_reporting=args.detailed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
