{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath = \"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = \"CIFAR_scheduler_experiments\"\n",
    "device = 1\n",
    "detailed_reporting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "experiment:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 1\n",
      "{'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.0005, 'scheduler_gamma': 0.1, 'scheduler_patience': 7, 'modelType': 'BB', 'lambda': 10, 'tl_model': 'preResNet', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'CIFARpre-s-triplet-hier', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f2f8a55be7472191afb1b735bbf2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_scheduler_experiments', 'modelName': 'models/d0311b2358292081642d48075a250a074d2dfcd843b9856b146c9731', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '75c49f1cee614d69e60ac97422e74f8fc93c5102612552d8e68f154e', 'trialHash': 'd0311b2358292081642d48075a250a074d2dfcd843b9856b146c9731', 'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.0005, 'scheduler_gamma': 0.1, 'scheduler_patience': 7, 'modelType': 'BB', 'lambda': 10, 'tl_model': 'preResNet', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'CIFARpre-s-triplet-hier', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">d0311b2358292081642d48075a250a074d2dfcd843b9856b146c9731</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1pg26equ\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1pg26equ</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210523_181101-1pg26equ</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:27<?, ?it/s, min_val_loss=inf, train=0.931, val=0.93, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [00:27<17:36, 27.09s/it, min_val_loss=inf, train=0.931, val=0.93, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [08:04<17:36, 27.09s/it, min_val_loss=inf, train=0.944, val=0.919, val_loss=3.76]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [08:04<1:38:50, 156.06s/it, min_val_loss=inf, train=0.944, val=0.919, val_loss=3.76]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [15:35<1:38:50, 156.06s/it, min_val_loss=1.09, train=0.927, val=0.88, val_loss=3.8] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [15:35<2:30:52, 244.67s/it, min_val_loss=1.09, train=0.927, val=0.88, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [23:06<2:30:52, 244.67s/it, min_val_loss=1.09, train=0.925, val=0.864, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [23:06<3:03:57, 306.61s/it, min_val_loss=1.09, train=0.925, val=0.864, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [30:38<3:03:57, 306.61s/it, min_val_loss=1.09, train=0.931, val=0.861, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [30:38<3:24:18, 350.26s/it, min_val_loss=1.09, train=0.931, val=0.861, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [38:05<3:24:18, 350.26s/it, min_val_loss=1.09, train=0.934, val=0.851, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [38:05<3:34:57, 379.33s/it, min_val_loss=1.09, train=0.934, val=0.851, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [45:35<3:34:57, 379.33s/it, min_val_loss=1.09, train=0.93, val=0.837, val_loss=3.83] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [45:35<3:40:09, 400.30s/it, min_val_loss=1.09, train=0.93, val=0.837, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [53:04<3:40:09, 400.30s/it, min_val_loss=1.09, train=0.939, val=0.843, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [53:04<3:41:25, 415.16s/it, min_val_loss=1.09, train=0.939, val=0.843, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [1:00:33<3:41:25, 415.16s/it, min_val_loss=1.09, train=0.941, val=0.836, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [1:00:33<3:39:41, 425.21s/it, min_val_loss=1.09, train=0.941, val=0.836, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [1:08:01<3:39:41, 425.21s/it, min_val_loss=1.09, train=0.943, val=0.833, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [1:08:01<3:35:59, 431.99s/it, min_val_loss=1.09, train=0.943, val=0.833, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [1:15:29<3:35:59, 431.99s/it, min_val_loss=1.09, train=0.986, val=0.881, val_loss=3.78]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [1:15:29<3:31:08, 436.86s/it, min_val_loss=1.09, train=0.986, val=0.881, val_loss=3.78]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [1:22:58<3:31:08, 436.86s/it, min_val_loss=1.09, train=0.989, val=0.886, val_loss=3.77]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [1:22:58<3:25:32, 440.43s/it, min_val_loss=1.09, train=0.989, val=0.886, val_loss=3.77]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [1:30:26<3:25:32, 440.43s/it, min_val_loss=1.09, train=0.992, val=0.889, val_loss=3.76]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [1:30:26<3:19:16, 442.85s/it, min_val_loss=1.09, train=0.992, val=0.889, val_loss=3.76]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [1:37:55<3:19:16, 442.85s/it, min_val_loss=1.09, train=0.993, val=0.888, val_loss=3.76]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [1:37:55<3:12:38, 444.56s/it, min_val_loss=1.09, train=0.993, val=0.888, val_loss=3.76]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [1:45:28<3:12:38, 444.56s/it, min_val_loss=1.09, train=0.995, val=0.895, val_loss=3.76]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [1:45:28<3:06:17, 447.09s/it, min_val_loss=1.09, train=0.995, val=0.895, val_loss=3.76]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [1:52:56<3:06:17, 447.09s/it, min_val_loss=1.09, train=0.996, val=0.897, val_loss=3.75]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [1:52:56<2:58:54, 447.27s/it, min_val_loss=1.09, train=0.996, val=0.897, val_loss=3.75]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [2:00:25<2:58:54, 447.27s/it, min_val_loss=1.09, train=0.996, val=0.899, val_loss=3.75]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [2:00:25<2:51:39, 447.79s/it, min_val_loss=1.09, train=0.996, val=0.899, val_loss=3.75]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [2:07:56<2:51:39, 447.79s/it, min_val_loss=1.09, train=0.996, val=0.901, val_loss=3.75]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [2:07:56<2:44:34, 448.85s/it, min_val_loss=1.09, train=0.996, val=0.901, val_loss=3.75]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [2:15:25<2:44:34, 448.85s/it, min_val_loss=1.09, train=0.997, val=0.899, val_loss=3.75]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [2:15:25<2:37:04, 448.78s/it, min_val_loss=1.09, train=0.997, val=0.899, val_loss=3.75]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [2:22:54<2:37:04, 448.78s/it, min_val_loss=1.09, train=0.998, val=0.904, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [2:22:54<2:29:39, 448.98s/it, min_val_loss=1.09, train=0.998, val=0.904, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [2:30:23<2:29:39, 448.98s/it, min_val_loss=1.09, train=0.997, val=0.902, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [2:30:23<2:22:09, 448.93s/it, min_val_loss=1.09, train=0.997, val=0.902, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [2:37:53<2:22:09, 448.93s/it, min_val_loss=1.09, train=0.998, val=0.903, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [2:37:53<2:14:47, 449.28s/it, min_val_loss=1.09, train=0.998, val=0.903, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [2:45:22<2:14:47, 449.28s/it, min_val_loss=1.09, train=0.998, val=0.907, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [2:45:22<2:07:14, 449.09s/it, min_val_loss=1.09, train=0.998, val=0.907, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [2:52:51<2:07:14, 449.09s/it, min_val_loss=1.09, train=0.997, val=0.908, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [2:52:51<1:59:45, 449.07s/it, min_val_loss=1.09, train=0.997, val=0.908, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [3:00:21<1:59:45, 449.07s/it, min_val_loss=1.09, train=0.998, val=0.903, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [3:00:21<1:52:20, 449.40s/it, min_val_loss=1.09, train=0.998, val=0.903, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [3:07:50<1:52:20, 449.40s/it, min_val_loss=1.09, train=0.998, val=0.906, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [3:07:50<1:44:51, 449.37s/it, min_val_loss=1.09, train=0.998, val=0.906, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [3:15:19<1:44:51, 449.37s/it, min_val_loss=1.09, train=0.998, val=0.904, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [3:15:19<1:37:18, 449.10s/it, min_val_loss=1.09, train=0.998, val=0.904, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [3:22:48<1:37:18, 449.10s/it, min_val_loss=1.09, train=0.998, val=0.904, val_loss=3.74]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [3:22:48<1:29:50, 449.18s/it, min_val_loss=1.09, train=0.998, val=0.904, val_loss=3.74]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    print('wandb not found')\n",
    "\n",
    "from myhelpers import config_plots, TrialStatistics\n",
    "from myhelpers.try_warning import try_running\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "config_plots.global_settings()\n",
    "\n",
    "experimetnsFileName = \"experiments.csv\"\n",
    "WANDB_message=\"wandb not working\"\n",
    "\n",
    "\n",
    "# For logging to server\n",
    "try_running(lambda : wandb.login(), WANDB_message)\n",
    "\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "# set cuda\n",
    "if device is not None:\n",
    "    print(\"using cuda\", device)\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# get experiment params\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "\n",
    "# init experiments file\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "\n",
    "paramsIterator = config_parser.getExperiments()  \n",
    "number_of_experiments = sum(1 for e in paramsIterator)  \n",
    "experiment_index = 0\n",
    "\n",
    "# Loop through experiments\n",
    "# with progressbar.ProgressBar(max_value=number_of_experiments) as bar:\n",
    "with tqdm(total=number_of_experiments, desc=\"experiment\") as bar:\n",
    "    for experiment_params in config_parser.getExperiments():\n",
    "        print(experiment_params)\n",
    "        experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "        # load images \n",
    "        if experiment_params['image_path'] == 'cifar-100-python':\n",
    "            datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        else:\n",
    "            datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "        train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "        architecture = {\n",
    "            \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "            \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "        }\n",
    "\n",
    "        # Loop through n trials\n",
    "        for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "            modelName = getModelName(experiment_params, i)\n",
    "            trialName = os.path.join(experimentPathAndName, modelName)\n",
    "            trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "            row_information = {\n",
    "                'experimentName': experimentName,\n",
    "                'modelName': modelName,\n",
    "                'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "                'experimentHash': experimentHash,\n",
    "                'trialHash': trialHash\n",
    "            }\n",
    "            row_information = {**row_information, **experiment_params} \n",
    "            print(row_information)\n",
    "\n",
    "            run = try_running(lambda : wandb.init(project='HGNN', group=experimentName+\"-\"+experimentHash, name=trialHash, config=row_information), WANDB_message) #, reinit=True\n",
    "\n",
    "            # Train/Load model\n",
    "            model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "#             print(model)\n",
    "\n",
    "#             try_running(lambda : wandb.watch(model, log=\"all\"), WANDB_message)\n",
    "\n",
    "            if os.path.exists(CNN.getModelFile(trialName)):\n",
    "                print(\"Model {0} found!\".format(trialName))\n",
    "            else:\n",
    "                initModelPath = CNN.getInitModelFile(experimentPathAndName)\n",
    "                if os.path.exists(initModelPath):\n",
    "                    model.load_state_dict(torch.load(initModelPath))\n",
    "                    print(\"Init Model {0} found!\".format(initModelPath))\n",
    "                CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=detailed_reporting)\n",
    "\n",
    "            # Add to experiments file\n",
    "            if os.path.exists(experimentsFileNameAndPath):\n",
    "                experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "            else:\n",
    "                experiments_df = pd.DataFrame()\n",
    "\n",
    "            record_exists = not (experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName]).empty if not experiments_df.empty else False\n",
    "            if record_exists:\n",
    "                experiments_df.drop(experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName].index, inplace = True) \n",
    "\n",
    "            experiments_df = experiments_df.append(pd.DataFrame(row_information, index=[0]), ignore_index = True)\n",
    "            experiments_df.to_csv(experimentsFileNameAndPath, header=True, index=False)\n",
    "\n",
    "            try_running(lambda : run.finish(), WANDB_message)\n",
    "\n",
    "        bar.update()\n",
    "\n",
    "        experiment_index = experiment_index + 1\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "#     import argparse\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--cuda', required=True, type=int)\n",
    "#     parser.add_argument('--experiments', required=True)\n",
    "#     parser.add_argument('--data', required=True)\n",
    "#     parser.add_argument('--name', required=True)\n",
    "#     parser.add_argument('--detailed', required=False, action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "#     main(experimentName=args.name, experimentsPath=args.experiments, dataPath=args.data, device=args.cuda, detailed_reporting=args.detailed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
