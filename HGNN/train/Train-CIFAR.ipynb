{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath = \"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = \"CIFAR_phylogeny_from_scratch_2\"\n",
    "device = 1\n",
    "detailed_reporting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "experiment:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 1\n",
      "{'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.1, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': False, 'epochs': 200, 'patience': -1, 'modelType': 'BB', 'lambda': 10, 'tl_model': 'preResNet', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'CIFAR preresnet from scratch no triplet', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb0f060b8f041a1b01993503b3430ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_phylogeny_from_scratch_2', 'modelName': 'models/c872f5b8a4fd28ecbcdea6165e24d5e2db74486ccf13af62f314d81f', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '3599eece106c56ef2f5bb246d59ac03857454f60ff22d9529d143466', 'trialHash': 'c872f5b8a4fd28ecbcdea6165e24d5e2db74486ccf13af62f314d81f', 'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.1, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': False, 'epochs': 200, 'patience': -1, 'modelType': 'BB', 'lambda': 10, 'tl_model': 'preResNet', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'CIFAR preresnet from scratch no triplet', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">c872f5b8a4fd28ecbcdea6165e24d5e2db74486ccf13af62f314d81f</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1h5ucc5p\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1h5ucc5p</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210512_201923-1h5ucc5p</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 is not in ['conv1', 'layer1', 'layer2', 'layer3', 'bn', 'relu', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "\n",
      "\n",
      "iteration:   0%|          | 0/200 [00:47<?, ?it/s, min_val_loss=inf, train=0.00133, val=0.00122, val_loss=4.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/200 [00:47<2:36:17, 47.12s/it, min_val_loss=inf, train=0.00133, val=0.00122, val_loss=4.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/200 [02:50<2:36:17, 47.12s/it, min_val_loss=inf, train=0.0857, val=0.0795, val_loss=4.56]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 2/200 [02:50<3:51:01, 70.01s/it, min_val_loss=inf, train=0.0857, val=0.0795, val_loss=4.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 2/200 [04:55<3:51:01, 70.01s/it, min_val_loss=12.6, train=0.149, val=0.148, val_loss=4.53] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 3/200 [04:55<4:43:51, 86.45s/it, min_val_loss=12.6, train=0.149, val=0.148, val_loss=4.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 3/200 [07:00<4:43:51, 86.45s/it, min_val_loss=6.77, train=0.22, val=0.209, val_loss=4.48] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 4/200 [07:00<5:20:20, 98.07s/it, min_val_loss=6.77, train=0.22, val=0.209, val_loss=4.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 4/200 [09:07<5:20:20, 98.07s/it, min_val_loss=4.79, train=0.255, val=0.241, val_loss=4.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 5/200 [09:07<5:46:53, 106.74s/it, min_val_loss=4.79, train=0.255, val=0.241, val_loss=4.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 5/200 [11:13<5:46:53, 106.74s/it, min_val_loss=4.14, train=0.304, val=0.285, val_loss=4.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 6/200 [11:13<6:03:32, 112.43s/it, min_val_loss=4.14, train=0.304, val=0.285, val_loss=4.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 6/200 [13:18<6:03:32, 112.43s/it, min_val_loss=3.51, train=0.414, val=0.377, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 7/200 [13:18<6:13:56, 116.25s/it, min_val_loss=3.51, train=0.414, val=0.377, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 7/200 [15:25<6:13:56, 116.25s/it, min_val_loss=2.65, train=0.336, val=0.309, val_loss=4.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 8/200 [15:25<6:22:20, 119.48s/it, min_val_loss=2.65, train=0.336, val=0.309, val_loss=4.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 8/200 [17:30<6:22:20, 119.48s/it, min_val_loss=2.65, train=0.42, val=0.372, val_loss=4.35] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 9/200 [17:30<6:25:32, 121.11s/it, min_val_loss=2.65, train=0.42, val=0.372, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 9/200 [19:34<6:25:32, 121.11s/it, min_val_loss=2.65, train=0.491, val=0.433, val_loss=4.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 10/200 [19:34<6:26:45, 122.13s/it, min_val_loss=2.65, train=0.491, val=0.433, val_loss=4.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 10/200 [21:38<6:26:45, 122.13s/it, min_val_loss=2.31, train=0.455, val=0.401, val_loss=4.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 11/200 [21:38<6:26:30, 122.70s/it, min_val_loss=2.31, train=0.455, val=0.401, val_loss=4.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 11/200 [23:42<6:26:30, 122.70s/it, min_val_loss=2.31, train=0.483, val=0.419, val_loss=4.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 12/200 [23:42<6:25:01, 122.88s/it, min_val_loss=2.31, train=0.483, val=0.419, val_loss=4.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 12/200 [25:47<6:25:01, 122.88s/it, min_val_loss=2.31, train=0.525, val=0.453, val_loss=4.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 13/200 [25:47<6:25:09, 123.58s/it, min_val_loss=2.31, train=0.525, val=0.453, val_loss=4.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 13/200 [27:52<6:25:09, 123.58s/it, min_val_loss=2.21, train=0.51, val=0.443, val_loss=4.28] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 14/200 [27:52<6:24:34, 124.05s/it, min_val_loss=2.21, train=0.51, val=0.443, val_loss=4.28]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    print('wandb not found')\n",
    "\n",
    "from myhelpers import config_plots, TrialStatistics\n",
    "from myhelpers.try_warning import try_running\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "config_plots.global_settings()\n",
    "\n",
    "experimetnsFileName = \"experiments.csv\"\n",
    "WANDB_message=\"wandb not working\"\n",
    "\n",
    "\n",
    "# For logging to server\n",
    "try_running(lambda : wandb.login(), WANDB_message)\n",
    "\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "# set cuda\n",
    "if device is not None:\n",
    "    print(\"using cuda\", device)\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# get experiment params\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "\n",
    "# init experiments file\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "\n",
    "paramsIterator = config_parser.getExperiments()  \n",
    "number_of_experiments = sum(1 for e in paramsIterator)  \n",
    "experiment_index = 0\n",
    "\n",
    "# Loop through experiments\n",
    "# with progressbar.ProgressBar(max_value=number_of_experiments) as bar:\n",
    "with tqdm(total=number_of_experiments, desc=\"experiment\") as bar:\n",
    "    for experiment_params in config_parser.getExperiments():\n",
    "        print(experiment_params)\n",
    "        experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "        # load images \n",
    "        if experiment_params['image_path'] == 'cifar-100-python':\n",
    "            datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        else:\n",
    "            datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "        train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "        architecture = {\n",
    "            \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "            \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "        }\n",
    "\n",
    "        # Loop through n trials\n",
    "        for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "            modelName = getModelName(experiment_params, i)\n",
    "            trialName = os.path.join(experimentPathAndName, modelName)\n",
    "            trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "            row_information = {\n",
    "                'experimentName': experimentName,\n",
    "                'modelName': modelName,\n",
    "                'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "                'experimentHash': experimentHash,\n",
    "                'trialHash': trialHash\n",
    "            }\n",
    "            row_information = {**row_information, **experiment_params} \n",
    "            print(row_information)\n",
    "\n",
    "            run = try_running(lambda : wandb.init(project='HGNN', group=experimentName+\"-\"+experimentHash, name=trialHash, config=row_information), WANDB_message) #, reinit=True\n",
    "\n",
    "            # Train/Load model\n",
    "            model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "#             print(model)\n",
    "\n",
    "#             try_running(lambda : wandb.watch(model, log=\"all\"), WANDB_message)\n",
    "\n",
    "            if os.path.exists(CNN.getModelFile(trialName)):\n",
    "                print(\"Model {0} found!\".format(trialName))\n",
    "            else:\n",
    "                initModelPath = CNN.getInitModelFile(experimentPathAndName)\n",
    "                if os.path.exists(initModelPath):\n",
    "                    model.load_state_dict(torch.load(initModelPath))\n",
    "                    print(\"Init Model {0} found!\".format(initModelPath))\n",
    "                CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=detailed_reporting)\n",
    "\n",
    "            # Add to experiments file\n",
    "            if os.path.exists(experimentsFileNameAndPath):\n",
    "                experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "            else:\n",
    "                experiments_df = pd.DataFrame()\n",
    "\n",
    "            record_exists = not (experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName]).empty if not experiments_df.empty else False\n",
    "            if record_exists:\n",
    "                experiments_df.drop(experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName].index, inplace = True) \n",
    "\n",
    "            experiments_df = experiments_df.append(pd.DataFrame(row_information, index=[0]), ignore_index = True)\n",
    "            experiments_df.to_csv(experimentsFileNameAndPath, header=True, index=False)\n",
    "\n",
    "            try_running(lambda : run.finish(), WANDB_message)\n",
    "\n",
    "        bar.update()\n",
    "\n",
    "        experiment_index = experiment_index + 1\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "#     import argparse\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--cuda', required=True, type=int)\n",
    "#     parser.add_argument('--experiments', required=True)\n",
    "#     parser.add_argument('--data', required=True)\n",
    "#     parser.add_argument('--name', required=True)\n",
    "#     parser.add_argument('--detailed', required=False, action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "#     main(experimentName=args.name, experimentsPath=args.experiments, dataPath=args.data, device=args.cuda, detailed_reporting=args.detailed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
