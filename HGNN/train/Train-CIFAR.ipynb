{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath = \"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = \"CIFAR_scheduler_experiments2\"\n",
    "device = 1\n",
    "detailed_reporting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "experiment:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 1\n",
      "{'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.0005, 'scheduler_gamma': 0.1, 'scheduler_patience': 10, 'modelType': 'HGNN', 'two_phase_lambda': False, 'lambda': 0, 'tl_model': 'preResNet', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'triplet_layers_dic': 'layer2,layer3', 'phylogeny_loss': False, 'displayName': 'CIFARpre-s-HGNN', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f6b801e56c45f98f475201a5eec785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_scheduler_experiments2', 'modelName': 'models/60fd4d3848461f38413024599d9deb50e23a27dba3fee2ac068c2607', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '9481160d0af0d85254d9f8b5aee42e11c800cb873f8098524be39218', 'trialHash': '60fd4d3848461f38413024599d9deb50e23a27dba3fee2ac068c2607', 'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.0005, 'scheduler_gamma': 0.1, 'scheduler_patience': 10, 'modelType': 'HGNN', 'two_phase_lambda': False, 'lambda': 0, 'tl_model': 'preResNet', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'triplet_layers_dic': 'layer2,layer3', 'phylogeny_loss': False, 'displayName': 'CIFARpre-s-HGNN', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">60fd4d3848461f38413024599d9deb50e23a27dba3fee2ac068c2607</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/17wzbcrh\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/17wzbcrh</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210528_135456-17wzbcrh</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:38<?, ?it/s, min_val_loss=inf, train=0.00156, val=0.00152, val_loss=4.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [00:38<25:07, 38.65s/it, min_val_loss=inf, train=0.00156, val=0.00152, val_loss=4.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [04:22<25:07, 38.65s/it, min_val_loss=inf, train=0.837, val=0.797, val_loss=3.95]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [04:22<59:40, 94.23s/it, min_val_loss=inf, train=0.837, val=0.797, val_loss=3.95]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [08:10<59:40, 94.23s/it, min_val_loss=1.25, train=0.868, val=0.81, val_loss=3.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [08:10<1:22:49, 134.30s/it, min_val_loss=1.25, train=0.868, val=0.81, val_loss=3.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [11:59<1:22:49, 134.30s/it, min_val_loss=1.23, train=0.878, val=0.802, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [11:59<1:37:43, 162.87s/it, min_val_loss=1.23, train=0.878, val=0.802, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [15:50<1:37:43, 162.87s/it, min_val_loss=1.23, train=0.882, val=0.795, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [15:51<1:46:56, 183.34s/it, min_val_loss=1.23, train=0.882, val=0.795, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [19:34<1:46:56, 183.34s/it, min_val_loss=1.23, train=0.877, val=0.778, val_loss=3.9] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [19:34<1:50:39, 195.27s/it, min_val_loss=1.23, train=0.877, val=0.778, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [23:16<1:50:39, 195.27s/it, min_val_loss=1.23, train=0.892, val=0.785, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [23:16<1:51:54, 203.46s/it, min_val_loss=1.23, train=0.892, val=0.785, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [27:00<1:51:54, 203.46s/it, min_val_loss=1.23, train=0.897, val=0.781, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [27:00<1:51:50, 209.71s/it, min_val_loss=1.23, train=0.897, val=0.781, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [30:43<1:51:50, 209.71s/it, min_val_loss=1.23, train=0.899, val=0.78, val_loss=3.89] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [30:43<1:50:17, 213.48s/it, min_val_loss=1.23, train=0.899, val=0.78, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [34:26<1:50:17, 213.48s/it, min_val_loss=1.23, train=0.908, val=0.774, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [34:26<1:48:08, 216.27s/it, min_val_loss=1.23, train=0.908, val=0.774, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [38:08<1:48:08, 216.27s/it, min_val_loss=1.23, train=0.91, val=0.766, val_loss=3.9]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [38:08<1:45:23, 218.04s/it, min_val_loss=1.23, train=0.91, val=0.766, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [41:50<1:45:23, 218.04s/it, min_val_loss=1.23, train=0.918, val=0.776, val_loss=3.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [41:50<1:42:19, 219.26s/it, min_val_loss=1.23, train=0.918, val=0.776, val_loss=3.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [45:32<1:42:19, 219.26s/it, min_val_loss=1.23, train=0.921, val=0.769, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [45:32<1:39:02, 220.08s/it, min_val_loss=1.23, train=0.921, val=0.769, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [49:27<1:39:02, 220.08s/it, min_val_loss=1.23, train=0.929, val=0.775, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [49:27<1:37:16, 224.50s/it, min_val_loss=1.23, train=0.929, val=0.775, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [53:18<1:37:16, 224.50s/it, min_val_loss=1.23, train=0.982, val=0.824, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [53:18<1:34:25, 226.64s/it, min_val_loss=1.23, train=0.982, val=0.824, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [57:22<1:34:25, 226.64s/it, min_val_loss=1.21, train=0.988, val=0.835, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [57:22<1:32:45, 231.89s/it, min_val_loss=1.21, train=0.988, val=0.835, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [1:01:14<1:32:45, 231.89s/it, min_val_loss=1.2, train=0.992, val=0.839, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [1:01:14<1:28:50, 231.76s/it, min_val_loss=1.2, train=0.992, val=0.839, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [1:04:57<1:28:50, 231.76s/it, min_val_loss=1.19, train=0.994, val=0.843, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [1:04:57<1:24:00, 229.12s/it, min_val_loss=1.19, train=0.994, val=0.843, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [1:08:40<1:24:00, 229.12s/it, min_val_loss=1.19, train=0.995, val=0.839, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [1:08:40<1:19:35, 227.40s/it, min_val_loss=1.19, train=0.995, val=0.839, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [1:12:23<1:19:35, 227.40s/it, min_val_loss=1.19, train=0.996, val=0.84, val_loss=3.81] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [1:12:24<1:15:23, 226.18s/it, min_val_loss=1.19, train=0.996, val=0.84, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [1:16:06<1:15:23, 226.18s/it, min_val_loss=1.19, train=0.996, val=0.836, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [1:16:06<1:11:18, 225.20s/it, min_val_loss=1.19, train=0.996, val=0.836, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [1:19:49<1:11:18, 225.20s/it, min_val_loss=1.19, train=0.997, val=0.844, val_loss=3.8] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [1:19:49<1:07:21, 224.53s/it, min_val_loss=1.19, train=0.997, val=0.844, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [1:23:32<1:07:21, 224.53s/it, min_val_loss=1.18, train=0.997, val=0.842, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [1:23:32<1:03:26, 223.92s/it, min_val_loss=1.18, train=0.997, val=0.842, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [1:27:14<1:03:26, 223.92s/it, min_val_loss=1.18, train=0.998, val=0.837, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [1:27:14<59:35, 223.49s/it, min_val_loss=1.18, train=0.998, val=0.837, val_loss=3.81]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [1:30:57<59:35, 223.49s/it, min_val_loss=1.18, train=0.998, val=0.838, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [1:30:57<55:49, 223.32s/it, min_val_loss=1.18, train=0.998, val=0.838, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [1:34:40<55:49, 223.32s/it, min_val_loss=1.18, train=0.998, val=0.84, val_loss=3.81] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [1:34:40<52:02, 223.01s/it, min_val_loss=1.18, train=0.998, val=0.84, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [1:38:22<52:02, 223.01s/it, min_val_loss=1.18, train=0.999, val=0.843, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [1:38:22<48:18, 222.96s/it, min_val_loss=1.18, train=0.999, val=0.843, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [1:42:04<48:18, 222.96s/it, min_val_loss=1.18, train=0.999, val=0.837, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [1:42:04<44:31, 222.66s/it, min_val_loss=1.18, train=0.999, val=0.837, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [1:45:46<44:31, 222.66s/it, min_val_loss=1.18, train=0.999, val=0.841, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 29/40 [1:45:46<40:44, 222.27s/it, min_val_loss=1.18, train=0.999, val=0.841, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 29/40 [1:49:28<40:44, 222.27s/it, min_val_loss=1.18, train=0.998, val=0.838, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 30/40 [1:49:28<37:01, 222.14s/it, min_val_loss=1.18, train=0.998, val=0.838, val_loss=3.81]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 30/40 [1:53:10<37:01, 222.14s/it, min_val_loss=1.18, train=0.999, val=0.836, val_loss=3.8] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 31/40 [1:53:10<33:20, 222.32s/it, min_val_loss=1.18, train=0.999, val=0.836, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 31/40 [1:56:52<33:20, 222.32s/it, min_val_loss=1.18, train=0.999, val=0.838, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 32/40 [1:56:52<29:37, 222.24s/it, min_val_loss=1.18, train=0.999, val=0.838, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 32/40 [2:00:36<29:37, 222.24s/it, min_val_loss=1.18, train=0.999, val=0.838, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 33/40 [2:00:36<25:59, 222.77s/it, min_val_loss=1.18, train=0.999, val=0.838, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 33/40 [2:04:21<25:59, 222.77s/it, min_val_loss=1.18, train=0.999, val=0.84, val_loss=3.8] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 34/40 [2:04:21<22:19, 223.22s/it, min_val_loss=1.18, train=0.999, val=0.84, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 34/40 [2:08:05<22:19, 223.22s/it, min_val_loss=1.18, train=0.999, val=0.84, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 35/40 [2:08:05<18:37, 223.49s/it, min_val_loss=1.18, train=0.999, val=0.84, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 35/40 [2:11:49<18:37, 223.49s/it, min_val_loss=1.18, train=0.999, val=0.844, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 36/40 [2:11:49<14:54, 223.67s/it, min_val_loss=1.18, train=0.999, val=0.844, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 36/40 [2:15:32<14:54, 223.67s/it, min_val_loss=1.18, train=0.999, val=0.839, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 37/40 [2:15:32<11:10, 223.45s/it, min_val_loss=1.18, train=0.999, val=0.839, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 37/40 [2:19:16<11:10, 223.45s/it, min_val_loss=1.18, train=1, val=0.842, val_loss=3.8]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 38/40 [2:19:16<07:27, 223.54s/it, min_val_loss=1.18, train=1, val=0.842, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 38/40 [2:23:00<07:27, 223.54s/it, min_val_loss=1.18, train=1, val=0.842, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 39/40 [2:23:00<03:43, 223.71s/it, min_val_loss=1.18, train=1, val=0.842, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 39/40 [2:26:44<03:43, 223.71s/it, min_val_loss=1.18, train=1, val=0.842, val_loss=3.8]\u001b[A\u001b[A\n",
      "\n",
      "iteration: 100%|██████████| 40/40 [2:26:59<00:00, 220.49s/it, min_val_loss=1.18, train=1, val=0.842, val_loss=3.8]\u001b[A\u001b[A\n",
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/ipykernel_launcher.py:110: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 67348<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae9a70eefc24021bbe4f002b0bce4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210528_135456-17wzbcrh/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210528_135456-17wzbcrh/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>39</td></tr><tr><td>learning rate</td><td>1e-05</td></tr><tr><td>validation_fine_f1</td><td>0.83949</td></tr><tr><td>training_fine_f1</td><td>0.99965</td></tr><tr><td>training_fine_acc</td><td>0.99965</td></tr><tr><td>test_fine_f1</td><td>0.76604</td></tr><tr><td>test_fine_acc</td><td>0.7663</td></tr><tr><td>validation_loss</td><td>3.79794</td></tr><tr><td>_runtime</td><td>8814</td></tr><tr><td>_timestamp</td><td>1622233310</td></tr><tr><td>_step</td><td>24414</td></tr><tr><td>loss</td><td>0.00528</td></tr><tr><td>batch</td><td>24999</td></tr><tr><td>loss_fine</td><td>0.00159</td></tr><tr><td>lambda_fine</td><td>0.75958</td></tr><tr><td>loss_coarse</td><td>0.01693</td></tr><tr><td>lambda_coarse</td><td>0.24042</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>█████████████▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁████▇█▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>training_fine_f1</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>training_fine_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▇█▇▇▇▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>test_fine_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>validation_loss</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▃▄▃▃▄▃▄▃▃▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▄▂▄▃▃▄▂▃▃▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▄█▄▃▆▄▅▅▃▅█▃▄▆▅▆▆▅▆▆▇▅▆▅▅▆▆▅▆▅▆▆▅▆▅▅▅▅▆</td></tr><tr><td>loss_coarse</td><td>█▅▄▄▃▄▄▃▄▂▃▄▃▂▂▁▂▂▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>█▅▁▅▆▃▅▄▄▆▄▁▆▅▃▄▃▃▄▃▃▂▄▃▄▄▃▃▄▃▄▃▃▄▃▄▄▄▄▃</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">60fd4d3848461f38413024599d9deb50e23a27dba3fee2ac068c2607</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/17wzbcrh\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/17wzbcrh</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  50%|█████     | 1/2 [2:27:14<2:27:14, 8834.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.0005, 'scheduler_gamma': 0.1, 'scheduler_patience': 10, 'modelType': 'DISCO', 'two_phase_lambda': False, 'lambda': 0, 'tl_model': 'preResNet', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'triplet_layers_dic': 'layer2,layer3', 'phylogeny_loss': False, 'displayName': 'CIFARpre-s-DISCO', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f3931d0ee6407c8141e82926283cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_scheduler_experiments2', 'modelName': 'models/45935f01ac421813cfaef969fec9663ebbfdec021da495d50451e4e4', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '44b61f346d0316a73bcda4c9e13c40f4da6e8d08f8abdc91d8d2fe03', 'trialHash': '45935f01ac421813cfaef969fec9663ebbfdec021da495d50451e4e4', 'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.0005, 'scheduler_gamma': 0.1, 'scheduler_patience': 10, 'modelType': 'DISCO', 'two_phase_lambda': False, 'lambda': 0, 'tl_model': 'preResNet', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'triplet_layers_dic': 'layer2,layer3', 'phylogeny_loss': False, 'displayName': 'CIFARpre-s-DISCO', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">45935f01ac421813cfaef969fec9663ebbfdec021da495d50451e4e4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1t8d2wuv\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1t8d2wuv</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210528_162211-1t8d2wuv</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:26<?, ?it/s, min_val_loss=inf, train=0.00279, val=0.00261, val_loss=4.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [00:26<17:05, 26.29s/it, min_val_loss=inf, train=0.00279, val=0.00261, val_loss=4.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [02:31<17:05, 26.29s/it, min_val_loss=inf, train=0.747, val=0.716, val_loss=4.15]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [02:31<35:22, 55.85s/it, min_val_loss=inf, train=0.747, val=0.716, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [04:34<35:22, 55.85s/it, min_val_loss=1.4, train=0.818, val=0.773, val_loss=3.98]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [04:34<46:58, 76.18s/it, min_val_loss=1.4, train=0.818, val=0.773, val_loss=3.98]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [06:38<46:58, 76.18s/it, min_val_loss=1.29, train=0.842, val=0.776, val_loss=3.95]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [06:38<54:20, 90.57s/it, min_val_loss=1.29, train=0.842, val=0.776, val_loss=3.95]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [08:42<54:20, 90.57s/it, min_val_loss=1.29, train=0.857, val=0.776, val_loss=3.93]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [08:42<58:32, 100.36s/it, min_val_loss=1.29, train=0.857, val=0.776, val_loss=3.93]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [10:45<58:32, 100.36s/it, min_val_loss=1.29, train=0.854, val=0.765, val_loss=3.93]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [10:45<1:00:43, 107.16s/it, min_val_loss=1.29, train=0.854, val=0.765, val_loss=3.93]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [12:47<1:00:43, 107.16s/it, min_val_loss=1.29, train=0.873, val=0.777, val_loss=3.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [12:47<1:01:24, 111.64s/it, min_val_loss=1.29, train=0.873, val=0.777, val_loss=3.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [14:50<1:01:24, 111.64s/it, min_val_loss=1.29, train=0.888, val=0.785, val_loss=3.9] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [14:50<1:01:21, 115.05s/it, min_val_loss=1.29, train=0.888, val=0.785, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [16:54<1:01:21, 115.05s/it, min_val_loss=1.27, train=0.897, val=0.784, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [16:54<1:00:49, 117.71s/it, min_val_loss=1.27, train=0.897, val=0.784, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [18:56<1:00:49, 117.71s/it, min_val_loss=1.27, train=0.896, val=0.771, val_loss=3.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [18:56<59:33, 119.13s/it, min_val_loss=1.27, train=0.896, val=0.771, val_loss=3.91] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [20:58<59:33, 119.13s/it, min_val_loss=1.27, train=0.898, val=0.773, val_loss=3.9] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [20:58<58:00, 120.03s/it, min_val_loss=1.27, train=0.898, val=0.773, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [23:02<58:00, 120.03s/it, min_val_loss=1.27, train=0.907, val=0.779, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [23:02<56:34, 121.23s/it, min_val_loss=1.27, train=0.907, val=0.779, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [25:07<56:34, 121.23s/it, min_val_loss=1.27, train=0.908, val=0.764, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [25:07<54:58, 122.15s/it, min_val_loss=1.27, train=0.908, val=0.764, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [27:10<54:58, 122.15s/it, min_val_loss=1.27, train=0.919, val=0.777, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [27:10<53:09, 122.67s/it, min_val_loss=1.27, train=0.919, val=0.777, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [29:14<53:09, 122.67s/it, min_val_loss=1.27, train=0.92, val=0.77, val_loss=3.89]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [29:14<51:16, 123.07s/it, min_val_loss=1.27, train=0.92, val=0.77, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [31:18<51:16, 123.07s/it, min_val_loss=1.27, train=0.929, val=0.78, val_loss=3.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [31:18<49:19, 123.33s/it, min_val_loss=1.27, train=0.929, val=0.78, val_loss=3.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [33:22<49:19, 123.33s/it, min_val_loss=1.27, train=0.935, val=0.783, val_loss=3.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [33:22<47:19, 123.45s/it, min_val_loss=1.27, train=0.935, val=0.783, val_loss=3.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [35:26<47:19, 123.45s/it, min_val_loss=1.27, train=0.928, val=0.768, val_loss=3.9] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [35:26<45:21, 123.68s/it, min_val_loss=1.27, train=0.928, val=0.768, val_loss=3.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [37:30<45:21, 123.68s/it, min_val_loss=1.27, train=0.941, val=0.775, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [37:30<43:19, 123.79s/it, min_val_loss=1.27, train=0.941, val=0.775, val_loss=3.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [39:35<43:19, 123.79s/it, min_val_loss=1.27, train=0.98, val=0.818, val_loss=3.84] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [39:35<41:23, 124.18s/it, min_val_loss=1.27, train=0.98, val=0.818, val_loss=3.84]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [41:40<41:23, 124.18s/it, min_val_loss=1.22, train=0.986, val=0.821, val_loss=3.84]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [41:40<39:19, 124.20s/it, min_val_loss=1.22, train=0.986, val=0.821, val_loss=3.84]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [43:44<39:19, 124.20s/it, min_val_loss=1.22, train=0.989, val=0.825, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [43:44<37:13, 124.10s/it, min_val_loss=1.22, train=0.989, val=0.825, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [45:47<37:13, 124.10s/it, min_val_loss=1.21, train=0.99, val=0.824, val_loss=3.83] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [45:47<35:05, 123.87s/it, min_val_loss=1.21, train=0.99, val=0.824, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [47:51<35:05, 123.87s/it, min_val_loss=1.21, train=0.991, val=0.824, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [47:51<33:05, 124.07s/it, min_val_loss=1.21, train=0.991, val=0.824, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [49:54<33:05, 124.07s/it, min_val_loss=1.21, train=0.993, val=0.823, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [49:54<30:55, 123.69s/it, min_val_loss=1.21, train=0.993, val=0.823, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [51:58<30:55, 123.69s/it, min_val_loss=1.21, train=0.994, val=0.825, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [51:58<28:51, 123.66s/it, min_val_loss=1.21, train=0.994, val=0.825, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [54:02<28:51, 123.66s/it, min_val_loss=1.21, train=0.994, val=0.824, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [54:02<26:51, 123.96s/it, min_val_loss=1.21, train=0.994, val=0.824, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [56:06<26:51, 123.96s/it, min_val_loss=1.21, train=0.996, val=0.825, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [56:06<24:47, 123.94s/it, min_val_loss=1.21, train=0.996, val=0.825, val_loss=3.83]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [58:11<24:47, 123.94s/it, min_val_loss=1.21, train=0.997, val=0.824, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 29/40 [58:11<22:46, 124.27s/it, min_val_loss=1.21, train=0.997, val=0.824, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 29/40 [1:00:18<22:46, 124.27s/it, min_val_loss=1.21, train=0.995, val=0.826, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 30/40 [1:00:18<20:49, 124.93s/it, min_val_loss=1.21, train=0.995, val=0.826, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 30/40 [1:02:22<20:49, 124.93s/it, min_val_loss=1.21, train=0.997, val=0.829, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 31/40 [1:02:22<18:42, 124.72s/it, min_val_loss=1.21, train=0.997, val=0.829, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 31/40 [1:04:26<18:42, 124.72s/it, min_val_loss=1.21, train=0.997, val=0.825, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 32/40 [1:04:26<16:36, 124.55s/it, min_val_loss=1.21, train=0.997, val=0.825, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 32/40 [1:06:30<16:36, 124.55s/it, min_val_loss=1.21, train=0.997, val=0.821, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 33/40 [1:06:30<14:30, 124.42s/it, min_val_loss=1.21, train=0.997, val=0.821, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 33/40 [1:08:35<14:30, 124.42s/it, min_val_loss=1.21, train=0.997, val=0.822, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 34/40 [1:08:35<12:26, 124.37s/it, min_val_loss=1.21, train=0.997, val=0.822, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 34/40 [1:10:38<12:26, 124.37s/it, min_val_loss=1.21, train=0.998, val=0.821, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 35/40 [1:10:38<10:20, 124.02s/it, min_val_loss=1.21, train=0.998, val=0.821, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 35/40 [1:12:40<10:20, 124.02s/it, min_val_loss=1.21, train=0.998, val=0.825, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 36/40 [1:12:40<08:14, 123.60s/it, min_val_loss=1.21, train=0.998, val=0.825, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 36/40 [1:14:45<08:14, 123.60s/it, min_val_loss=1.21, train=0.998, val=0.821, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 37/40 [1:14:45<06:11, 123.77s/it, min_val_loss=1.21, train=0.998, val=0.821, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 37/40 [1:16:49<06:11, 123.77s/it, min_val_loss=1.21, train=0.998, val=0.822, val_loss=3.82]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 38/40 [1:16:49<04:08, 124.01s/it, min_val_loss=1.21, train=0.998, val=0.822, val_loss=3.82]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    print('wandb not found')\n",
    "\n",
    "from myhelpers import config_plots, TrialStatistics\n",
    "from myhelpers.try_warning import try_running\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "config_plots.global_settings()\n",
    "\n",
    "experimetnsFileName = \"experiments.csv\"\n",
    "WANDB_message=\"wandb not working\"\n",
    "\n",
    "\n",
    "# For logging to server\n",
    "try_running(lambda : wandb.login(), WANDB_message)\n",
    "\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "# set cuda\n",
    "if device is not None:\n",
    "    print(\"using cuda\", device)\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# get experiment params\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "\n",
    "# init experiments file\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "\n",
    "paramsIterator = config_parser.getExperiments()  \n",
    "number_of_experiments = sum(1 for e in paramsIterator)  \n",
    "experiment_index = 0\n",
    "\n",
    "# Loop through experiments\n",
    "# with progressbar.ProgressBar(max_value=number_of_experiments) as bar:\n",
    "with tqdm(total=number_of_experiments, desc=\"experiment\") as bar:\n",
    "    for experiment_params in config_parser.getExperiments():\n",
    "        print(experiment_params)\n",
    "        experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "        # load images \n",
    "        if experiment_params['image_path'] == 'cifar-100-python':\n",
    "            datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        else:\n",
    "            datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "        train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "        architecture = {\n",
    "            \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "            \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "        }\n",
    "\n",
    "        # Loop through n trials\n",
    "        for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "            modelName = getModelName(experiment_params, i)\n",
    "            trialName = os.path.join(experimentPathAndName, modelName)\n",
    "            trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "            row_information = {\n",
    "                'experimentName': experimentName,\n",
    "                'modelName': modelName,\n",
    "                'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "                'experimentHash': experimentHash,\n",
    "                'trialHash': trialHash\n",
    "            }\n",
    "            row_information = {**row_information, **experiment_params} \n",
    "            print(row_information)\n",
    "\n",
    "            run = try_running(lambda : wandb.init(project='HGNN', group=experimentName+\"-\"+experimentHash, name=trialHash, config=row_information), WANDB_message) #, reinit=True\n",
    "\n",
    "            # Train/Load model\n",
    "            model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "#             print(model)\n",
    "\n",
    "#             try_running(lambda : wandb.watch(model, log=\"all\"), WANDB_message)\n",
    "\n",
    "            if os.path.exists(CNN.getModelFile(trialName)):\n",
    "                print(\"Model {0} found!\".format(trialName))\n",
    "            else:\n",
    "                initModelPath = CNN.getInitModelFile(experimentPathAndName)\n",
    "                if os.path.exists(initModelPath):\n",
    "                    model.load_state_dict(torch.load(initModelPath))\n",
    "                    print(\"Init Model {0} found!\".format(initModelPath))\n",
    "                CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=detailed_reporting)\n",
    "\n",
    "            # Add to experiments file\n",
    "            if os.path.exists(experimentsFileNameAndPath):\n",
    "                experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "            else:\n",
    "                experiments_df = pd.DataFrame()\n",
    "\n",
    "            record_exists = not (experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName]).empty if not experiments_df.empty else False\n",
    "            if record_exists:\n",
    "                experiments_df.drop(experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName].index, inplace = True) \n",
    "\n",
    "            experiments_df = experiments_df.append(pd.DataFrame(row_information, index=[0]), ignore_index = True)\n",
    "            experiments_df.to_csv(experimentsFileNameAndPath, header=True, index=False)\n",
    "\n",
    "            try_running(lambda : run.finish(), WANDB_message)\n",
    "\n",
    "        bar.update()\n",
    "\n",
    "        experiment_index = experiment_index + 1\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "#     import argparse\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--cuda', required=True, type=int)\n",
    "#     parser.add_argument('--experiments', required=True)\n",
    "#     parser.add_argument('--data', required=True)\n",
    "#     parser.add_argument('--name', required=True)\n",
    "#     parser.add_argument('--detailed', required=False, action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "#     main(experimentName=args.name, experimentsPath=args.experiments, dataPath=args.data, device=args.cuda, detailed_reporting=args.detailed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
