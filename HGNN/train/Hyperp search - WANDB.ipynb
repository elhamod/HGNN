{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "device=0 # No other value works. known bug in Jupyter.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=JUPYTER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME=JUPYTER\n",
    "\n",
    "import wandb\n",
    "import math\n",
    "\n",
    "import os\n",
    "from myhelpers import TrialStatistics\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sweep params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_config = {\n",
    "#     'method': 'random'\n",
    "# }\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'early_terminate': {\n",
    "       'type': 'hyperband',\n",
    "       'min_iter': 8   \n",
    "    }\n",
    "}\n",
    "metric = {\n",
    "    'name': 'best_val_f1',\n",
    "    'goal': 'maximize'   \n",
    "    }\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=\"CIFAR_KLDiv_Hyperp\"\n",
    "params = {\n",
    "    \"image_path\":  {\n",
    "        'value':\"cifar-100-python\"#\"Curated4/Easy_50\"\n",
    "    },\n",
    "    \"suffix\":{\n",
    "        'value': None\n",
    "    },\n",
    "    \n",
    "    # dataset\n",
    "    \"img_res\": {\n",
    "        'value': 32 #448\n",
    "    }, \n",
    "    \"augmented\": {\n",
    "        'values': [True, False]\n",
    "    }, \n",
    "\n",
    "    # training\n",
    "    \"batchSize\": {\n",
    "          'values': [8, 16, 23, 64, 128, 256] # 8, 16, 64, 128\n",
    "        }, \n",
    "    \"learning_rate\":{\n",
    "        'distribution': 'log_uniform',\n",
    "        'min': math.log(0.00005),\n",
    "        'max': math.log(0.5),\n",
    "      },\n",
    "#     \"learning_rate\": { 'value': 0.00002},\n",
    "    \"numOfTrials\":{\n",
    "        'value': 1\n",
    "    },\n",
    "    \"fc_layers\": {\n",
    "          'values': [1, 2, 3]\n",
    "        },\n",
    "#     \"fc_layers\": {'value': 2},\n",
    "    \"modelType\":{\n",
    "        'value':\"BB\"\n",
    "    }, #BB DISCO DSN HGNN HGNN_add HGNN_cat\n",
    "#     \"lambda\": {\n",
    "#         'distribution': 'log_uniform',\n",
    "#         'min': math.log(0.005),\n",
    "#         'max': math.log(0.5),\n",
    "#       },\n",
    "    \"lambda\":{\n",
    "        'value': 0\n",
    "    },\n",
    "    \"unsupervisedOnTest\": {\n",
    "        'value': False\n",
    "    },\n",
    "    \"tl_model\": {\n",
    "        'value': 'CIFAR'#'ResNet18'\n",
    "    },\n",
    "    \"link_layer\": {\n",
    "        'value': 'avgpool'\n",
    "    },\n",
    "#     \"link_layer\":  {\n",
    "#         'values': [\"avgpool\",\"layer3\"]\n",
    "#     }, \n",
    "    \n",
    "#     \"adaptive_smoothing\": {\n",
    "#         'values': [True,False]\n",
    "#     },\n",
    "    \"adaptive_smoothing\": {\n",
    "        'value': False\n",
    "    },\n",
    "    \"adaptive_lambda\": {\n",
    "        'value': 0.001\n",
    "    },\n",
    "#     \"adaptive_lambda\": {\n",
    "#         'distribution': 'log_uniform',\n",
    "#         'min': math.log(0.001),\n",
    "#         'max': math.log(0.1),\n",
    "#       }, \n",
    "#     \"adaptive_alpha\": {\n",
    "#         'min': 0.5,\n",
    "#         'max': 0.99,\n",
    "#       }, \n",
    "    \"adaptive_alpha\": {\n",
    "        'value': 0.5\n",
    "    },\n",
    "    \n",
    "#     \"noSpeciesBackprop\":{\n",
    "#         'values': [True,False]\n",
    "#     },\n",
    "    \"noSpeciesBackprop\": {\n",
    "        'value': False\n",
    "    },\n",
    "    \"phylogeny_loss\":{\n",
    "        'values': [\"KLDiv\"]#[False, \"MSE\", \"KLDiv\"] # False, \"MSE\", \"KLDiv\"\n",
    "    },\n",
    "#     \"phylogeny_loss_epsilon\": {\n",
    "#         'distribution': 'log_uniform',\n",
    "#         'min': math.log(0.001),\n",
    "#         'max': math.log(1),\n",
    "#       }, \n",
    "    \"phylogeny_loss_epsilon\": {\n",
    "        'value': 0.03\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# python3 HGNN/code/HGNN/HGNN/train/train.py --cuda=6 --name=\"biology_paper_curated4_Medium_backprop_experiment\" -- -- --detailed\n",
    "\n",
    "experimentsPath = \"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = project\n",
    "\n",
    "def train(config=None):\n",
    "    run = wandb.init()\n",
    "\n",
    "#     try:\n",
    "    experiment_params=wandb.config if config is None else config\n",
    "    experiment_params = dict(experiment_params)\n",
    "    config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "    print(experiment_params)\n",
    "    experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "    # load images \n",
    "    experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "    if experiment_params['image_path'] == 'cifar-100-python':\n",
    "        datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "    else:\n",
    "        datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "    datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "    train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "    architecture = {\n",
    "        \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "        \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "    }\n",
    "\n",
    "    # Loop through n trials\n",
    "    for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "        modelName = getModelName(experiment_params, i)\n",
    "        trialName = os.path.join(experimentPathAndName, modelName)\n",
    "        trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "        row_information = {\n",
    "            'experimentName': experimentName,\n",
    "            'modelName': modelName,\n",
    "            'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "            'experimentHash': experimentHash,\n",
    "            'trialHash': trialHash\n",
    "        }\n",
    "        row_information = {**row_information, **experiment_params} \n",
    "        print(row_information)\n",
    "\n",
    "\n",
    "        # Train/Load model\n",
    "        model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "\n",
    "        if os.path.exists(CNN.getModelFile(trialName)):\n",
    "            print(\"Model {0} found!\".format(trialName))\n",
    "        else:\n",
    "            CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=False)\n",
    "\n",
    "    run.finish()\n",
    "#     except:\n",
    "#         print('terminated! Error:', sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call sweeper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can call with your own configs if you want:\n",
    "# train(config={...})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: tquv2ll1\n",
      "Sweep URL: https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1\n"
     ]
    }
   ],
   "source": [
    "# Or call the sweeper and let it do the magic!\n",
    "sweep_id = wandb.sweep(sweep_config, project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c6pxep5o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.08200490621168555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoSpeciesBackprop: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: KLDiv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunsupervisedOnTest: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">worthy-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/c6pxep5o\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/c6pxep5o</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_095740-c6pxep5o</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': False, 'batchSize': 8, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.08200490621168555, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac34fc9abe947db89337bd62479723b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_KLDiv_Hyperp', 'modelName': 'models/3e2d532c9da7f6c513b8d4d0225243afe57dcfde93f0183a6ea84fbc', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': 'f18bc1e0246d0e6260f3542aff2cee9ef8d94f1f621771fc1739fbb7', 'trialHash': '3e2d532c9da7f6c513b8d4d0225243afe57dcfde93f0183a6ea84fbc', 'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': False, 'batchSize': 8, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.08200490621168555, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/torch/nn/functional.py:2352: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "iteration:   3%|▎         | 13/500 [1:45:31<66:29:47, 491.56s/it, min_val_loss=3.02e+3, train=0.000198, val=0.0002, val_loss=4.61]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   3%|▎         | 13/500 [1:45:57<66:09:39, 489.08s/it, min_val_loss=3.02e+3, train=0.000198, val=0.0002, val_loss=4.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 54583<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_095740-c6pxep5o/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_095740-c6pxep5o/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.0098</td></tr><tr><td>batch</td><td>64999</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>loss_fine</td><td>0.0098</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0</td></tr><tr><td>_step</td><td>65012</td></tr><tr><td>_runtime</td><td>6340</td></tr><tr><td>_timestamp</td><td>1619106200</td></tr><tr><td>validation_fine_f1</td><td>0.00033</td></tr><tr><td>training_fine_f1</td><td>0.0002</td></tr><tr><td>test_fine_f1</td><td>0.00035</td></tr><tr><td>validation_loss</td><td>4.60542</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>▆█▆▆▅▅▇▅█▇▆▆▆▄▇▅▄▆▆▇▆▆▅▄▁▇▇▆▆█▆▇▇▆▇▇▇▇▅▆</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>▆█▆▆▅▅▇▅█▇▆▆▆▄▇▅▄▆▆▇▆▆▅▄▁▇▇▆▆█▆▇▇▆▇▇▇▇▅▆</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>validation_fine_f1</td><td>▂▁▂▃▁▃▂█▅▂▂▂▂</td></tr><tr><td>training_fine_f1</td><td>▂▂▂▂▂▁▂█▁▂▂▂▂</td></tr><tr><td>validation_loss</td><td>▁▂█▇▂▇▁▁▁▁██▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">worthy-sweep-1</strong>: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/c6pxep5o\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/c6pxep5o</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8ndnzlrv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005130134825984203\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoSpeciesBackprop: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: KLDiv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunsupervisedOnTest: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dashing-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/8ndnzlrv\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/8ndnzlrv</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_114350-8ndnzlrv</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': True, 'batchSize': 16, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.005130134825984203, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132058715c074117b761dfd0fcfe7a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_KLDiv_Hyperp', 'modelName': 'models/50852086804626ad8b93c92eff6bd96ecb08930c91ad01683a4cade5', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '1e434f2c82e82661cc39d54738b55dc84c91e0c90418ab418febdb6a', 'trialHash': '50852086804626ad8b93c92eff6bd96ecb08930c91ad01683a4cade5', 'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': True, 'batchSize': 16, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.005130134825984203, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   1%|▏         | 7/500 [35:49<42:16:51, 308.74s/it, min_val_loss=4.02e+3, train=0.000195, val=0.00021, val_loss=4.61] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   1%|▏         | 7/500 [36:09<42:26:43, 309.95s/it, min_val_loss=4.02e+3, train=0.000195, val=0.00021, val_loss=4.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5859<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_114350-8ndnzlrv/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_114350-8ndnzlrv/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.00965</td></tr><tr><td>batch</td><td>17499</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>loss_fine</td><td>0.00965</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0</td></tr><tr><td>_step</td><td>17506</td></tr><tr><td>_runtime</td><td>2154</td></tr><tr><td>_timestamp</td><td>1619108384</td></tr><tr><td>validation_fine_f1</td><td>0.00025</td></tr><tr><td>training_fine_f1</td><td>0.0002</td></tr><tr><td>test_fine_f1</td><td>0.0002</td></tr><tr><td>validation_loss</td><td>4.60518</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▄▃▁▄▃██▇██████████▇████████████▇███████</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>loss_fine</td><td>█▄▃▁▄▃██▇██████████▇████████████▇███████</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁█▁▁▄▃▃</td></tr><tr><td>training_fine_f1</td><td>█▁██▅▆▆</td></tr><tr><td>validation_loss</td><td>█▅▁▄▅▃▂</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">dashing-sweep-2</strong>: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/8ndnzlrv\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/8ndnzlrv</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8ncdm7rz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0008746860303340056\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoSpeciesBackprop: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: KLDiv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunsupervisedOnTest: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">earnest-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/8ncdm7rz\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/8ncdm7rz</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_122007-8ncdm7rz</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': True, 'batchSize': 256, 'fc_layers': 2, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.0008746860303340056, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbc718347fd45a39e40a7c7b2ddeddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_KLDiv_Hyperp', 'modelName': 'models/eb82cf474639f343c386044e4e88912659d7ed2dfb00a342bd1e6a29', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '6bbcf303de4dc8008ccf89da67ddb274263b24b3a0e2aec268cf4372', 'trialHash': 'eb82cf474639f343c386044e4e88912659d7ed2dfb00a342bd1e6a29', 'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': True, 'batchSize': 256, 'fc_layers': 2, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.0008746860303340056, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   1%|          | 6/500 [13:45<18:49:05, 137.14s/it, min_val_loss=3.03, train=0.195, val=0.183, val_loss=4.58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   1%|          | 6/500 [13:57<19:09:51, 139.66s/it, min_val_loss=3.03, train=0.195, val=0.183, val_loss=4.58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12912<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_122007-8ncdm7rz/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_122007-8ncdm7rz/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.00589</td></tr><tr><td>batch</td><td>941</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>loss_fine</td><td>0.00589</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0</td></tr><tr><td>_step</td><td>947</td></tr><tr><td>_runtime</td><td>829</td></tr><tr><td>_timestamp</td><td>1619109237</td></tr><tr><td>validation_fine_f1</td><td>0.32958</td></tr><tr><td>training_fine_f1</td><td>0.19482</td></tr><tr><td>test_fine_f1</td><td>0.31818</td></tr><tr><td>validation_loss</td><td>4.57641</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▅▃▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▃▂▃▂</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▄▄▄▄▄▄▄▅▅▅▅▅▅▅▇▇▇▇▇▇███████</td></tr><tr><td>loss_fine</td><td>█▅▃▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▃▂▃▂</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇██████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇██████</td></tr><tr><td>validation_fine_f1</td><td>█▄▄▇▇▁</td></tr><tr><td>training_fine_f1</td><td>█▄▄▇▇▁</td></tr><tr><td>validation_loss</td><td>▂▃▅▁▃█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">earnest-sweep-3</strong>: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/8ncdm7rz\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/8ncdm7rz</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 18p8g63y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.002457862814796102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoSpeciesBackprop: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: KLDiv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunsupervisedOnTest: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">iconic-sweep-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/18p8g63y\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/18p8g63y</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_123414-18p8g63y</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': False, 'batchSize': 8, 'fc_layers': 3, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.002457862814796102, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f214cefd704a448ca7179778472fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_KLDiv_Hyperp', 'modelName': 'models/16fc8e284e02aecc04d684d8ed62b88c14aebb5e14f76591deb9415d', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': 'c52f2714a02373ea7e6c6c726e0418cb11645ab09d76da960e0f8118', 'trialHash': '16fc8e284e02aecc04d684d8ed62b88c14aebb5e14f76591deb9415d', 'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': False, 'batchSize': 8, 'fc_layers': 3, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.002457862814796102, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 9/500 [1:22:06<77:08:18, 565.58s/it, min_val_loss=3.84e+3, train=0.000197, val=0.000202, val_loss=4.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   2%|▏         | 9/500 [1:22:37<75:07:20, 550.80s/it, min_val_loss=3.84e+3, train=0.000197, val=0.000202, val_loss=4.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15445<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_123414-18p8g63y/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_123414-18p8g63y/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.00967</td></tr><tr><td>batch</td><td>44999</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>loss_fine</td><td>0.00967</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0</td></tr><tr><td>_step</td><td>45008</td></tr><tr><td>_runtime</td><td>4931</td></tr><tr><td>_timestamp</td><td>1619114185</td></tr><tr><td>validation_fine_f1</td><td>0.00026</td></tr><tr><td>training_fine_f1</td><td>0.0002</td></tr><tr><td>test_fine_f1</td><td>0.0002</td></tr><tr><td>validation_loss</td><td>4.60519</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▄▇▁▆▅▆▆▆▅▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▅▆▅▅▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>loss_fine</td><td>█▄▇▁▆▅▆▆▆▅▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▅▆▅▅▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▄▁▄█▂▄▁▄▃</td></tr><tr><td>training_fine_f1</td><td>▅█▅▁▇▅█▅▆</td></tr><tr><td>validation_loss</td><td>▇▁▄▆▃▅▂▁█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">iconic-sweep-4</strong>: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/18p8g63y\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/18p8g63y</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2wqatdlg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.021030479662296687\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoSpeciesBackprop: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: KLDiv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunsupervisedOnTest: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">crisp-sweep-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/2wqatdlg\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/2wqatdlg</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_135659-2wqatdlg</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': True, 'batchSize': 8, 'fc_layers': 3, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.021030479662296687, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f29a9bb40b45539c55a0eb6b174eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_KLDiv_Hyperp', 'modelName': 'models/ad7eb9898f8f8a484d8b4f5adefe7182b4e8d9282af2d75975038c76', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '4dc154c6161a60fe559a8675b1f798432b9c1e92c24213c889260910', 'trialHash': 'ad7eb9898f8f8a484d8b4f5adefe7182b4e8d9282af2d75975038c76', 'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': True, 'batchSize': 8, 'fc_layers': 3, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.021030479662296687, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 8/500 [1:26:02<90:28:25, 662.00s/it, min_val_loss=4.55e+3, train=0.0002, val=0.00019, val_loss=4.61]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   2%|▏         | 8/500 [1:26:31<88:40:56, 648.89s/it, min_val_loss=4.55e+3, train=0.0002, val=0.00019, val_loss=4.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33187<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_135659-2wqatdlg/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_135659-2wqatdlg/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.00969</td></tr><tr><td>batch</td><td>39999</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>loss_fine</td><td>0.00969</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0</td></tr><tr><td>_step</td><td>40007</td></tr><tr><td>_runtime</td><td>5167</td></tr><tr><td>_timestamp</td><td>1619119387</td></tr><tr><td>validation_fine_f1</td><td>0.00022</td></tr><tr><td>training_fine_f1</td><td>0.0002</td></tr><tr><td>test_fine_f1</td><td>0.0002</td></tr><tr><td>validation_loss</td><td>4.60517</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>▅▇▇▇▇▇▁▇▇▆▇▆▇▇▆▆▇▇▅▆▇▇█▇▆▇▇▇█▇▇▅▆▇▇▅▇█▇█</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>loss_fine</td><td>▅▇▇▇▇▇▁▇▇▆▇▆▇▇▆▆▇▇▅▆▇▇█▇▆▇▇▇█▇▇▅▆▇▇▅▇█▇█</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁▅█▄▂▂▂▃</td></tr><tr><td>training_fine_f1</td><td>█▅▁▅▇▇▇▆</td></tr><tr><td>validation_loss</td><td>▁▃▁▇█▅▇▄</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">crisp-sweep-5</strong>: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/2wqatdlg\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/2wqatdlg</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1qxxzc82 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00015353842799601297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoSpeciesBackprop: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: KLDiv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunsupervisedOnTest: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">radiant-sweep-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/1qxxzc82\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/1qxxzc82</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_152340-1qxxzc82</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': True, 'batchSize': 8, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.00015353842799601297, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e937059b8a4aca9e18595e03f0c821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_KLDiv_Hyperp', 'modelName': 'models/f941917b90cf67164cc01d01e2023a6552c09327bb8addc24eb83bb6', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '770ef5da74a3ae2dbade9085bf6c8cf399af85734ea066abc278f4de', 'trialHash': 'f941917b90cf67164cc01d01e2023a6552c09327bb8addc24eb83bb6', 'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': True, 'batchSize': 8, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.00015353842799601297, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   1%|          | 5/500 [53:56<88:34:22, 644.17s/it, min_val_loss=4.59, train=0.182, val=0.174, val_loss=4.57]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: Error 1040: Too many connections (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop. See /home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_152340-1qxxzc82/logs/debug-internal.log for full traceback.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error resolved after 0:00:45.144497, resuming normal operation.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop. See /home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_152340-1qxxzc82/logs/debug-internal.log for full traceback.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: Error 1040: Too many connections (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop. See /home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_152340-1qxxzc82/logs/debug-internal.log for full traceback.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error resolved after 0:00:40.897833, resuming normal operation.\n",
      "iteration:   1%|          | 6/500 [1:03:12<84:46:30, 617.79s/it, min_val_loss=4.59, train=0.152, val=0.148, val_loss=4.57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   1%|          | 6/500 [1:03:44<87:27:36, 637.36s/it, min_val_loss=4.59, train=0.152, val=0.148, val_loss=4.57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4847<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_152340-1qxxzc82/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_152340-1qxxzc82/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.00835</td></tr><tr><td>batch</td><td>29999</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>loss_fine</td><td>0.00835</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0</td></tr><tr><td>_step</td><td>30005</td></tr><tr><td>_runtime</td><td>3798</td></tr><tr><td>_timestamp</td><td>1619123218</td></tr><tr><td>validation_fine_f1</td><td>0.21781</td></tr><tr><td>training_fine_f1</td><td>0.15207</td></tr><tr><td>test_fine_f1</td><td>0.21104</td></tr><tr><td>validation_loss</td><td>4.57128</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▂▃▃▂▂▃▁▂▁▃▄▃▅▃▃▅▆▃▄▄▄▅▄▄▆▃▄▄▆▃▅▃▄▄▃▄▃▅▄</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▂▄▄▄▄▄▄▅▅▅▅▅▅▅▇▇▇▇▇▇███████</td></tr><tr><td>loss_fine</td><td>█▂▃▃▂▂▃▁▂▁▃▄▃▅▃▃▅▆▃▄▄▄▅▄▄▆▃▄▄▆▃▅▃▄▄▃▄▃▅▄</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>validation_fine_f1</td><td>█▅▅▄▄▁</td></tr><tr><td>training_fine_f1</td><td>█▅▄▄▄▁</td></tr><tr><td>validation_loss</td><td>▁▆▇█▄▅</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">radiant-sweep-6</strong>: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/1qxxzc82\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/1qxxzc82</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nza7ufb1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00017646943262504745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnoSpeciesBackprop: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: KLDiv\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tunsupervisedOnTest: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">likely-sweep-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/sweeps/tquv2ll1</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/nza7ufb1\" target=\"_blank\">https://wandb.ai/mndhamod/CIFAR_KLDiv_Hyperp/runs/nza7ufb1</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210422_162732-nza7ufb1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': True, 'batchSize': 8, 'fc_layers': 2, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.00017646943262504745, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0ac2fcf1d34b1d880ed830ad133f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_KLDiv_Hyperp', 'modelName': 'models/1d4400e9769755e40ae25bf0022c9b4a7006cf0a944defc32e9fbc2f', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '66f3539a6b90747edf7cdee48877e870880c1e99e252d46d94a93db0', 'trialHash': '1d4400e9769755e40ae25bf0022c9b4a7006cf0a944defc32e9fbc2f', 'adaptive_alpha': 0.5, 'adaptive_lambda': 0.001, 'adaptive_smoothing': False, 'augmented': True, 'batchSize': 8, 'fc_layers': 2, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.00017646943262504745, 'link_layer': 'avgpool', 'modelType': 'BB', 'noSpeciesBackprop': False, 'numOfTrials': 1, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'unsupervisedOnTest': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 8/500 [1:15:31<76:18:13, 558.32s/it, min_val_loss=5.98, train=0.174, val=0.163, val_loss=4.58]"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train, count=100)\n",
    "# !CUDA_VISIBLE_DEVICES=1 wandb agent k7cnzayi --count 100 #train.py from program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum([1.5041e-03, 1.2033e-03, 1.1173e-03, 1.5041e-03, 2.8363e-03, 1.3752e-03,\n",
    "#          1.3322e-03, 2.3636e-03, 2.2776e-03, 2.3636e-03, 2.2346e-03, 2.3206e-03,\n",
    "#          2.3206e-03, 2.4065e-03, 2.4065e-03, 1.9338e-03, 1.9768e-03, 3.8676e-04,\n",
    "#          5.1569e-04, 4.2974e-04, 3.4379e-04, 4.7271e-04, 0.0000e+00, 8.5948e-05,\n",
    "#          2.5784e-04, 4.7271e-04, 4.2974e-04, 4.2974e-04, 2.5784e-04, 4.7271e-04,\n",
    "#          3.4379e-04, 4.7271e-04, 1.4181e-03, 1.6330e-03, 1.5471e-03, 1.6760e-03,\n",
    "#          1.5041e-03, 3.0082e-04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "[x / 1362 for x in [30., 31., 29., 30., 61., 27., 26., 50., 48., 50., 47., 49., 49., 51.,\n",
    "         51., 40., 41., 40., 43., 41., 39., 42., 33., 33., 37., 42., 41., 41.,\n",
    "         37., 42., 39., 42.,  0.,  7.,  5.,  8.,  4., 36.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
