{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "device=0 # No other value works. known bug in Jupyter.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=JUPYTER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME=JUPYTER\n",
    "\n",
    "import wandb\n",
    "import math\n",
    "\n",
    "import os\n",
    "from myhelpers import TrialStatistics\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sweep params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep_config = {\n",
    "#     'method': 'random'\n",
    "# }\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'early_terminate': {\n",
    "       'type': 'hyperband',\n",
    "       'min_iter': 8   \n",
    "    }\n",
    "}\n",
    "metric = {\n",
    "    'name': 'test_fine_f1',\n",
    "    'goal': 'maximize'   \n",
    "    }\n",
    "sweep_config['metric'] = metric\n",
    "COUNT=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=\"HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\"\n",
    "params = {\n",
    "    \"image_path\":  {\n",
    "        'value':\"cifar-100-python\"#\"Curated4/Easy_50\"\n",
    "    },\n",
    "    \"suffix\":{\n",
    "        'value': None\n",
    "    },\n",
    "    \n",
    "    # dataset\n",
    "    \"img_res\": {\n",
    "        'value': 32 #448\n",
    "    }, \n",
    "    \"augmented\": {\n",
    "        'value': False\n",
    "    }, \n",
    "\n",
    "    # training\n",
    "    \"batchSize\": {\n",
    "          'value': 128 #[8, 16, 23, 64, 128, 256] # 8, 16, 64, 128\n",
    "        }, \n",
    "#     \"learning_rate\":{\n",
    "#         'distribution': 'log_uniform',\n",
    "#         'min': math.log(0.00005),\n",
    "#         'max': math.log(0.5),\n",
    "#       },\n",
    "    \"learning_rate\": { 'value': 0.001},\n",
    "    \"numOfTrials\":{\n",
    "        'value': 1\n",
    "    },\n",
    "    \"fc_layers\": {\n",
    "          'value': 1\n",
    "        },\n",
    "#     \"fc_layers\": {'value': 2},\n",
    "    \"modelType\":{\n",
    "        'value':\"BB\"\n",
    "    }, #BB DISCO DSN HGNN HGNN_add HGNN_cat\n",
    "#     \"lambda\": {\n",
    "#         'distribution': 'log_uniform',\n",
    "#         'min': math.log(0.005),\n",
    "#         'max': math.log(0.5),\n",
    "#       },\n",
    "    \"lambda\":{\n",
    "        'value': 0\n",
    "    },\n",
    "#     \"unsupervisedOnTest\": {\n",
    "#         'value': False\n",
    "#     },\n",
    "    \"tl_model\": {\n",
    "        'value': 'CIFAR'#'ResNet18'\n",
    "    },\n",
    "    \"link_layer\": {\n",
    "        'value': 'avgpool'\n",
    "    },\n",
    "#     \"link_layer\":  {\n",
    "#         'values': [\"avgpool\",\"layer3\"]\n",
    "#     }, \n",
    "    \n",
    "#     \"adaptive_smoothing\": {\n",
    "#         'values': [True,False]\n",
    "#     },\n",
    "    \"adaptive_smoothing\": {\n",
    "        'value': True\n",
    "    },\n",
    "#     \"adaptive_lambda\": {\n",
    "#         'value': 0.015\n",
    "#     },\n",
    "    \"adaptive_lambda\": {\n",
    "        'distribution': 'log_uniform',\n",
    "        'min': math.log(0.001),\n",
    "        'max': math.log(0.5),\n",
    "      }, \n",
    "    \"adaptive_alpha\": {\n",
    "        'min': 0.01,\n",
    "        'max': 0.99,\n",
    "      }, \n",
    "    \n",
    "    \"tripletEnabled\": {'value':True},\n",
    "    \"tripletSamples\": {'value': 2},\n",
    "    \"tripletSelector\": {'values': [\"semihard\", 'hard', 'random']},\n",
    "    \"tripletMargin\": {'values': [0.002, 0.02, 0.2, 2, 20]},\n",
    "#     \"adaptive_alpha\": {\n",
    "#         'value': 0.5\n",
    "#     },\n",
    "    \n",
    "#     \"noSpeciesBackprop\":{\n",
    "#         'values': [True,False]\n",
    "#     },\n",
    "#     \"noSpeciesBackprop\": {\n",
    "#         'value': False\n",
    "#     },\n",
    "    \"phylogeny_loss\":{\n",
    "        'value': False#[False, \"MSE\", \"KLDiv\"] # False, \"MSE\", \"KLDiv\"\n",
    "    },\n",
    "#     \"phylogeny_loss_epsilon\": {\n",
    "#         'distribution': 'log_uniform',\n",
    "#         'min': math.log(0.001),\n",
    "#         'max': math.log(1),\n",
    "#       }, \n",
    "    \"phylogeny_loss_epsilon\": {\n",
    "        'value': 0.03\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# python3 HGNN/code/HGNN/HGNN/train/train.py --cuda=6 --name=\"biology_paper_curated4_Medium_backprop_experiment\" -- -- --detailed\n",
    "\n",
    "experimentsPath = \"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = project\n",
    "\n",
    "def train(config=None):\n",
    "    run = wandb.init()\n",
    "\n",
    "#     try:\n",
    "    experiment_params=wandb.config if config is None else config\n",
    "    experiment_params = dict(experiment_params)\n",
    "    config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "    print(experiment_params)\n",
    "    experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "    # load images \n",
    "    experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "    if experiment_params['image_path'] == 'cifar-100-python':\n",
    "        datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "    else:\n",
    "        datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "    datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "    train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "    architecture = {\n",
    "        \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "        \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "    }\n",
    "\n",
    "    # Loop through n trials\n",
    "    for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "        modelName = getModelName(experiment_params, i)\n",
    "        trialName = os.path.join(experimentPathAndName, modelName)\n",
    "        trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "        row_information = {\n",
    "            'experimentName': experimentName,\n",
    "            'modelName': modelName,\n",
    "            'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "            'experimentHash': experimentHash,\n",
    "            'trialHash': trialHash\n",
    "        }\n",
    "        row_information = {**row_information, **experiment_params} \n",
    "        print(row_information)\n",
    "\n",
    "\n",
    "        # Train/Load model\n",
    "        model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "\n",
    "        if os.path.exists(CNN.getModelFile(trialName)):\n",
    "            print(\"Model {0} found!\".format(trialName))\n",
    "        else:\n",
    "            CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=True)\n",
    "\n",
    "    run.finish()\n",
    "#     except:\n",
    "#         print('terminated! Error:', sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call sweeper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can call with your own configs if you want:\n",
    "# train(config={...})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: q0gbehkn\n",
      "Sweep URL: https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\n"
     ]
    }
   ],
   "source": [
    "# Or call the sweeper and let it do the magic!\n",
    "sweep_id = wandb.sweep(sweep_config, project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a4qjcggv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.575701917398489\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.055949818864499096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: hard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">classic-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/a4qjcggv\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/a4qjcggv</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_021500-a4qjcggv</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.575701917398489, 'adaptive_lambda': 0.055949818864499096, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'hard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8561f247f2f4c019d3d8cbc50b2a5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/6e519725309dddbc0e19df34a017dd37d1184d0bda82ec82547c6f03', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '8fd8abde45ef4ad766e42fae75c0b7de7526561a408fdb16451df6e9', 'trialHash': '6e519725309dddbc0e19df34a017dd37d1184d0bda82ec82547c6f03', 'adaptive_alpha': 0.575701917398489, 'adaptive_lambda': 0.055949818864499096, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'hard'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   1%|          | 6/500 [37:44<51:46:54, 377.36s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]       \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 69913<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_021500-a4qjcggv/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_021500-a4qjcggv/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>6</td></tr><tr><td>validation_fine_f1</td><td>0.9989</td></tr><tr><td>training_fine_f1</td><td>0.99982</td></tr><tr><td>test_fine_f1</td><td>0.70869</td></tr><tr><td>validation_loss</td><td>3.63026</td></tr><tr><td>training_loss</td><td>3.62756</td></tr><tr><td>_runtime</td><td>2273</td></tr><tr><td>_timestamp</td><td>1620543173</td></tr><tr><td>_step</td><td>1904</td></tr><tr><td>loss</td><td>nan</td></tr><tr><td>batch</td><td>2190</td></tr><tr><td>loss_fine</td><td>nan</td></tr><tr><td>lambda_fine</td><td>nan</td></tr><tr><td>loss_layer2</td><td>nan</td></tr><tr><td>lambda_layer2</td><td>nan</td></tr><tr><td>nonzerotriplets_layer2</td><td>1</td></tr><tr><td>loss_layer3</td><td>nan</td></tr><tr><td>lambda_layer3</td><td>nan</td></tr><tr><td>nonzerotriplets_layer3</td><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▄▄▄▄▄▄▄▅▅▅▅▅▅▅▇▇▇▇▇▇███████</td></tr><tr><td>validation_fine_f1</td><td>▁█████</td></tr><tr><td>training_fine_f1</td><td>▁█████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇█▇█</td></tr><tr><td>validation_loss</td><td>█▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁      </td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁      </td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁      </td></tr><tr><td>loss_layer2</td><td>▆▇▄▄▄▃▁▂▁▁▃▃▁▄▃▅▃▂▄▃▂▂█▃▃▅▄▅▂▄▂▆▃▂      </td></tr><tr><td>lambda_layer2</td><td>█▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁▂▂▁▁▁      </td></tr><tr><td>nonzerotriplets_layer2</td><td>██████████████████████████████████▁▁▁▁▁▁</td></tr><tr><td>loss_layer3</td><td>▄▄▅▃▆▅▃▄▃▁▂▂▆▃▂▃▂▅▇▁▄▅█▃▅▁▂▇▅▃▂▆▃▃      </td></tr><tr><td>lambda_layer3</td><td>█▆▃▂▂▂▂▂▂▂▂▃▅▂▂▂▂▁▂▁▂▃▁▂▂▃▁▂▃▂▁▂▅▃      </td></tr><tr><td>nonzerotriplets_layer3</td><td>██████████████████████████████████▁▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">classic-sweep-1</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/a4qjcggv\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/a4qjcggv</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run a4qjcggv errored: ValueError('autodetected range of [nan, nan] is not finite',)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run a4qjcggv errored: ValueError('autodetected range of [nan, nan] is not finite',)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jxthkt5x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5916382609688451\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.0313152347675638\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: random\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">sparkling-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/jxthkt5x\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/jxthkt5x</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_025256-jxthkt5x</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5916382609688451, 'adaptive_lambda': 0.0313152347675638, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2a633517604594abe093246436c796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/96c4fe009388feef3ea7d714fc21ae404dd7aeba11111ffa613f5bc0', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '661a8e03c22e8151ed2751e32527ac9c7e94e7584b1f22a702c77c22', 'trialHash': '96c4fe009388feef3ea7d714fc21ae404dd7aeba11111ffa613f5bc0', 'adaptive_alpha': 0.5916382609688451, 'adaptive_lambda': 0.0313152347675638, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'random'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   3%|▎         | 16/500 [1:40:52<54:07:31, 402.59s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   3%|▎         | 16/500 [1:40:58<50:54:32, 378.66s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 953<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_025256-jxthkt5x/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_025256-jxthkt5x/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>15</td></tr><tr><td>validation_fine_f1</td><td>0.9994</td></tr><tr><td>training_fine_f1</td><td>0.99985</td></tr><tr><td>test_fine_f1</td><td>0.70836</td></tr><tr><td>validation_loss</td><td>3.62974</td></tr><tr><td>training_loss</td><td>3.62622</td></tr><tr><td>_runtime</td><td>6057</td></tr><tr><td>_timestamp</td><td>1620549233</td></tr><tr><td>_step</td><td>4770</td></tr><tr><td>loss</td><td>0.01262</td></tr><tr><td>batch</td><td>5007</td></tr><tr><td>loss_fine</td><td>0.01262</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr><tr><td>loss_layer2</td><td>2.5545</td></tr><tr><td>lambda_layer2</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer2</td><td>898</td></tr><tr><td>loss_layer3</td><td>2.44428</td></tr><tr><td>lambda_layer3</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer3</td><td>100</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>validation_fine_f1</td><td>▁███████████████</td></tr><tr><td>training_fine_f1</td><td>▁███████████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇███▇█████████</td></tr><tr><td>validation_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▃▂▃▂▂▄▁▂▁▂▁▁▁▂▃▂▂▂▂▁▂▁▁▂▂▁▁▁▂▁▂▁▂▁▁▁▂▁</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▄▃▂▃▂▂▄▁▂▁▂▁▁▁▂▃▂▂▂▂▁▂▁▁▂▂▁▁▁▂▁▂▁▂▁▁▁▂▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer2</td><td>▄▆▆▆▄▃▄▃▁█▅▄▄▄▄▃▄▂▇▃▆▇▆▁▅▄▅▄▂▄▆▇█▆▄▇▅▅▆▅</td></tr><tr><td>lambda_layer2</td><td>█▄▂▃▂▁▂▂▂▂▁▂▂▂▁▃▂▂▂▂▂▁▂▁▂▂▂▂▁▃▄▁▂▂▂▁▂▂▂▂</td></tr><tr><td>nonzerotriplets_layer2</td><td>▆▆▆▄▆▆▅▆█▆▅▆▃▄▇▆█▅▅▅▃▇▅▅▆█▄▅▁▇▆▆▆▅▅▄▂▅▂▆</td></tr><tr><td>loss_layer3</td><td>▅▅▅▇▅█▂▄▆▁▅▄▇▅▅▄▅▇▅▇▂▄▅▆▆▇▄▆▇▆█▇▇▃▄▅▆▃▇▆</td></tr><tr><td>lambda_layer3</td><td>█▂▁▁▂▄▂▂▆▁▄▂▃▃▅▁▃▃▂▁▂▁▂▅▂▂▂▂▂▂▂▂▂▅▁▅▄▃▂▃</td></tr><tr><td>nonzerotriplets_layer3</td><td>▁▆▆▃█▆▁█▆▆██▆▆▁▆█▃▆▆██▃▆▆█▃▆▆▆▁▆▁▆▆█▆█▃▃</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">sparkling-sweep-2</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/jxthkt5x\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/jxthkt5x</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vyx3wyi4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5559087615592129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.0011719150499296684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: semihard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">helpful-sweep-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/vyx3wyi4\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/vyx3wyi4</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_043403-vyx3wyi4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5559087615592129, 'adaptive_lambda': 0.0011719150499296684, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008432f9da944ea1a27dcf28747b9dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/8e97b8e53a4b05a893845e862e1e00f628f672faf7b965b24ced36d1', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '543fa8b963f2e302484b92ace9cee8cddb03da057f2e4c0dce76bdec', 'trialHash': '8e97b8e53a4b05a893845e862e1e00f628f672faf7b965b24ced36d1', 'adaptive_alpha': 0.5559087615592129, 'adaptive_lambda': 0.0011719150499296684, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 9/500 [57:23<52:10:41, 382.57s/it, min_val_loss=1, train=0.999, val=0.998, val_loss=3.64]   \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11029<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_043403-vyx3wyi4/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_043403-vyx3wyi4/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>validation_fine_f1</td><td>0.9978</td></tr><tr><td>training_fine_f1</td><td>0.99937</td></tr><tr><td>test_fine_f1</td><td>0.69982</td></tr><tr><td>validation_loss</td><td>3.63623</td></tr><tr><td>training_loss</td><td>3.63219</td></tr><tr><td>_runtime</td><td>3393</td></tr><tr><td>_timestamp</td><td>1620552636</td></tr><tr><td>_step</td><td>2861</td></tr><tr><td>loss</td><td>0.01846</td></tr><tr><td>batch</td><td>3129</td></tr><tr><td>loss_fine</td><td>0.03167</td></tr><tr><td>lambda_fine</td><td>0.55591</td></tr><tr><td>loss_layer2</td><td>0.00193</td></tr><tr><td>lambda_layer2</td><td>0.22016</td></tr><tr><td>nonzerotriplets_layer2</td><td>10</td></tr><tr><td>loss_layer3</td><td>0.0</td></tr><tr><td>lambda_layer3</td><td>0.22393</td></tr><tr><td>nonzerotriplets_layer3</td><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁████████</td></tr><tr><td>training_fine_f1</td><td>▁████████</td></tr><tr><td>test_fine_f1</td><td>▁▇█▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>█▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▃▂▂▁▁▁▁▂▁▁▁▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▃▂▂▁▁▁▁▂▁▁▁▂▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▂▁▃▁▁▁▁▁▁▁▁▁▁▂▁█▁▄▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer2</td><td>▅▅▆▄▆▇▆▆▆▆▄▄▇▅▃▅▆▅▆▅▄▆▆▁▅▆▃▆▅▅▅▆▅█▅▅▆▂▄▃</td></tr><tr><td>lambda_layer2</td><td>▁▃▃▃▃█▃▃▃▃▃▁▁▁▃█▃▃▃█▂▃█▃▁▁▃▃▃█████▃█▃██▃</td></tr><tr><td>nonzerotriplets_layer2</td><td>▇▃▅▅▅█▂▇█▆▅▅▃▂▆▇▃▅▄▇▆▅▂▁▃▇▄▃▄▄▆▇▅▅▆▃▅▂▃▂</td></tr><tr><td>loss_layer3</td><td>▁▁▁▁▂▁▁▁▁▂▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▃▁▂▁▁▁▁▃▁▁▁█▁▁▁</td></tr><tr><td>lambda_layer3</td><td>█▆▆▆▆▁▆▆▆▆▆███▆▁▆▆▆▁▆▆▁▆██▆▆▆▁▁▁▁▁▆▁▆▁▁▆</td></tr><tr><td>nonzerotriplets_layer3</td><td>▁▁▁▅▁▁▅▁▁▁▁█▁█▅▁▁▁▁▅▁▁▁▁▅▁▁▁▁█▁▁▁▁▅▁▁▅▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">helpful-sweep-3</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/vyx3wyi4\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/vyx3wyi4</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h6op66tz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.48373399250239435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.0014836571983863369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: semihard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">youthful-sweep-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/h6op66tz\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/h6op66tz</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_053136-h6op66tz</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.48373399250239435, 'adaptive_lambda': 0.0014836571983863369, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6bb5ed1a1b4ab8a5285fe1717f3fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/870122734bc95251d4636bb6ac4afe7f3a4ea953d5e5c936bbbc32a9', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '147b10f929948b27c4fbc4caf1e3bb05590818a2121eb2d9fdb18ebf', 'trialHash': '870122734bc95251d4636bb6ac4afe7f3a4ea953d5e5c936bbbc32a9', 'adaptive_alpha': 0.48373399250239435, 'adaptive_lambda': 0.0014836571983863369, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 12/500 [1:11:41<51:23:42, 379.14s/it, min_val_loss=1, train=0.999, val=0.996, val_loss=3.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   2%|▏         | 12/500 [1:11:47<48:39:18, 358.93s/it, min_val_loss=1, train=0.999, val=0.996, val_loss=3.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 16533<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_053136-h6op66tz/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_053136-h6op66tz/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>11</td></tr><tr><td>validation_fine_f1</td><td>0.99737</td></tr><tr><td>training_fine_f1</td><td>0.9993</td></tr><tr><td>test_fine_f1</td><td>0.69887</td></tr><tr><td>validation_loss</td><td>3.63964</td></tr><tr><td>training_loss</td><td>3.63436</td></tr><tr><td>_runtime</td><td>4306</td></tr><tr><td>_timestamp</td><td>1620557003</td></tr><tr><td>_step</td><td>3498</td></tr><tr><td>loss</td><td>0.01386</td></tr><tr><td>batch</td><td>3755</td></tr><tr><td>loss_fine</td><td>0.02795</td></tr><tr><td>lambda_fine</td><td>0.48375</td></tr><tr><td>loss_layer2</td><td>0.00191</td></tr><tr><td>lambda_layer2</td><td>0.17415</td></tr><tr><td>nonzerotriplets_layer2</td><td>5</td></tr><tr><td>loss_layer3</td><td>0.00196</td></tr><tr><td>lambda_layer3</td><td>0.34209</td></tr><tr><td>nonzerotriplets_layer3</td><td>2</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁███████████</td></tr><tr><td>training_fine_f1</td><td>▁███████████</td></tr><tr><td>test_fine_f1</td><td>▁▇██▇▇▇▇▇▇▇█</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▂▂▂▂</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▃▂▁▁▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▂▁▂▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▃▂▁▁▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂▁▁▂▁▂▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▄▃▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁█▁▁▁▄▁▂</td></tr><tr><td>loss_layer2</td><td>▆▅█▄▄▅▅▄▄▄▅▆▆▄▇▅▃▅▆▅▆▅▄▅▅▄▇▅▅▂▂▃▃▆▁▃▅▅▅▅</td></tr><tr><td>lambda_layer2</td><td>▃█▃▃▃▃█▁▃▃███▃▃▃▁▃▃▁▃▃█▁▁▃▃▃▁▃▃▁▃▇▃▃▃▃▃▃</td></tr><tr><td>nonzerotriplets_layer2</td><td>▅▅▂▂▄▄▅▇▁▅▃▄▃▅▄▂▄▅▅▇▃▄▄▂▄▅▃▄▆▄▃▂▃▅▄▂▄▄█▅</td></tr><tr><td>loss_layer3</td><td>▁▁▁▁▁▄▄▅▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>lambda_layer3</td><td>▆▁▆▆▆▆▁█▆▆▁▁▁▆▆▆█▆▆█▆▆▁██▆▆▆█▆▆█▆▁▆▆▆▆▆▆</td></tr><tr><td>nonzerotriplets_layer3</td><td>▁▁▁▁▁▁▁▁▅▁▁▁▁▁█▅▁▅▁▁▁█▁▁▁█▁▅▁▁▁▅▁▁▁█▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">youthful-sweep-4</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/h6op66tz\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/h6op66tz</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ue7xx4uh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5243650947746347\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.036562064503362676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: random\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ancient-sweep-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/ue7xx4uh\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/ue7xx4uh</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_064333-ue7xx4uh</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5243650947746347, 'adaptive_lambda': 0.036562064503362676, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ceef2f92ea6404ea8959b79b51335ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/98b6f2378d05cc83e224889f0898127e00b256895864ba6f6f487d9e', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': 'f208776e17e37981981bfddc7572f69116c0f02f3b47ef3d821d38c1', 'trialHash': '98b6f2378d05cc83e224889f0898127e00b256895864ba6f6f487d9e', 'adaptive_alpha': 0.5243650947746347, 'adaptive_lambda': 0.036562064503362676, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   3%|▎         | 15/500 [1:36:34<54:22:59, 403.67s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]    /home/elhamod/projects/myhelpers/myhelpers/tripletloss.py:134: RuntimeWarning: invalid value encountered in greater\n",
      "  hard_negatives = np.where(loss_values > 0)[0]\n",
      "iteration:   3%|▎         | 15/500 [1:42:05<55:01:13, 408.40s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23647<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_064333-ue7xx4uh/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_064333-ue7xx4uh/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>15</td></tr><tr><td>validation_fine_f1</td><td>0.9993</td></tr><tr><td>training_fine_f1</td><td>0.99982</td></tr><tr><td>test_fine_f1</td><td>0.70896</td></tr><tr><td>validation_loss</td><td>3.6297</td></tr><tr><td>training_loss</td><td>3.62622</td></tr><tr><td>_runtime</td><td>6131</td></tr><tr><td>_timestamp</td><td>1620563144</td></tr><tr><td>_step</td><td>4766</td></tr><tr><td>loss</td><td>nan</td></tr><tr><td>batch</td><td>5007</td></tr><tr><td>loss_fine</td><td>nan</td></tr><tr><td>lambda_fine</td><td>nan</td></tr><tr><td>loss_layer2</td><td>nan</td></tr><tr><td>lambda_layer2</td><td>nan</td></tr><tr><td>nonzerotriplets_layer2</td><td>1</td></tr><tr><td>loss_layer3</td><td>nan</td></tr><tr><td>lambda_layer3</td><td>nan</td></tr><tr><td>nonzerotriplets_layer3</td><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>validation_fine_f1</td><td>▁██████████████</td></tr><tr><td>training_fine_f1</td><td>▁██████████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇██▇█▇█▇█████</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▆▄█▃▄▄▄▃▃▃▂▇▃▃▂▂▄▂▂▃▄▂▂▃▃▄▂▃▁▂▂▂▂▁▂▃▆▂▃ </td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>▆▄█▃▄▄▄▃▃▃▂▇▃▃▂▂▄▂▂▃▄▂▂▃▃▄▂▃▁▂▂▂▂▁▂▃▆▂▃ </td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ </td></tr><tr><td>loss_layer2</td><td>▆▆▆▁▅▅▅▇▇▆▇█▅▆▄▆▇▅▄▇█▅▆▆▇▇▇█▄▇▃▇▄▃▆▆▇▆▄ </td></tr><tr><td>lambda_layer2</td><td>▆▂▂▆▃█▄▂▄▇▆█▂▂▃▂▄▃▁▆▃█▃▄▃█▁▅▆▃▁▄▅▅▂▄▇▃▂ </td></tr><tr><td>nonzerotriplets_layer2</td><td>███████████████████████████████████████▁</td></tr><tr><td>loss_layer3</td><td>▅▄▆█▁▁▁▅▃▅▆▃▄▇▁▅▇▇▄▇▄▂▆▇▃▅▆▆▅▂▄▅▇▅▆▃▃▃▅ </td></tr><tr><td>lambda_layer3</td><td>▁▄▃▁▂▁▁▅▃▃▂▃▄▁▃▂▂▃▅▃▇▃▂▄▂▂▁▂▁▁▄▂█▁▁▁▃▅▁ </td></tr><tr><td>nonzerotriplets_layer3</td><td>███████████████████████████████████████▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ancient-sweep-5</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/ue7xx4uh\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/ue7xx4uh</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run ue7xx4uh errored: ValueError('autodetected range of [nan, nan] is not finite',)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ue7xx4uh errored: ValueError('autodetected range of [nan, nan] is not finite',)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: do5bul1z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.1255494121468659\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.36788517104673696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: semihard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">upbeat-sweep-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/do5bul1z\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/do5bul1z</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_082547-do5bul1z</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.1255494121468659, 'adaptive_lambda': 0.36788517104673696, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.2, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938cb8eb95054fc195f5a27b182835ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/3eadbc3c55ed842b48815306de097a68d9668ede288884ac113027e2', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': 'cf512d835d298825420734e92285bda184324b93c60bbc7bc1face9f', 'trialHash': '3eadbc3c55ed842b48815306de097a68d9668ede288884ac113027e2', 'adaptive_alpha': 0.1255494121468659, 'adaptive_lambda': 0.36788517104673696, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.2, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 9/500 [50:21<45:47:35, 335.75s/it, min_val_loss=1, train=0.999, val=0.997, val_loss=3.64]   \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 34041<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_082547-do5bul1z/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_082547-do5bul1z/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>validation_fine_f1</td><td>0.99662</td></tr><tr><td>training_fine_f1</td><td>0.99945</td></tr><tr><td>test_fine_f1</td><td>0.70074</td></tr><tr><td>validation_loss</td><td>3.63673</td></tr><tr><td>training_loss</td><td>3.63237</td></tr><tr><td>_runtime</td><td>3027</td></tr><tr><td>_timestamp</td><td>1620566174</td></tr><tr><td>_step</td><td>2649</td></tr><tr><td>loss</td><td>0.10214</td></tr><tr><td>batch</td><td>2921</td></tr><tr><td>loss_fine</td><td>0.01269</td></tr><tr><td>lambda_fine</td><td>0.50559</td></tr><tr><td>loss_layer2</td><td>0.19373</td></tr><tr><td>lambda_layer2</td><td>0.24717</td></tr><tr><td>nonzerotriplets_layer2</td><td>445</td></tr><tr><td>loss_layer3</td><td>0.19338</td></tr><tr><td>lambda_layer3</td><td>0.24723</td></tr><tr><td>nonzerotriplets_layer3</td><td>59</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>validation_fine_f1</td><td>▁████████</td></tr><tr><td>training_fine_f1</td><td>▁████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇█████▇</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▂▄▂▁▂▁▁▂▁▁▁▁▂▁▂▁▁▁▂▁▁▁▁▂▂▂▂▂▂▁▁▁▁▁▂▂▂▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▄▂▃▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>lambda_fine</td><td>▁▅▇▆▇█▇██▇██████████▇████▇▇▇▇▇████████▇█</td></tr><tr><td>loss_layer2</td><td>▅▄▄▅▆▄▅▅▅▃▆▃▄▃▂▄▇▆▆▇▆▃▃▅▄█▅█▃█▁▃▇▇▄▃█▆█▄</td></tr><tr><td>lambda_layer2</td><td>█▄▂▃▂▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>▄▅▄▅▆▅▅▆▅▄▆▇▆▄▅▅▆▄█▆▇▇▁▄▇▅▆▅▂▅▄▃▇▆▆▇▇▅▅▇</td></tr><tr><td>loss_layer3</td><td>▇█▁▅▁▅▅▅▆▄▅▄▄▄▄▄▆▂▅▄▃▄▆▅▅▂▂▄▁▅▂▇▆▅▅▄▁▄▇▂</td></tr><tr><td>lambda_layer3</td><td>█▄▂▃▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>nonzerotriplets_layer3</td><td>▃▆▃▇▆▇▄▆▂█▅▅▁▇▅▆▄▅█▄▅▅▃▃▆▇▂█▃▆▃▅▅▄▅▄▆▆▄▃</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">upbeat-sweep-6</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/do5bul1z\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/do5bul1z</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6o43gjx6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.18383265522545014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.21119828442273292\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: hard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">visionary-sweep-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/6o43gjx6\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/6o43gjx6</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_091619-6o43gjx6</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.18383265522545014, 'adaptive_lambda': 0.21119828442273292, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 20, 'tripletSamples': 2, 'tripletSelector': 'hard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7435eacd55c4cceaffd02592f21aba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/023344225fd070ba22dc5d587ff6ea07a9e5077d5593984077eeeb83', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': 'd32406dcb3c25a8b73a38ac7b1922f2bb23f7204ba908b4fe789d063', 'trialHash': '023344225fd070ba22dc5d587ff6ea07a9e5077d5593984077eeeb83', 'adaptive_alpha': 0.18383265522545014, 'adaptive_lambda': 0.21119828442273292, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 20, 'tripletSamples': 2, 'tripletSelector': 'hard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   4%|▍         | 21/500 [1:40:35<39:52:06, 299.64s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   4%|▍         | 21/500 [1:40:40<38:16:14, 287.63s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40115<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_091619-6o43gjx6/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_091619-6o43gjx6/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>20</td></tr><tr><td>validation_fine_f1</td><td>0.9994</td></tr><tr><td>training_fine_f1</td><td>0.99985</td></tr><tr><td>test_fine_f1</td><td>0.70708</td></tr><tr><td>validation_loss</td><td>3.62972</td></tr><tr><td>training_loss</td><td>3.626</td></tr><tr><td>_runtime</td><td>6042</td></tr><tr><td>_timestamp</td><td>1620572221</td></tr><tr><td>_step</td><td>6360</td></tr><tr><td>loss</td><td>0.01096</td></tr><tr><td>batch</td><td>6572</td></tr><tr><td>loss_fine</td><td>0.01096</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr><tr><td>loss_layer2</td><td>21.93409</td></tr><tr><td>lambda_layer2</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer2</td><td>900</td></tr><tr><td>loss_layer3</td><td>21.55553</td></tr><tr><td>lambda_layer3</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer3</td><td>100</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁████████████████████</td></tr><tr><td>training_fine_f1</td><td>▁████████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇▇▇▇▇▇████▇▇█▇█████</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▆█▃▂▂▅▄▃▂▂▃▃▂▂▄▂▂▃▂▂▁▁▂▃▁▁▂▂▂▂▂▄▁▁▁▂▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>▆█▃▂▂▅▄▃▂▂▃▃▂▂▄▂▂▃▂▂▁▁▂▃▁▁▂▂▂▂▂▄▁▁▁▂▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer2</td><td>▄▃▃▅▆▅▃▂▄▁▆▂▅▅▄▆▅▇▅▁▂▂▂▄▅▆▆▇▆▆█▅▇▇▆▅▃▆▂▂</td></tr><tr><td>lambda_layer2</td><td>▃▄▆▅▅▆▂▄▃▁▃▃▅▂▂▃▃▂▂▄▄▂▃▄▃▂▃▂▂▅▅▄▅▂▂▁█▂▃▆</td></tr><tr><td>nonzerotriplets_layer2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer3</td><td>▅▄▃▅▇▂▄▃▄▃▃▅▅▅▃▁▄▃▂▃▅▅▃█▅█▁▂▄▁▆▅▃▇▄█▃▇▄▇</td></tr><tr><td>lambda_layer3</td><td>▅▇▅▄▅▆▆█▅▄▄▆▄▇█▆▆▆▆▇▅▄▄▇▄▇▅▁▆▆▆▆▅▅▃▄█▂▄▆</td></tr><tr><td>nonzerotriplets_layer3</td><td>█████████████████████████████████████▁██</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">visionary-sweep-7</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/6o43gjx6\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/6o43gjx6</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 741nppc8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.33872548646378897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.0010225820291841605\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: hard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">graceful-sweep-8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/741nppc8\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/741nppc8</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_105709-741nppc8</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.33872548646378897, 'adaptive_lambda': 0.0010225820291841605, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 20, 'tripletSamples': 2, 'tripletSelector': 'hard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a5b7a6cefa49c0afb67c90248edd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/1611bcb8e7942c934236ac7e5d3566381bf4957a50fdc82bf4230222', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '57228dc24934fb97e6fcd3dc2a7e68a766ddaf88f692775b27cf5864', 'trialHash': '1611bcb8e7942c934236ac7e5d3566381bf4957a50fdc82bf4230222', 'adaptive_alpha': 0.33872548646378897, 'adaptive_lambda': 0.0010225820291841605, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 20, 'tripletSamples': 2, 'tripletSelector': 'hard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   5%|▌         | 27/500 [2:11:14<39:46:47, 302.76s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   5%|▌         | 27/500 [2:11:18<38:20:23, 291.80s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 49751<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_105709-741nppc8/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_105709-741nppc8/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>26</td></tr><tr><td>validation_fine_f1</td><td>0.9994</td></tr><tr><td>training_fine_f1</td><td>0.99985</td></tr><tr><td>test_fine_f1</td><td>0.70832</td></tr><tr><td>validation_loss</td><td>3.62999</td></tr><tr><td>training_loss</td><td>3.62596</td></tr><tr><td>_runtime</td><td>7880</td></tr><tr><td>_timestamp</td><td>1620580109</td></tr><tr><td>_step</td><td>8268</td></tr><tr><td>loss</td><td>0.0268</td></tr><tr><td>batch</td><td>8450</td></tr><tr><td>loss_fine</td><td>0.0268</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr><tr><td>loss_layer2</td><td>21.72493</td></tr><tr><td>lambda_layer2</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer2</td><td>900</td></tr><tr><td>loss_layer3</td><td>21.57533</td></tr><tr><td>lambda_layer3</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer3</td><td>100</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>validation_fine_f1</td><td>▁██████████████████████████</td></tr><tr><td>training_fine_f1</td><td>▁██████████████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇▇▇▇▇█████▇██████████████</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▅▅▆▄▅▃▂▃▁▃▂▂▅▂▃▂█▂▅▃█▃▃▂▃▂▂▄▃▁▃▂▂▆▂▂▅</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▅▄▅▅▆▄▅▃▂▃▁▃▂▂▅▂▃▂█▂▅▃█▃▃▂▃▂▂▄▃▁▃▂▂▆▂▂▅</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer2</td><td>▄▆▄█▅▅▃▂▃▁▃▅▅▆▃▄▃▄█▆▄▆▂▆▃▄▁▃▂▅▃▃▃▇▅▂▄▆▆▃</td></tr><tr><td>lambda_layer2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer3</td><td>▄▇▄▅▇▁▅█▆▃▄▁▄█▃▄▇▆▆▃▅▅▄▆▆▆▄▇▆▃█▆▆▇▂▆▅▂▆▆</td></tr><tr><td>lambda_layer3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer3</td><td>██████████████████████████▁█████████████</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">graceful-sweep-8</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/741nppc8\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/741nppc8</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yn5nnkv0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.6585883802353443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.22458648625453892\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: random\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">zesty-sweep-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/yn5nnkv0\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/yn5nnkv0</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_130836-yn5nnkv0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.6585883802353443, 'adaptive_lambda': 0.22458648625453892, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5565fb068bd2444c9b9d35780590e487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/020920cc8843b4c4bd04a90c1b683fd55a9406805c0973ddf93b11ff', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '8b1d4a9aa07c85860d7f798f0308a1c3ca3a581aca95602ddf910b88', 'trialHash': '020920cc8843b4c4bd04a90c1b683fd55a9406805c0973ddf93b11ff', 'adaptive_alpha': 0.6585883802353443, 'adaptive_lambda': 0.22458648625453892, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 11/500 [53:37<42:07:44, 310.15s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   2%|▏         | 11/500 [53:41<39:46:57, 292.88s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 62346<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_130836-yn5nnkv0/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_130836-yn5nnkv0/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>10</td></tr><tr><td>validation_fine_f1</td><td>0.99919</td></tr><tr><td>training_fine_f1</td><td>0.99982</td></tr><tr><td>test_fine_f1</td><td>0.70564</td></tr><tr><td>validation_loss</td><td>3.62986</td></tr><tr><td>training_loss</td><td>3.62666</td></tr><tr><td>_runtime</td><td>3223</td></tr><tr><td>_timestamp</td><td>1620583339</td></tr><tr><td>_step</td><td>3180</td></tr><tr><td>loss</td><td>0.07723</td></tr><tr><td>batch</td><td>3442</td></tr><tr><td>loss_fine</td><td>0.04121</td></tr><tr><td>lambda_fine</td><td>0.98565</td></tr><tr><td>loss_layer2</td><td>2.65772</td></tr><tr><td>lambda_layer2</td><td>0.00649</td></tr><tr><td>nonzerotriplets_layer2</td><td>898</td></tr><tr><td>loss_layer3</td><td>2.41277</td></tr><tr><td>lambda_layer3</td><td>0.00786</td></tr><tr><td>nonzerotriplets_layer3</td><td>99</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁██████████</td></tr><tr><td>training_fine_f1</td><td>▁██████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇▇█▇█████</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▂▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▂▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▅▅▆▅▅▅▅▇▆▅▅▆▇▅▆▆▇▆▇▆█▆▅▆▇▆▇▇▆▅▅▇█▅▆▆▆▅▆</td></tr><tr><td>loss_layer2</td><td>▆▅▆▃▄▇▃▃▇▃▂▂▁▅▅▄▄▆▂▃▃▂▆▅▂▃█▅▇█▇▅▅▆▇▃▄▅▃▇</td></tr><tr><td>lambda_layer2</td><td>█▃▃▅▃▆▅▅▄▄▂▅▂▃▄▁▄▄▃▃▄▃▃▅▄▂▅▃▂▃▃▅▄▁▅▁▃▆▃▂</td></tr><tr><td>nonzerotriplets_layer2</td><td>▄▅▄▃▅▆▅▇▁▅▄▇▅▇▅▄▆▅▆▄▅▃▇▆▆▆▁▅▄▃▆▂▅█▅▆▆▅▆▅</td></tr><tr><td>loss_layer3</td><td>▄▁▅▅▃▃▄▅▅▅▄▄▆▄▆▅▅▃▅▃▅▅▄▂▃▃▅▄▄▆▆▃▅█▄▃▆▅▃▄</td></tr><tr><td>lambda_layer3</td><td>█▄▄▃▄▄▃▃▁▃▄▄▃▂▅▄▃▂▄▃▃▁▃▃▃▃▂▂▂▄▄▅▂▁▅▄▃▂▄▄</td></tr><tr><td>nonzerotriplets_layer3</td><td>▆█▃██▅▅▆▅▃▆▆▆▆▅▆▆▆▆▃█▆▆▅▆█▅▆█▆▃▆▆▅▅█▃▃▁█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">zesty-sweep-9</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/yn5nnkv0\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/yn5nnkv0</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xqmmoq2p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.9470465239388136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.08298819933028323\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: semihard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">divine-sweep-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/xqmmoq2p\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/xqmmoq2p</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_140228-xqmmoq2p</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.9470465239388136, 'adaptive_lambda': 0.08298819933028323, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236334035872471d97813ed21de47de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/edcd47ffccf6f8a7821b40dc50b627fe763b2a1a1560c5dfa8f36f90', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '1d35d133a2e874e213019d1c1b74845cf947ebf16faa018fef643f4a', 'trialHash': 'edcd47ffccf6f8a7821b40dc50b627fe763b2a1a1560c5dfa8f36f90', 'adaptive_alpha': 0.9470465239388136, 'adaptive_lambda': 0.08298819933028323, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   8%|▊         | 39/500 [3:46:15<41:43:35, 325.85s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   8%|▊         | 39/500 [3:46:20<44:35:30, 348.22s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 68438<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_140228-xqmmoq2p/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_140228-xqmmoq2p/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>38</td></tr><tr><td>validation_fine_f1</td><td>0.9994</td></tr><tr><td>training_fine_f1</td><td>0.99985</td></tr><tr><td>test_fine_f1</td><td>0.71046</td></tr><tr><td>validation_loss</td><td>3.63089</td></tr><tr><td>training_loss</td><td>3.62611</td></tr><tr><td>_runtime</td><td>13581</td></tr><tr><td>_timestamp</td><td>1620596930</td></tr><tr><td>_step</td><td>12084</td></tr><tr><td>loss</td><td>0.01463</td></tr><tr><td>batch</td><td>12206</td></tr><tr><td>loss_fine</td><td>0.01509</td></tr><tr><td>lambda_fine</td><td>0.9646</td></tr><tr><td>loss_layer2</td><td>0.00194</td></tr><tr><td>lambda_layer2</td><td>0.0177</td></tr><tr><td>nonzerotriplets_layer2</td><td>8</td></tr><tr><td>loss_layer3</td><td>0.00194</td></tr><tr><td>lambda_layer3</td><td>0.0177</td></tr><tr><td>nonzerotriplets_layer3</td><td>2</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>validation_fine_f1</td><td>▁██████████████████████████████████████</td></tr><tr><td>training_fine_f1</td><td>▁██████████████████████████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇▇█▇▇██▇▇█▇▇████▇████▇███████████████</td></tr><tr><td>validation_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▃█▆▁▃▂▃▁▁▂▂▂▄▂▂▁▁▁▁▁▁▁▂▁▂▂▂▂▃▁▄▂▂▂▁▁▂▃▂▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>▄██▂▃▂▄▁▁▂▂▃▂▂▂▁▁▂▁▂▁▁▂▁▂▂▂▁▁▂▂▂▂▂▁▁▂▂▂▁</td></tr><tr><td>lambda_fine</td><td>▁▆▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▅▁█▁▁▁▁▁▁▄▁▁</td></tr><tr><td>loss_layer2</td><td>▄▅▄▆▄█▁▄▆▆▅▅▆▃▃▂▅▃▃▃▇▄▄▆█▃▅▃█▆▇▇▄▅▄▄▅▄▅▅</td></tr><tr><td>lambda_layer2</td><td>▁▆▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▅▁█▁▁▁▁▁▁▃▁▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>▆▆█▅▃▃▃▆▇▃▄▆▆▅▂▅▆▄▅▅▄▆▄▅▁▃▃▃▁▅▃▃▃▃▂▂▄▅▄▆</td></tr><tr><td>loss_layer3</td><td>▂▁▁▁▁▄▁▁▁▁▁▅▁▁█▆▁▂▁▁▃▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_layer3</td><td>█▃██████████▁██████████████▆▄█▁██████▆██</td></tr><tr><td>nonzerotriplets_layer3</td><td>▁▁▁▁▅▁▅▅▁▁▁▁▁▅▁▁▁▁▁█▁▁▁█▁▅▁▁▅▁▁▁█▁▅▅▁▁█▅</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">divine-sweep-10</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/xqmmoq2p\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/xqmmoq2p</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p6iqyctt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.36842674515143464\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.0014439420438298544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: semihard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">elated-sweep-11</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/p6iqyctt\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/p6iqyctt</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_174858-p6iqyctt</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.36842674515143464, 'adaptive_lambda': 0.0014439420438298544, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.2, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8089e92fbfe44851b5a394cad5059740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/d23a4f1a68e2c3950e4e073b706bbcc0b922437df962c0d99aba92b9', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '53ea7d667d21cce512b0331fa24edff2620074c300075d4a682fc11d', 'trialHash': 'd23a4f1a68e2c3950e4e073b706bbcc0b922437df962c0d99aba92b9', 'adaptive_alpha': 0.36842674515143464, 'adaptive_lambda': 0.0014439420438298544, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.2, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 9/500 [47:07<42:50:30, 314.11s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]       \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 61404<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_174858-p6iqyctt/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_174858-p6iqyctt/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>validation_fine_f1</td><td>0.99909</td></tr><tr><td>training_fine_f1</td><td>0.99982</td></tr><tr><td>test_fine_f1</td><td>0.70621</td></tr><tr><td>validation_loss</td><td>3.62982</td></tr><tr><td>training_loss</td><td>3.62689</td></tr><tr><td>_runtime</td><td>2832</td></tr><tr><td>_timestamp</td><td>1620599770</td></tr><tr><td>_step</td><td>2601</td></tr><tr><td>loss</td><td>0.00613</td></tr><tr><td>batch</td><td>2873</td></tr><tr><td>loss_fine</td><td>0.00613</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr><tr><td>loss_layer2</td><td>0.19365</td></tr><tr><td>lambda_layer2</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer2</td><td>522</td></tr><tr><td>loss_layer3</td><td>0.19362</td></tr><tr><td>lambda_layer3</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer3</td><td>54</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>validation_fine_f1</td><td>▁████████</td></tr><tr><td>training_fine_f1</td><td>▁████████</td></tr><tr><td>test_fine_f1</td><td>▁▇███████</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁███████████████████████████████████████</td></tr><tr><td>loss_layer2</td><td>▄▅▆▆▆▆▂▆▅▁▅▄▆▄▅▄▂▅▅▇▅▄▃▄▆▃▅▃▇▄▃▃▅█▄▄▃▄▅▃</td></tr><tr><td>lambda_layer2</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>█▅▅▅▆▄▅▅▂▅▅▅▇▆▃▆▅▆▄▄▅▅▅▇█▆▅▆▇▃▅▁▄▅▆▅▄▅▆▃</td></tr><tr><td>loss_layer3</td><td>▁▇▅▆▅█▆▃▃▆▄▆▆▂▄▄▇▆▆█▇▄▄▇▅▅▁▄█▅▃▄▇▅▇▇▅█▅▂</td></tr><tr><td>lambda_layer3</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer3</td><td>▄▆▄▃▅▂▄▃▃█▅▂▁▃▃▄▅▄▄▅▃▃▂▅▅▃▁▂▁▃▄▂▂▁▃▁▅▃▃▃</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">elated-sweep-11</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/p6iqyctt\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/p6iqyctt</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bxkd3ncv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.2991798801739154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.005391977527006105\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: random\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">amber-sweep-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/bxkd3ncv\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/bxkd3ncv</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_183616-bxkd3ncv</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.2991798801739154, 'adaptive_lambda': 0.005391977527006105, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9335b664a1584dac8d1fc09236cb81dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/5652c32f0f0993d04a5abe08aad3397875b60c368607e27d38d2f0fa', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': 'e8a4cbf5b7c067ba1fe05254f18a3cc9387e4212ad94018fa76391ce', 'trialHash': '5652c32f0f0993d04a5abe08aad3397875b60c368607e27d38d2f0fa', 'adaptive_alpha': 0.2991798801739154, 'adaptive_lambda': 0.005391977527006105, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 10/500 [54:53<44:49:26, 329.32s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 72498<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_183616-bxkd3ncv/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_183616-bxkd3ncv/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>10</td></tr><tr><td>validation_fine_f1</td><td>0.9992</td></tr><tr><td>training_fine_f1</td><td>0.99982</td></tr><tr><td>test_fine_f1</td><td>0.70661</td></tr><tr><td>validation_loss</td><td>3.6297</td></tr><tr><td>training_loss</td><td>3.62664</td></tr><tr><td>_runtime</td><td>3298</td></tr><tr><td>_timestamp</td><td>1620603074</td></tr><tr><td>_step</td><td>3005</td></tr><tr><td>loss</td><td>0.00737</td></tr><tr><td>batch</td><td>3272</td></tr><tr><td>loss_fine</td><td>0.00737</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr><tr><td>loss_layer2</td><td>0.67266</td></tr><tr><td>lambda_layer2</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer2</td><td>894</td></tr><tr><td>loss_layer3</td><td>0.49798</td></tr><tr><td>lambda_layer3</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer3</td><td>96</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>validation_fine_f1</td><td>▁█████████</td></tr><tr><td>training_fine_f1</td><td>▁█████████</td></tr><tr><td>test_fine_f1</td><td>▁▇█▇██████</td></tr><tr><td>validation_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▃▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▃▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer2</td><td>▃▁▄▃▅▃▂▂▂▃▃▃▅▃▃▄▄▅▇▂▂▃▂▃▆▃▃▅▅▄▅▃▄▃█▂▄▄▂▄</td></tr><tr><td>lambda_layer2</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>▆▆▇▄▄▆▆▆▄▃▆▃▅▇█▆▄▄▇▆▅▅▁▅▅▅▅▅▅▄▇▆▃▄▅▇▄▇▆▆</td></tr><tr><td>loss_layer3</td><td>▆▄▁▆▆▂▃▃▅▄▆▅▅▅▄▄▃▄▁▂▃▆█▅▃▆▆▄▆▄▄▂▄▆▅▃▁▄▅▄</td></tr><tr><td>lambda_layer3</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer3</td><td>▆▅▆█▁▇█▇▆▅▇▃▇▇▃▆█▆▆▅█▆▅▅▃▆▅▇▅▅▇▇▇▆▆▇▃▆▆▃</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">amber-sweep-12</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/bxkd3ncv\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/bxkd3ncv</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0g7tak3h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.10748946786056952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.010414185653661479\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: random\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">misunderstood-sweep-13</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/0g7tak3h\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/0g7tak3h</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_193121-0g7tak3h</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.10748946786056952, 'adaptive_lambda': 0.010414185653661479, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd17c3a0c134d0f9bc8f796bc3bcf16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/2fa4370359d7aa1b301affedc67a48dcd78ed6d32f1abf0bccaeb8ad', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '0907f162d97ca44f4ea476a84a12b78b08398fe2b2282c9c96551e75', 'trialHash': '2fa4370359d7aa1b301affedc67a48dcd78ed6d32f1abf0bccaeb8ad', 'adaptive_alpha': 0.10748946786056952, 'adaptive_lambda': 0.010414185653661479, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   3%|▎         | 13/500 [1:08:01<44:55:36, 332.11s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   3%|▎         | 13/500 [1:08:05<42:30:55, 314.28s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10616<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_193121-0g7tak3h/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_193121-0g7tak3h/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>12</td></tr><tr><td>validation_fine_f1</td><td>0.9994</td></tr><tr><td>training_fine_f1</td><td>0.99982</td></tr><tr><td>test_fine_f1</td><td>0.70687</td></tr><tr><td>validation_loss</td><td>3.62969</td></tr><tr><td>training_loss</td><td>3.62639</td></tr><tr><td>_runtime</td><td>4087</td></tr><tr><td>_timestamp</td><td>1620607168</td></tr><tr><td>_step</td><td>3816</td></tr><tr><td>loss</td><td>0.01547</td></tr><tr><td>batch</td><td>4068</td></tr><tr><td>loss_fine</td><td>0.01547</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr><tr><td>loss_layer2</td><td>0.64489</td></tr><tr><td>lambda_layer2</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer2</td><td>894</td></tr><tr><td>loss_layer3</td><td>0.74572</td></tr><tr><td>lambda_layer3</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer3</td><td>96</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>validation_fine_f1</td><td>▁████████████</td></tr><tr><td>training_fine_f1</td><td>▁████████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇█▇██▇█████</td></tr><tr><td>validation_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer2</td><td>▃▄▃██▄▆▆▅▆▅▅▄▄▆▄▂▁▅▃▁▅▃▃▁▄▅▄▅▄▄▄▅▅▅▄▇▄▅▇</td></tr><tr><td>lambda_layer2</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>█▃▃▃▆▅▇▂▅▇▅▃▆▃█▅▅▃▂▃▄▄▄▅▄▅▆▅▅▁▅▃▆▅▇▆▅▅▇▃</td></tr><tr><td>loss_layer3</td><td>▆█▄▇▄▆▅▄▅▆█▄▅▆▁▅▃▄▆▅▅▅▄▄▆▆▇▇▅▃▇▆▅▄▂▅█▇▇▂</td></tr><tr><td>lambda_layer3</td><td>▃▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer3</td><td>▆█▃▆▆▅▅▅▃▃▆▆▅▃█▅█▃█▆▆▆▃▆█▃▁█▆▃▁▆▃▆▅▃▅▅▅▆</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">misunderstood-sweep-13</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/0g7tak3h\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/0g7tak3h</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: utjtr84q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.6333086501154143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.019259499718624933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: semihard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">major-sweep-14</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/utjtr84q\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/utjtr84q</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_203936-utjtr84q</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.6333086501154143, 'adaptive_lambda': 0.019259499718624933, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 20, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7610fe4ba4194b348578abab4949d6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/533eea63875aa7f7b376b299eee2ce0120f048198bf111e8901233c7', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '2a5692a773710d7dc3285c26b1350c3d44a31a27e63a824d78418d7f', 'trialHash': '533eea63875aa7f7b376b299eee2ce0120f048198bf111e8901233c7', 'adaptive_alpha': 0.6333086501154143, 'adaptive_lambda': 0.019259499718624933, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 20, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   3%|▎         | 14/500 [1:13:04<44:19:48, 328.37s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   3%|▎         | 14/500 [1:13:08<42:19:06, 313.47s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 24010<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_203936-utjtr84q/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_203936-utjtr84q/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>13</td></tr><tr><td>validation_fine_f1</td><td>0.9993</td></tr><tr><td>training_fine_f1</td><td>0.99985</td></tr><tr><td>test_fine_f1</td><td>0.70862</td></tr><tr><td>validation_loss</td><td>3.62955</td></tr><tr><td>training_loss</td><td>3.62629</td></tr><tr><td>_runtime</td><td>4389</td></tr><tr><td>_timestamp</td><td>1620611565</td></tr><tr><td>_step</td><td>4134</td></tr><tr><td>loss</td><td>0.01691</td></tr><tr><td>batch</td><td>4381</td></tr><tr><td>loss_fine</td><td>0.01691</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr><tr><td>loss_layer2</td><td>19.47899</td></tr><tr><td>lambda_layer2</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer2</td><td>895</td></tr><tr><td>loss_layer3</td><td>19.50689</td></tr><tr><td>lambda_layer3</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer3</td><td>99</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>validation_fine_f1</td><td>▁█████████████</td></tr><tr><td>training_fine_f1</td><td>▁█████████████</td></tr><tr><td>test_fine_f1</td><td>▁▇█▇███▇██████</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer2</td><td>▅█▆▅▅▃██▇▆▅▆▃▆▅▅▄▇▆▅▇▄▅▅▆▂▅▇▅█▇▄▅▄▃▅▆▃▁▅</td></tr><tr><td>lambda_layer2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>▆▄▆▄▁▇▃▆▇▆▃▄▅▂▄▆▄▇▅▂█▆▆▅▃▅▆▄▃▆▆▆▂▃▅▄▆▇▅█</td></tr><tr><td>loss_layer3</td><td>▇▅▄▆▃▄▄▂▄▄▄▅▆▄▄▅▄▅▄▆▇▄█▁▃▃▂▆▅▆▅▅▂▆▅▃▆▄▆▆</td></tr><tr><td>lambda_layer3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer3</td><td>███▁██████████▁▁███▁████▁██▁███▁█▁▁███▁█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">major-sweep-14</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/utjtr84q\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/utjtr84q</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3254j6l9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.2055971233655008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.007240916946280587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 0.002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: semihard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">driven-sweep-15</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/3254j6l9\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/3254j6l9</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_215253-3254j6l9</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.2055971233655008, 'adaptive_lambda': 0.007240916946280587, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74db76a31e964804834a33c13c737d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/afd3a7847b4ab689b62e4ef0ae7aab51de3aa34189190da35e4cee68', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '09c2893e4f9bf640ccf3e33312b339c9fce266ee8dea7002ff79f567', 'trialHash': 'afd3a7847b4ab689b62e4ef0ae7aab51de3aa34189190da35e4cee68', 'adaptive_alpha': 0.2055971233655008, 'adaptive_lambda': 0.007240916946280587, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.002, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 8/500 [36:49<39:18:33, 287.63s/it, min_val_loss=1.01, train=0.983, val=0.976, val_loss=3.67]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   2%|▏         | 8/500 [36:53<37:49:10, 276.73s/it, min_val_loss=1.01, train=0.983, val=0.976, val_loss=3.67]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 33693<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_215253-3254j6l9/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_215253-3254j6l9/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>7</td></tr><tr><td>validation_fine_f1</td><td>0.99169</td></tr><tr><td>training_fine_f1</td><td>0.98305</td></tr><tr><td>test_fine_f1</td><td>0.68892</td></tr><tr><td>validation_loss</td><td>3.67188</td></tr><tr><td>training_loss</td><td>3.66559</td></tr><tr><td>_runtime</td><td>2215</td></tr><tr><td>_timestamp</td><td>1620613789</td></tr><tr><td>_step</td><td>2226</td></tr><tr><td>loss</td><td>0.03548</td></tr><tr><td>batch</td><td>2503</td></tr><tr><td>loss_fine</td><td>0.16513</td></tr><tr><td>lambda_fine</td><td>0.2056</td></tr><tr><td>loss_layer2</td><td>0.00194</td></tr><tr><td>lambda_layer2</td><td>0.7944</td></tr><tr><td>nonzerotriplets_layer2</td><td>7</td></tr><tr><td>loss_layer3</td><td>0.0</td></tr><tr><td>lambda_layer3</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer3</td><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██████</td></tr><tr><td>validation_fine_f1</td><td>▁▇███▇▅▄</td></tr><tr><td>training_fine_f1</td><td>▁▇████▆▆</td></tr><tr><td>test_fine_f1</td><td>▁▆▇▇██▅▅</td></tr><tr><td>validation_loss</td><td>█▁▁▁▁▂▅▅</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▄▄</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▃▂▂▂▁▁▁▁▁▂▁▁▂▂▂▂▂▂▂▃▂▂▁▂▂▂▂▂▁▁▂▂▃▃▂▃▃▂</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▅▃▂▂▃▁▂▁▁▁▂▁▁▂▂▂▂▂▂▂▃▂▂▁▂▂▂▂▂▁▁▂▂▃▃▂▃▃▂</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▅▁▂▃▃▁▅▃▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer2</td><td>▄▃▆▂▄▆▇▅▄▅▃▁▅▆█▂▇▃▇▇▃▅▄▄▄▇▅▄▁▅▃▅▅▆▃▆▆▅▄▃</td></tr><tr><td>lambda_layer2</td><td>▂▂▂▂█▂▇▂▂▂▂▂▁█▂▂▂▂▂▂▂▂▂▂█▂▂▂▂▂▆▂█▂▂▂▂▂▂▂</td></tr><tr><td>nonzerotriplets_layer2</td><td>▄▅▅▂▃▇▂▄▅▄▆▂▃▅▅▆▂▇▃▄▃▆▃▇▇▄▇▆▂▅▂▁▇█▄▃▇▁▇▁</td></tr><tr><td>loss_layer3</td><td>▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▆▁▁▁▅▁▁▁▁▁▁▁▁▁▁▇</td></tr><tr><td>lambda_layer3</td><td>▇▇▇▇▁▇▁█▇▇▇▇▇▁▇█▇▇▇█▇▇▇▇▁█▇█▇█▁▇▁█▇▇▇▇▇▇</td></tr><tr><td>nonzerotriplets_layer3</td><td>▁▁▅▁▁▁▁▁▁▁▁▁▅▁▁▅▁▁▁▁▁█▁▁▁▁▁▁▁█▁▁▅▁█▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">driven-sweep-15</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/3254j6l9\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/3254j6l9</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ldpjoiqx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5916861045898322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.18886280009687878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: hard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">firm-sweep-16</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/ldpjoiqx\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/ldpjoiqx</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_222956-ldpjoiqx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5916861045898322, 'adaptive_lambda': 0.18886280009687878, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 20, 'tripletSamples': 2, 'tripletSelector': 'hard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443bb8c521e54346bc85d1c4529574af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/cc40552a0bcacbf283335d3d425ff508ff8509d2f9a0a96fce4293b8', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '899fa4e15363a3325e13535a5561ca6deb818058d8fb72495317ace9', 'trialHash': 'cc40552a0bcacbf283335d3d425ff508ff8509d2f9a0a96fce4293b8', 'adaptive_alpha': 0.5916861045898322, 'adaptive_lambda': 0.18886280009687878, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 20, 'tripletSamples': 2, 'tripletSelector': 'hard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 9/500 [43:07<39:12:46, 287.51s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]       \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 39688<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_222956-ldpjoiqx/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_222956-ldpjoiqx/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>validation_fine_f1</td><td>0.99911</td></tr><tr><td>training_fine_f1</td><td>0.99982</td></tr><tr><td>test_fine_f1</td><td>0.70692</td></tr><tr><td>validation_loss</td><td>3.6299</td></tr><tr><td>training_loss</td><td>3.62688</td></tr><tr><td>_runtime</td><td>2592</td></tr><tr><td>_timestamp</td><td>1620616389</td></tr><tr><td>_step</td><td>2569</td></tr><tr><td>loss</td><td>0.01256</td></tr><tr><td>batch</td><td>2841</td></tr><tr><td>loss_fine</td><td>0.01256</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr><tr><td>loss_layer2</td><td>21.67493</td></tr><tr><td>lambda_layer2</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer2</td><td>900</td></tr><tr><td>loss_layer3</td><td>21.50019</td></tr><tr><td>lambda_layer3</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer3</td><td>100</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>validation_fine_f1</td><td>▁████████</td></tr><tr><td>training_fine_f1</td><td>▁████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇██████</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▂▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▂▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer2</td><td>▆▅▄▅▃▆▄█▄▇▆▄▂▃▆▆▄▃▆▆▇▆▅▅▆▃▅▃▃▆▄▅▆▁▃▆▂▆▅▆</td></tr><tr><td>lambda_layer2</td><td>█▂▄▃▃▃▅▃▄▄▂▂▃▃▅▂▃▃▄▃▃▂▄▂▄▂▁▄▂▁▃▂▃▃▂▂▂▄▄▃</td></tr><tr><td>nonzerotriplets_layer2</td><td>██████▁█████████████████████████████████</td></tr><tr><td>loss_layer3</td><td>▇▃▄▁▆▅▃▅▃▅▅▂▄▅▅▁▅▅▅█▄▄▃▂▆▄▃▄▃▅▄▅▄▃▄▄▆▇▃▄</td></tr><tr><td>lambda_layer3</td><td>█▇▆▄▄▄▆▃▅▄▆▄▃▅▅▄▁▄▄▃▄▃▃▆▅▄▅▃▄▅▄▃▄▆▄▂▅▄▆▅</td></tr><tr><td>nonzerotriplets_layer3</td><td>███████████████████████▁████████████████</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">firm-sweep-16</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/ldpjoiqx\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/ldpjoiqx</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p77bx3mw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.20823597913774058\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.43891147605715497\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: hard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">autumn-sweep-17</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/p77bx3mw\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/p77bx3mw</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_231315-p77bx3mw</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.20823597913774058, 'adaptive_lambda': 0.43891147605715497, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.2, 'tripletSamples': 2, 'tripletSelector': 'hard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc35b4f5d654c0bbef342b7eea9f82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/9852f8d86539e36d7573c21d39a981d60c64274a9ec5cfd575871ab3', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '4ff078cbda202e534b2539d305764ee8bbda9448eac081912039f476', 'trialHash': '9852f8d86539e36d7573c21d39a981d60c64274a9ec5cfd575871ab3', 'adaptive_alpha': 0.20823597913774058, 'adaptive_lambda': 0.43891147605715497, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.2, 'tripletSamples': 2, 'tripletSelector': 'hard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 10/500 [52:18<42:43:25, 313.89s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 48640<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_231315-p77bx3mw/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_231315-p77bx3mw/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>10</td></tr><tr><td>validation_fine_f1</td><td>0.99899</td></tr><tr><td>training_fine_f1</td><td>0.99982</td></tr><tr><td>test_fine_f1</td><td>0.7081</td></tr><tr><td>validation_loss</td><td>3.62973</td></tr><tr><td>training_loss</td><td>3.62667</td></tr><tr><td>_runtime</td><td>3144</td></tr><tr><td>_timestamp</td><td>1620619539</td></tr><tr><td>_step</td><td>3176</td></tr><tr><td>loss</td><td>nan</td></tr><tr><td>batch</td><td>3442</td></tr><tr><td>loss_fine</td><td>nan</td></tr><tr><td>lambda_fine</td><td>nan</td></tr><tr><td>loss_layer2</td><td>nan</td></tr><tr><td>lambda_layer2</td><td>nan</td></tr><tr><td>nonzerotriplets_layer2</td><td>1</td></tr><tr><td>loss_layer3</td><td>nan</td></tr><tr><td>lambda_layer3</td><td>nan</td></tr><tr><td>nonzerotriplets_layer3</td><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁█████████</td></tr><tr><td>training_fine_f1</td><td>▁█████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇█▇▇████</td></tr><tr><td>validation_loss</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▃▂▂▂▁▃▁▁▂▁▁▁▂▂▁▂▁▂▂▁▁▂▂▂▂▁▂▁▁▁▂▁▁▂▂▁▁▂▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▂▂▂▂▁▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▂▅█▅▂▇▅▇█▇▅▆▆▄▅█▁▆▃▆▆▇▄▃▄▄▇▅▆▅▇▅▆▇▁▅▆▅▃█</td></tr><tr><td>loss_layer2</td><td>▅▅▂▅▃▄▄▄▆▃▂▆█▂▆▅▄▃▄▅▃▃▄▄▃▁▄▂▄▄▃▅▄▇▅▄▆▅█▄</td></tr><tr><td>lambda_layer2</td><td>█▁▃█▇▄▄▃▁▃▆▅▄▆▇▂█▅▅▆▄▃█▃▃▄▄▇▆▅▂▆▅▃▇▆▅▆▄▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>▅▆▁▆▃█▄▅▅▅▄▄▄▆▇▅▂▄█▄▅▄█▆▆▆▇▆▅▆▆▄▆█▅▅▇▅▅▇</td></tr><tr><td>loss_layer3</td><td>▅▃▇▂▃▅▄▄▄▄▆▄▆▆▄▅▄▄▄▂█▄▄▄▃▃█▅▂▄▅▁▄▂▂▇▄▂▂▄</td></tr><tr><td>lambda_layer3</td><td>▆▆▁▂▇▁▄▂▂▂▄▃▂▄▃▁█▃▇▂▃▂▃█▇▆▂▃▁▄▂▄▂▂█▃▂▄█▂</td></tr><tr><td>nonzerotriplets_layer3</td><td>▅▅▇▇▇▅▄█▅█▇█▅▅▄▇▅▁▇▅▇▇▇▇▅▅▄▄▄▇▇█▅▇▇▄█▇█▄</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">autumn-sweep-17</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/p77bx3mw\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/p77bx3mw</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run p77bx3mw errored: ValueError('autodetected range of [nan, nan] is not finite',)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run p77bx3mw errored: ValueError('autodetected range of [nan, nan] is not finite',)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zppao872 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.5975719800145226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.061815354851673866\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: random\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">elated-sweep-18</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/zppao872\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/zppao872</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210510_000543-zppao872</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.5975719800145226, 'adaptive_lambda': 0.061815354851673866, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4367f0668b264bd29bb767c9e0c3c466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/34ee1c4579924da2a23be212ada687b4d405adbf21dde249e4e767c2', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '0356fdd6697c6b7afd9d15ce0c4d33fbebfa6e0a5fa798864c9a3ba4', 'trialHash': '34ee1c4579924da2a23be212ada687b4d405adbf21dde249e4e767c2', 'adaptive_alpha': 0.5975719800145226, 'adaptive_lambda': 0.061815354851673866, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'random'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 10/500 [55:22<45:13:10, 332.23s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]      \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 55778<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210510_000543-zppao872/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210510_000543-zppao872/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>10</td></tr><tr><td>validation_fine_f1</td><td>0.9993</td></tr><tr><td>training_fine_f1</td><td>0.99982</td></tr><tr><td>test_fine_f1</td><td>0.70829</td></tr><tr><td>validation_loss</td><td>3.62962</td></tr><tr><td>training_loss</td><td>3.62665</td></tr><tr><td>_runtime</td><td>3328</td></tr><tr><td>_timestamp</td><td>1620622871</td></tr><tr><td>_step</td><td>3172</td></tr><tr><td>loss</td><td>0.00479</td></tr><tr><td>batch</td><td>3439</td></tr><tr><td>loss_fine</td><td>0.00479</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr><tr><td>loss_layer2</td><td>2.56348</td></tr><tr><td>lambda_layer2</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer2</td><td>890</td></tr><tr><td>loss_layer3</td><td>2.47505</td></tr><tr><td>lambda_layer3</td><td>0.0</td></tr><tr><td>nonzerotriplets_layer3</td><td>98</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁█████████</td></tr><tr><td>training_fine_f1</td><td>▁█████████</td></tr><tr><td>test_fine_f1</td><td>▁▇▇▇██████</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▅▅▅▅▅▁▁▅▅▁▁▁▅█▅▁▁▅▅▅███▅▅▅▅▁▅▅█▁▁▅▁▅▅</td></tr><tr><td>loss_layer2</td><td>▄▃▂▃▅▂▅▇▃▁▆▆▇▃▄▄▃▅▆▂▄▃█▁▃▅▅▆▄▅▅▃▇▃▄▃▅▇▆▃</td></tr><tr><td>lambda_layer2</td><td>█▃▂▂▂▂▃▁▃▂▂▂▂▂▃▂▂▂▃▃▂▃▁▂▁▂▂▂▂▂▃▂▂▂▃▂▂▂▂▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>▅▄▄▃▆▇▇▆▃▆▆▅▆█▇▅▇▃▄▅▁▇▅▇▇▄▅▄▄▇▆▅▆▅▇▇▆▆▅▇</td></tr><tr><td>loss_layer3</td><td>▆▄▃▆▇▆▄▅▅▅▃▆▄▅█▄▅▄█▁▃█▂▆▄▇▂▄▃▄▅▄▆▄▃▆▃▆▅▄</td></tr><tr><td>lambda_layer3</td><td>▇▂▂▂▄▇▁▃▅█▃▅▅▃▄▂▁▄▄▅▃▁▂▁▁▁▆▃▃▃▂▄▅▁▃▃▂▃▄▅</td></tr><tr><td>nonzerotriplets_layer3</td><td>▁▆▆▃▁▃▆▆▆███▃▆▃▃▆▃▆█▃▁█▆▃▆▃█▃▁███▁▆██▃▆▆</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">elated-sweep-18</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/zppao872\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/zppao872</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g7oumhr4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.2413061128491081\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.012310610033384455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: semihard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dashing-sweep-19</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/g7oumhr4\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/g7oumhr4</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210510_010116-g7oumhr4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.2413061128491081, 'adaptive_lambda': 0.012310610033384455, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.2, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804ef63b0f724c8daeeabed190cbbb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/86c53b7c7c7b562c28996530d3f629b4226e1eb021bfef9acb0c4591', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '1c70383487f994b036079dba0c9c0fa5016881c6e0e82c57f75bec7e', 'trialHash': '86c53b7c7c7b562c28996530d3f629b4226e1eb021bfef9acb0c4591', 'adaptive_alpha': 0.2413061128491081, 'adaptive_lambda': 0.012310610033384455, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 0.2, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   2%|▏         | 9/500 [44:44<40:40:29, 298.23s/it, min_val_loss=1, train=1, val=0.999, val_loss=3.63]       \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 62070<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210510_010116-g7oumhr4/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210510_010116-g7oumhr4/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>9</td></tr><tr><td>validation_fine_f1</td><td>0.9993</td></tr><tr><td>training_fine_f1</td><td>0.99982</td></tr><tr><td>test_fine_f1</td><td>0.70785</td></tr><tr><td>validation_loss</td><td>3.6295</td></tr><tr><td>training_loss</td><td>3.62671</td></tr><tr><td>_runtime</td><td>2688</td></tr><tr><td>_timestamp</td><td>1620625565</td></tr><tr><td>_step</td><td>2633</td></tr><tr><td>loss</td><td>0.00714</td></tr><tr><td>batch</td><td>2905</td></tr><tr><td>loss_fine</td><td>0.00714</td></tr><tr><td>lambda_fine</td><td>0.99998</td></tr><tr><td>loss_layer2</td><td>0.19396</td></tr><tr><td>lambda_layer2</td><td>1e-05</td></tr><tr><td>nonzerotriplets_layer2</td><td>493</td></tr><tr><td>loss_layer3</td><td>0.19404</td></tr><tr><td>lambda_layer3</td><td>1e-05</td></tr><tr><td>nonzerotriplets_layer3</td><td>54</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>validation_fine_f1</td><td>▁████████</td></tr><tr><td>training_fine_f1</td><td>▁████████</td></tr><tr><td>test_fine_f1</td><td>▁▇███████</td></tr><tr><td>validation_loss</td><td>█▂▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▂▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▃██████████████████████████████████████</td></tr><tr><td>loss_layer2</td><td>▃▂▃▅▃▁▃▅▆█▆▄▄▅▇▃▆▅▅▆▅▆▅▂█▆▇▇▆▆▆▃▆▄▅▆▄▆▄▂</td></tr><tr><td>lambda_layer2</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>▇▇█▆▆▅▅▅▆▄▃▅▆▅▆▆▇▂▆▅▄▆▄▅▃▂▆▅▆▅▇▆▄▇▅▆▆█▁▅</td></tr><tr><td>loss_layer3</td><td>▅▄▄▆▆▃▁▄▄▅▄▆▃▁▄▃▆▄▂█▅▃▅▅▄▅▃▂▄▁▄▄▃▆▅▇▅▄▂▆</td></tr><tr><td>lambda_layer3</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer3</td><td>▅▄▇▅█▃▅▃▆▅▆▅▄▅▃▄▅▄▇▅▅▄▆▅▆▃▄▇▆▂▅▆▃▃▇▄▄▅▁▅</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">dashing-sweep-19</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/g7oumhr4\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/g7oumhr4</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jf21o5nu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_alpha: 0.8792429964187144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_lambda: 0.0020293948242825267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadaptive_smoothing: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmented: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchSize: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_path: cifar-100-python\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_res: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlambda: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlink_layer: avgpool\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodelType: BB\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumOfTrials: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tphylogeny_loss_epsilon: 0.03\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsuffix: None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttl_model: CIFAR\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletEnabled: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletMargin: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSamples: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttripletSelector: semihard\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find JUPYTER\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">super-sweep-20</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/sweeps/q0gbehkn</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/jf21o5nu\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD/runs/jf21o5nu</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210510_014612-jf21o5nu</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adaptive_alpha': 0.8792429964187144, 'adaptive_lambda': 0.0020293948242825267, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "Loading saved indices...\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db790f38dd7e49e0b24fa5d9a1ec4b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'HGNN_CIFAR_Adaptivesmoothing_Tripletloss_Hyperp_SGD', 'modelName': 'models/a2bb1e314fc7ca6e6c53074b4303455088441103c000371682f5cfc7', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '53f5650754cd6316e3d0b05ae6f5bca797e2119473de99d1c1fdac76', 'trialHash': 'a2bb1e314fc7ca6e6c53074b4303455088441103c000371682f5cfc7', 'adaptive_alpha': 0.8792429964187144, 'adaptive_lambda': 0.0020293948242825267, 'adaptive_smoothing': True, 'augmented': False, 'batchSize': 128, 'fc_layers': 1, 'image_path': 'cifar-100-python', 'img_res': 32, 'lambda': 0, 'learning_rate': 0.001, 'link_layer': 'avgpool', 'modelType': 'BB', 'numOfTrials': 1, 'phylogeny_loss': False, 'phylogeny_loss_epsilon': 0.03, 'suffix': None, 'tl_model': 'CIFAR', 'tripletEnabled': True, 'tripletMargin': 2, 'tripletSamples': 2, 'tripletSelector': 'semihard'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4 is not in ['conv1', 'bn1', 'relu', 'layer1', 'layer2', 'layer3', 'avgpool']\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   0%|          | 2/500 [06:16<18:14:09, 131.83s/it, min_val_loss=1.04, train=0.999, val=0.997, val_loss=3.63]"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train, count=COUNT)\n",
    "# !CUDA_VISIBLE_DEVICES=1 wandb agent k7cnzayi --count 100 #train.py from program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
