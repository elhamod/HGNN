{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath = \"/raid/elhamod/Fish/experiments/\" #\"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/Fish/\" # \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = \"Fish_tripletloss_lr_experiments\"\n",
    "device = 1\n",
    "detailed_reporting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "experiment:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 1\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.1, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'lr 0.1', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e31606f934948cc825cb818177b39bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_tripletloss_lr_experiments', 'modelName': 'models/fda39256680909f44d3dc1c311de1e879a7190f50c992adda281a0fb', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '55844f9138e7fd8d5775212ab048b2c2b41d6be9f4ccaba97bac5e9f', 'trialHash': 'fda39256680909f44d3dc1c311de1e879a7190f50c992adda281a0fb', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.1, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'lr 0.1', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fda39256680909f44d3dc1c311de1e879a7190f50c992adda281a0fb</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1sf7dui2\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1sf7dui2</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_153656-1sf7dui2</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:59<?, ?it/s, min_val_loss=inf, train=0.0036, val=0.00138, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [00:59<8:13:04, 59.29s/it, min_val_loss=inf, train=0.0036, val=0.00138, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [02:30<8:13:04, 59.29s/it, min_val_loss=725, train=0.00237, val=0.00242, val_loss=3.65]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [02:30<9:32:18, 68.95s/it, min_val_loss=725, train=0.00237, val=0.00242, val_loss=3.65]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [04:02<9:32:18, 68.95s/it, min_val_loss=413, train=0.0147, val=0.0159, val_loss=3.62]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [04:02<10:27:47, 75.79s/it, min_val_loss=413, train=0.0147, val=0.0159, val_loss=3.62]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [05:31<10:27:47, 75.79s/it, min_val_loss=63, train=0.0872, val=0.0942, val_loss=3.56] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [05:31<10:58:12, 79.62s/it, min_val_loss=63, train=0.0872, val=0.0942, val_loss=3.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [07:04<10:58:12, 79.62s/it, min_val_loss=10.6, train=0.0651, val=0.0623, val_loss=3.58]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [07:04<11:29:54, 83.63s/it, min_val_loss=10.6, train=0.0651, val=0.0623, val_loss=3.58]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [08:32<11:29:54, 83.63s/it, min_val_loss=10.6, train=0.358, val=0.289, val_loss=3.39]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [08:32<11:41:05, 85.15s/it, min_val_loss=10.6, train=0.358, val=0.289, val_loss=3.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [10:04<11:41:05, 85.15s/it, min_val_loss=3.46, train=0.202, val=0.194, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [10:04<11:54:59, 87.02s/it, min_val_loss=3.46, train=0.202, val=0.194, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [11:34<11:54:59, 87.02s/it, min_val_loss=3.46, train=0.217, val=0.18, val_loss=3.49] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [11:34<12:02:57, 88.17s/it, min_val_loss=3.46, train=0.217, val=0.18, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [13:05<12:02:57, 88.17s/it, min_val_loss=3.46, train=0.385, val=0.27, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [13:05<12:07:54, 88.95s/it, min_val_loss=3.46, train=0.385, val=0.27, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [14:35<12:07:54, 88.95s/it, min_val_loss=3.46, train=0.572, val=0.429, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [14:35<12:07:36, 89.09s/it, min_val_loss=3.46, train=0.572, val=0.429, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [16:07<12:07:36, 89.09s/it, min_val_loss=2.33, train=0.577, val=0.414, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [16:07<12:13:41, 90.02s/it, min_val_loss=2.33, train=0.577, val=0.414, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [17:40<12:13:41, 90.02s/it, min_val_loss=2.33, train=0.497, val=0.367, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [17:40<12:19:53, 90.97s/it, min_val_loss=2.33, train=0.497, val=0.367, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [19:07<12:19:53, 90.97s/it, min_val_loss=2.33, train=0.697, val=0.444, val_loss=3.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [19:07<12:09:28, 89.87s/it, min_val_loss=2.33, train=0.697, val=0.444, val_loss=3.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [20:45<12:09:28, 89.87s/it, min_val_loss=2.25, train=0.751, val=0.497, val_loss=3.2] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [20:45<12:26:30, 92.16s/it, min_val_loss=2.25, train=0.751, val=0.497, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [22:16<12:26:30, 92.16s/it, min_val_loss=2.01, train=0.88, val=0.55, val_loss=3.17] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [22:16<12:22:54, 91.91s/it, min_val_loss=2.01, train=0.88, val=0.55, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [23:47<12:22:54, 91.91s/it, min_val_loss=1.82, train=0.484, val=0.289, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [23:47<12:18:29, 91.55s/it, min_val_loss=1.82, train=0.484, val=0.289, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [25:16<12:18:29, 91.55s/it, min_val_loss=1.82, train=0.906, val=0.556, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [25:16<12:11:28, 90.87s/it, min_val_loss=1.82, train=0.906, val=0.556, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [26:53<12:11:28, 90.87s/it, min_val_loss=1.8, train=0.883, val=0.573, val_loss=3.19] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [26:53<12:23:13, 92.52s/it, min_val_loss=1.8, train=0.883, val=0.573, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [28:26<12:23:13, 92.52s/it, min_val_loss=1.75, train=0.611, val=0.442, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [28:26<12:24:01, 92.81s/it, min_val_loss=1.75, train=0.611, val=0.442, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [30:00<12:24:01, 92.81s/it, min_val_loss=1.75, train=0.586, val=0.351, val_loss=3.3] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [30:00<12:24:36, 93.08s/it, min_val_loss=1.75, train=0.586, val=0.351, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [31:31<12:24:36, 93.08s/it, min_val_loss=1.75, train=0.793, val=0.487, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [31:31<12:19:12, 92.59s/it, min_val_loss=1.75, train=0.793, val=0.487, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [32:59<12:19:12, 92.59s/it, min_val_loss=1.75, train=0.9, val=0.589, val_loss=3.15]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [32:59<12:06:21, 91.17s/it, min_val_loss=1.75, train=0.9, val=0.589, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [34:33<12:06:21, 91.17s/it, min_val_loss=1.7, train=0.934, val=0.607, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [34:33<12:10:41, 91.91s/it, min_val_loss=1.7, train=0.934, val=0.607, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [36:03<12:10:41, 91.91s/it, min_val_loss=1.65, train=0.934, val=0.591, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [36:03<12:06:21, 91.56s/it, min_val_loss=1.65, train=0.934, val=0.591, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [37:31<12:06:21, 91.56s/it, min_val_loss=1.65, train=0.875, val=0.532, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [37:31<11:54:55, 90.31s/it, min_val_loss=1.65, train=0.875, val=0.532, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [38:59<11:54:55, 90.31s/it, min_val_loss=1.65, train=0.966, val=0.623, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [38:59<11:47:24, 89.55s/it, min_val_loss=1.65, train=0.966, val=0.623, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [40:29<11:47:24, 89.55s/it, min_val_loss=1.6, train=0.937, val=0.582, val_loss=3.14] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [40:29<11:47:49, 89.79s/it, min_val_loss=1.6, train=0.937, val=0.582, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [42:00<11:47:49, 89.79s/it, min_val_loss=1.6, train=0.969, val=0.62, val_loss=3.12] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [42:00<11:49:13, 90.16s/it, min_val_loss=1.6, train=0.969, val=0.62, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [43:28<11:49:13, 90.16s/it, min_val_loss=1.6, train=0.938, val=0.603, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [43:28<11:42:12, 89.45s/it, min_val_loss=1.6, train=0.938, val=0.603, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [44:56<11:42:12, 89.45s/it, min_val_loss=1.6, train=0.965, val=0.646, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [44:56<11:37:24, 89.03s/it, min_val_loss=1.6, train=0.965, val=0.646, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [46:25<11:37:24, 89.03s/it, min_val_loss=1.55, train=0.695, val=0.433, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [46:25<11:36:06, 89.05s/it, min_val_loss=1.55, train=0.695, val=0.433, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [47:58<11:36:06, 89.05s/it, min_val_loss=1.55, train=0.93, val=0.569, val_loss=3.14] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [47:58<11:43:55, 90.25s/it, min_val_loss=1.55, train=0.93, val=0.569, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   6%|▋         | 32/500 [49:27<11:43:55, 90.25s/it, min_val_loss=1.55, train=0.776, val=0.512, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [49:27<11:40:09, 89.96s/it, min_val_loss=1.55, train=0.776, val=0.512, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [50:58<11:40:09, 89.96s/it, min_val_loss=1.55, train=0.851, val=0.517, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [50:58<11:40:44, 90.22s/it, min_val_loss=1.55, train=0.851, val=0.517, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [52:29<11:40:44, 90.22s/it, min_val_loss=1.55, train=0.998, val=0.703, val_loss=3.03]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 35/500 [52:29<11:40:48, 90.43s/it, min_val_loss=1.55, train=0.998, val=0.703, val_loss=3.03]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 35/500 [54:01<11:40:48, 90.43s/it, min_val_loss=1.42, train=0.998, val=0.717, val_loss=3.01]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 36/500 [54:01<11:43:32, 90.97s/it, min_val_loss=1.42, train=0.998, val=0.717, val_loss=3.01]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 36/500 [55:33<11:43:32, 90.97s/it, min_val_loss=1.4, train=0.999, val=0.724, val_loss=3.01] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 37/500 [55:33<11:43:01, 91.10s/it, min_val_loss=1.4, train=0.999, val=0.724, val_loss=3.01]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 37/500 [57:05<11:43:01, 91.10s/it, min_val_loss=1.38, train=0.999, val=0.73, val_loss=3]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 38/500 [57:05<11:44:40, 91.52s/it, min_val_loss=1.38, train=0.999, val=0.73, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 38/500 [58:39<11:44:40, 91.52s/it, min_val_loss=1.37, train=1, val=0.731, val_loss=3]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 39/500 [58:39<11:47:26, 92.07s/it, min_val_loss=1.37, train=1, val=0.731, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 39/500 [1:00:06<11:47:26, 92.07s/it, min_val_loss=1.37, train=0.999, val=0.72, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 40/500 [1:00:06<11:36:25, 90.84s/it, min_val_loss=1.37, train=0.999, val=0.72, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 40/500 [1:01:35<11:36:25, 90.84s/it, min_val_loss=1.37, train=1, val=0.715, val_loss=3]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 41/500 [1:01:35<11:29:07, 90.08s/it, min_val_loss=1.37, train=1, val=0.715, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 41/500 [1:03:04<11:29:07, 90.08s/it, min_val_loss=1.37, train=1, val=0.715, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 42/500 [1:03:04<11:25:06, 89.75s/it, min_val_loss=1.37, train=1, val=0.715, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 42/500 [1:04:35<11:25:06, 89.75s/it, min_val_loss=1.37, train=1, val=0.734, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▊         | 43/500 [1:04:35<11:26:09, 90.09s/it, min_val_loss=1.37, train=1, val=0.734, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▊         | 43/500 [1:06:07<11:26:09, 90.09s/it, min_val_loss=1.36, train=1, val=0.725, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 44/500 [1:06:07<11:29:03, 90.67s/it, min_val_loss=1.36, train=1, val=0.725, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 44/500 [1:07:34<11:29:03, 90.67s/it, min_val_loss=1.36, train=1, val=0.73, val_loss=2.99] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 45/500 [1:07:34<11:20:24, 89.72s/it, min_val_loss=1.36, train=1, val=0.73, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 45/500 [1:09:05<11:20:24, 89.72s/it, min_val_loss=1.36, train=1, val=0.724, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 46/500 [1:09:05<11:21:36, 90.08s/it, min_val_loss=1.36, train=1, val=0.724, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 46/500 [1:10:35<11:21:36, 90.08s/it, min_val_loss=1.36, train=1, val=0.733, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 47/500 [1:10:35<11:18:43, 89.90s/it, min_val_loss=1.36, train=1, val=0.733, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 47/500 [1:12:05<11:18:43, 89.90s/it, min_val_loss=1.36, train=1, val=0.729, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|▉         | 48/500 [1:12:05<11:18:02, 90.01s/it, min_val_loss=1.36, train=1, val=0.729, val_loss=2.99]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:  10%|▉         | 48/500 [1:12:20<11:21:11, 90.42s/it, min_val_loss=1.36, train=1, val=0.729, val_loss=2.99]\n",
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/ipykernel_launcher.py:109: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17180<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_153656-1sf7dui2/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_153656-1sf7dui2/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>47</td></tr><tr><td>learning rate</td><td>0.001</td></tr><tr><td>validation_fine_f1</td><td>0.73389</td></tr><tr><td>training_fine_f1</td><td>1.0</td></tr><tr><td>test_fine_f1</td><td>0.79239</td></tr><tr><td>validation_loss</td><td>2.9884</td></tr><tr><td>_runtime</td><td>4334</td></tr><tr><td>_timestamp</td><td>1620593350</td></tr><tr><td>_step</td><td>517</td></tr><tr><td>loss</td><td>0.04999</td></tr><tr><td>batch</td><td>479</td></tr><tr><td>loss_fine</td><td>0.04999</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>████████████████████████████▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>validation_fine_f1</td><td>▁▁▁▂▂▃▃▄▅▅▅▆▆▄▆▅▄▆▇▇▆▇▇▇▇▅▆▆▆███████████</td></tr><tr><td>training_fine_f1</td><td>▁▁▁▂▁▂▃▄▅▅▆▆▇▄▇▅▅▇▇█▇████▆█▆▇███████████</td></tr><tr><td>test_fine_f1</td><td>▁▁▁▂▂▃▂▃▅▅▅▆▆▄▆▄▅▅▆▇▆▇▇▇▇▅▆▆▆███████████</td></tr><tr><td>validation_loss</td><td>███▇▇▆▆▆▄▄▄▃▃▅▃▄▄▃▃▂▃▂▃▂▂▄▃▄▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▅▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▆▅▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">fda39256680909f44d3dc1c311de1e879a7190f50c992adda281a0fb</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1sf7dui2\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1sf7dui2</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "experiment:  33%|███▎      | 1/3 [1:12:33<2:25:06, 4353.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'lr 0.01', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719efb538b214fb3abcf2375a2940823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_tripletloss_lr_experiments', 'modelName': 'models/db578781861801a63df48cf4b9e8bdafcf157c2f23bdc427f5c1ca84', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '852d484d34bc9fde5fd3e30f8090135dd1dc637becd42c2866dfb269', 'trialHash': 'db578781861801a63df48cf4b9e8bdafcf157c2f23bdc427f5c1ca84', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'lr 0.01', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">db578781861801a63df48cf4b9e8bdafcf157c2f23bdc427f5c1ca84</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1ijcvcqd\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1ijcvcqd</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_164929-1ijcvcqd</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [01:04<?, ?it/s, min_val_loss=inf, train=0.00225, val=0.00516, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [01:04<8:53:48, 64.19s/it, min_val_loss=inf, train=0.00225, val=0.00516, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [02:34<8:53:48, 64.19s/it, min_val_loss=194, train=0.026, val=0.0146, val_loss=3.62]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [02:34<9:56:42, 71.89s/it, min_val_loss=194, train=0.026, val=0.0146, val_loss=3.62]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [04:06<9:56:42, 71.89s/it, min_val_loss=68.6, train=0.186, val=0.137, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [04:06<10:47:20, 78.15s/it, min_val_loss=68.6, train=0.186, val=0.137, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [05:37<10:47:20, 78.15s/it, min_val_loss=7.3, train=0.534, val=0.419, val_loss=3.43] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [05:37<11:16:53, 81.88s/it, min_val_loss=7.3, train=0.534, val=0.419, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [07:06<11:16:53, 81.88s/it, min_val_loss=2.38, train=0.808, val=0.636, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [07:06<11:33:52, 84.11s/it, min_val_loss=2.38, train=0.808, val=0.636, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [08:37<11:33:52, 84.11s/it, min_val_loss=1.57, train=0.898, val=0.663, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [08:37<11:49:18, 86.15s/it, min_val_loss=1.57, train=0.898, val=0.663, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [10:04<11:49:18, 86.15s/it, min_val_loss=1.51, train=0.953, val=0.708, val_loss=3.3] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [10:04<11:50:19, 86.45s/it, min_val_loss=1.51, train=0.953, val=0.708, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [11:34<11:50:19, 86.45s/it, min_val_loss=1.41, train=0.952, val=0.663, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [11:34<11:56:24, 87.37s/it, min_val_loss=1.41, train=0.952, val=0.663, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [13:00<11:56:24, 87.37s/it, min_val_loss=1.41, train=0.997, val=0.738, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [13:00<11:51:04, 86.89s/it, min_val_loss=1.41, train=0.997, val=0.738, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [14:32<11:51:04, 86.89s/it, min_val_loss=1.35, train=0.999, val=0.782, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [14:32<12:03:32, 88.60s/it, min_val_loss=1.35, train=0.999, val=0.782, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [16:01<12:03:32, 88.60s/it, min_val_loss=1.28, train=1, val=0.772, val_loss=3.18]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [16:01<12:03:24, 88.76s/it, min_val_loss=1.28, train=1, val=0.772, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [17:31<12:03:24, 88.76s/it, min_val_loss=1.28, train=0.998, val=0.776, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [17:31<12:03:05, 88.90s/it, min_val_loss=1.28, train=0.998, val=0.776, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [18:59<12:03:05, 88.90s/it, min_val_loss=1.28, train=1, val=0.792, val_loss=3.13]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [18:59<11:59:58, 88.70s/it, min_val_loss=1.28, train=1, val=0.792, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [20:30<11:59:58, 88.70s/it, min_val_loss=1.26, train=1, val=0.824, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [20:30<12:05:35, 89.58s/it, min_val_loss=1.26, train=1, val=0.824, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [22:03<12:05:35, 89.58s/it, min_val_loss=1.21, train=0.999, val=0.804, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [22:03<12:10:24, 90.36s/it, min_val_loss=1.21, train=0.999, val=0.804, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [23:34<12:10:24, 90.36s/it, min_val_loss=1.21, train=0.998, val=0.796, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [23:34<12:10:32, 90.56s/it, min_val_loss=1.21, train=0.998, val=0.796, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [25:02<12:10:32, 90.56s/it, min_val_loss=1.21, train=1, val=0.823, val_loss=3.1]     \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [25:02<12:04:17, 89.97s/it, min_val_loss=1.21, train=1, val=0.823, val_loss=3.1]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [26:33<12:04:17, 89.97s/it, min_val_loss=1.21, train=1, val=0.815, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [26:33<12:05:39, 90.33s/it, min_val_loss=1.21, train=1, val=0.815, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [28:02<12:05:39, 90.33s/it, min_val_loss=1.21, train=1, val=0.831, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [28:02<12:00:55, 89.93s/it, min_val_loss=1.21, train=1, val=0.831, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [29:32<12:00:55, 89.93s/it, min_val_loss=1.2, train=1, val=0.839, val_loss=3.08] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [29:32<11:58:45, 89.85s/it, min_val_loss=1.2, train=1, val=0.839, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [31:05<11:58:45, 89.85s/it, min_val_loss=1.19, train=1, val=0.826, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [31:05<12:05:33, 90.88s/it, min_val_loss=1.19, train=1, val=0.826, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [32:33<12:05:33, 90.88s/it, min_val_loss=1.19, train=1, val=0.834, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [32:33<11:57:11, 90.02s/it, min_val_loss=1.19, train=1, val=0.834, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [34:07<11:57:11, 90.02s/it, min_val_loss=1.19, train=1, val=0.831, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [34:07<12:03:55, 91.06s/it, min_val_loss=1.19, train=1, val=0.831, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [35:37<12:03:55, 91.06s/it, min_val_loss=1.19, train=1, val=0.833, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [35:37<12:00:45, 90.85s/it, min_val_loss=1.19, train=1, val=0.833, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [37:11<12:00:45, 90.85s/it, min_val_loss=1.19, train=0.999, val=0.825, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [37:11<12:05:44, 91.67s/it, min_val_loss=1.19, train=0.999, val=0.825, val_loss=3.07]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   5%|▌         | 25/500 [37:26<11:51:19, 89.85s/it, min_val_loss=1.19, train=0.999, val=0.825, val_loss=3.07]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 41399<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_164929-1ijcvcqd/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_164929-1ijcvcqd/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>24</td></tr><tr><td>learning rate</td><td>0.0001</td></tr><tr><td>validation_fine_f1</td><td>0.83911</td></tr><tr><td>training_fine_f1</td><td>0.99918</td></tr><tr><td>test_fine_f1</td><td>0.86082</td></tr><tr><td>validation_loss</td><td>3.07304</td></tr><tr><td>_runtime</td><td>2234</td></tr><tr><td>_timestamp</td><td>1620595603</td></tr><tr><td>_step</td><td>264</td></tr><tr><td>loss</td><td>0.12764</td></tr><tr><td>batch</td><td>249</td></tr><tr><td>loss_fine</td><td>0.12764</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>learning rate</td><td>█████████████████▂▂▂▂▂▂▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁▁▂▄▆▇▇▇▇█▇▇█████████████</td></tr><tr><td>training_fine_f1</td><td>▁▁▂▅▇▇███████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▁▂▅▆▇▇▇▇█▇██████████████</td></tr><tr><td>validation_loss</td><td>██▇▅▄▄▄▄▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▅▅▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▇▅▅▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">db578781861801a63df48cf4b9e8bdafcf157c2f23bdc427f5c1ca84</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1ijcvcqd\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1ijcvcqd</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "experiment:  67%|██████▋   | 2/3 [1:50:06<1:02:03, 3723.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'lr 0.00', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a321882dfa746aab9819e1d77199735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_tripletloss_lr_experiments', 'modelName': 'models/1749ebf92069bdc963a2767497fa10b64ce8029e951933ff7cf3f880', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': 'e4e9a67249246bc31040b1f7ba5dbd5f4441fe1a1fb2ec4eca881ac9', 'trialHash': '1749ebf92069bdc963a2767497fa10b64ce8029e951933ff7cf3f880', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'tripletSamples': 3, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'lr 0.00', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">1749ebf92069bdc963a2767497fa10b64ce8029e951933ff7cf3f880</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/vvyb04dm\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/vvyb04dm</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_172702-vvyb04dm</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [01:05<?, ?it/s, min_val_loss=inf, train=0.00968, val=0.00359, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [01:05<9:03:07, 65.31s/it, min_val_loss=inf, train=0.00968, val=0.00359, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [02:37<9:03:07, 65.31s/it, min_val_loss=278, train=0.047, val=0.035, val_loss=3.63]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [02:37<10:09:33, 73.44s/it, min_val_loss=278, train=0.047, val=0.035, val_loss=3.63]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [04:13<10:09:33, 73.44s/it, min_val_loss=28.6, train=0.162, val=0.119, val_loss=3.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [04:13<11:02:49, 80.02s/it, min_val_loss=28.6, train=0.162, val=0.119, val_loss=3.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [05:46<11:02:49, 80.02s/it, min_val_loss=8.4, train=0.33, val=0.244, val_loss=3.56]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [05:46<11:34:54, 84.06s/it, min_val_loss=8.4, train=0.33, val=0.244, val_loss=3.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [07:18<11:34:54, 84.06s/it, min_val_loss=4.09, train=0.517, val=0.409, val_loss=3.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [07:18<11:52:05, 86.31s/it, min_val_loss=4.09, train=0.517, val=0.409, val_loss=3.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [08:50<11:52:05, 86.31s/it, min_val_loss=2.44, train=0.644, val=0.429, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [08:50<12:04:32, 88.00s/it, min_val_loss=2.44, train=0.644, val=0.429, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [10:20<12:04:32, 88.00s/it, min_val_loss=2.33, train=0.733, val=0.513, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [10:20<12:09:04, 88.73s/it, min_val_loss=2.33, train=0.733, val=0.513, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [11:54<12:09:04, 88.73s/it, min_val_loss=1.95, train=0.803, val=0.581, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [11:54<12:20:45, 90.34s/it, min_val_loss=1.95, train=0.803, val=0.581, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [13:29<12:20:45, 90.34s/it, min_val_loss=1.72, train=0.817, val=0.588, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [13:29<12:30:59, 91.77s/it, min_val_loss=1.72, train=0.817, val=0.588, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [14:58<12:30:59, 91.77s/it, min_val_loss=1.7, train=0.899, val=0.609, val_loss=3.4]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [14:58<12:22:30, 90.92s/it, min_val_loss=1.7, train=0.899, val=0.609, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [16:33<12:22:30, 90.92s/it, min_val_loss=1.64, train=0.907, val=0.654, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [16:33<12:29:56, 92.02s/it, min_val_loss=1.64, train=0.907, val=0.654, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [18:08<12:29:56, 92.02s/it, min_val_loss=1.53, train=0.927, val=0.665, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [18:08<12:37:30, 93.14s/it, min_val_loss=1.53, train=0.927, val=0.665, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [19:43<12:37:30, 93.14s/it, min_val_loss=1.5, train=0.961, val=0.706, val_loss=3.35] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [19:43<12:38:09, 93.41s/it, min_val_loss=1.5, train=0.961, val=0.706, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [21:14<12:38:09, 93.41s/it, min_val_loss=1.42, train=0.966, val=0.732, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [21:14<12:32:58, 92.96s/it, min_val_loss=1.42, train=0.966, val=0.732, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [22:46<12:32:58, 92.96s/it, min_val_loss=1.37, train=0.968, val=0.703, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [22:46<12:28:36, 92.61s/it, min_val_loss=1.37, train=0.968, val=0.703, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [24:20<12:28:36, 92.61s/it, min_val_loss=1.37, train=0.982, val=0.741, val_loss=3.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [24:20<12:29:50, 92.96s/it, min_val_loss=1.37, train=0.982, val=0.741, val_loss=3.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [25:55<12:29:50, 92.96s/it, min_val_loss=1.35, train=0.982, val=0.709, val_loss=3.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [25:55<12:33:57, 93.66s/it, min_val_loss=1.35, train=0.982, val=0.709, val_loss=3.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [27:29<12:33:57, 93.66s/it, min_val_loss=1.35, train=0.99, val=0.733, val_loss=3.31] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [27:29<12:33:26, 93.79s/it, min_val_loss=1.35, train=0.99, val=0.733, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [29:02<12:33:26, 93.79s/it, min_val_loss=1.35, train=0.991, val=0.742, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [29:02<12:27:54, 93.29s/it, min_val_loss=1.35, train=0.991, val=0.742, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [30:41<12:27:54, 93.29s/it, min_val_loss=1.35, train=0.99, val=0.762, val_loss=3.31] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [30:41<12:40:08, 95.02s/it, min_val_loss=1.35, train=0.99, val=0.762, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [32:12<12:40:08, 95.02s/it, min_val_loss=1.31, train=0.993, val=0.733, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [32:12<12:28:52, 93.81s/it, min_val_loss=1.31, train=0.993, val=0.733, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [33:47<12:28:52, 93.81s/it, min_val_loss=1.31, train=0.993, val=0.772, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [33:47<12:31:09, 94.29s/it, min_val_loss=1.31, train=0.993, val=0.772, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [35:22<12:31:09, 94.29s/it, min_val_loss=1.3, train=0.994, val=0.765, val_loss=3.28] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [35:22<12:31:07, 94.48s/it, min_val_loss=1.3, train=0.994, val=0.765, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [36:55<12:31:07, 94.48s/it, min_val_loss=1.3, train=0.993, val=0.761, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [36:55<12:26:53, 94.15s/it, min_val_loss=1.3, train=0.993, val=0.761, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [38:31<12:26:53, 94.15s/it, min_val_loss=1.3, train=0.998, val=0.764, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [38:31<12:28:49, 94.59s/it, min_val_loss=1.3, train=0.998, val=0.764, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [40:03<12:28:49, 94.59s/it, min_val_loss=1.3, train=0.991, val=0.778, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [40:03<12:20:57, 93.79s/it, min_val_loss=1.3, train=0.991, val=0.778, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [41:38<12:20:57, 93.79s/it, min_val_loss=1.29, train=0.999, val=0.769, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [41:38<12:21:32, 94.07s/it, min_val_loss=1.29, train=0.999, val=0.769, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [43:13<12:21:32, 94.07s/it, min_val_loss=1.29, train=0.998, val=0.747, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [43:13<12:23:47, 94.55s/it, min_val_loss=1.29, train=0.998, val=0.747, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [44:49<12:23:47, 94.55s/it, min_val_loss=1.29, train=0.997, val=0.779, val_loss=3.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [44:49<12:25:56, 95.02s/it, min_val_loss=1.29, train=0.997, val=0.779, val_loss=3.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [46:25<12:25:56, 95.02s/it, min_val_loss=1.28, train=0.998, val=0.761, val_loss=3.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [46:25<12:24:54, 95.09s/it, min_val_loss=1.28, train=0.998, val=0.761, val_loss=3.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [47:57<12:24:54, 95.09s/it, min_val_loss=1.28, train=0.996, val=0.749, val_loss=3.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [47:57<12:16:04, 94.17s/it, min_val_loss=1.28, train=0.996, val=0.749, val_loss=3.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [49:32<12:16:04, 94.17s/it, min_val_loss=1.28, train=0.998, val=0.755, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [49:32<12:17:55, 94.61s/it, min_val_loss=1.28, train=0.998, val=0.755, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   6%|▋         | 32/500 [51:07<12:17:55, 94.61s/it, min_val_loss=1.28, train=0.997, val=0.763, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [51:07<12:16:01, 94.56s/it, min_val_loss=1.28, train=0.997, val=0.763, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [52:38<12:16:01, 94.56s/it, min_val_loss=1.28, train=0.998, val=0.754, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [52:38<12:07:48, 93.71s/it, min_val_loss=1.28, train=0.998, val=0.754, val_loss=3.24]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   7%|▋         | 34/500 [52:53<12:04:53, 93.33s/it, min_val_loss=1.28, train=0.998, val=0.754, val_loss=3.24]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 52424<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_172702-vvyb04dm/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210509_172702-vvyb04dm/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>33</td></tr><tr><td>learning rate</td><td>0.0001</td></tr><tr><td>validation_fine_f1</td><td>0.7793</td></tr><tr><td>training_fine_f1</td><td>0.99753</td></tr><tr><td>test_fine_f1</td><td>0.75549</td></tr><tr><td>validation_loss</td><td>3.24394</td></tr><tr><td>_runtime</td><td>3162</td></tr><tr><td>_timestamp</td><td>1620598784</td></tr><tr><td>_step</td><td>363</td></tr><tr><td>loss</td><td>0.38902</td></tr><tr><td>batch</td><td>339</td></tr><tr><td>loss_fine</td><td>0.38902</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>learning rate</td><td>████████████████████████████████▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁▁▂▃▅▅▆▆▆▆▇▇▇█▇█▇█████████████████</td></tr><tr><td>training_fine_f1</td><td>▁▁▂▃▅▅▆▇▇▇▇▇██████████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▁▂▃▅▅▅▆▆▇▇▇▇█▇▇▇█████████████████</td></tr><tr><td>validation_loss</td><td>███▇▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">1749ebf92069bdc963a2767497fa10b64ce8029e951933ff7cf3f880</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/vvyb04dm\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/vvyb04dm</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment: 100%|██████████| 3/3 [2:43:07<00:00, 3262.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    print('wandb not found')\n",
    "\n",
    "from myhelpers import config_plots, TrialStatistics\n",
    "from myhelpers.try_warning import try_running\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "config_plots.global_settings()\n",
    "\n",
    "experimetnsFileName = \"experiments.csv\"\n",
    "WANDB_message=\"wandb not working\"\n",
    "\n",
    "\n",
    "# For logging to server\n",
    "try_running(lambda : wandb.login(), WANDB_message)\n",
    "\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "# set cuda\n",
    "if device is not None:\n",
    "    print(\"using cuda\", device)\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# get experiment params\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "\n",
    "# init experiments file\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "\n",
    "paramsIterator = config_parser.getExperiments()  \n",
    "number_of_experiments = sum(1 for e in paramsIterator)  \n",
    "experiment_index = 0\n",
    "\n",
    "# Loop through experiments\n",
    "# with progressbar.ProgressBar(max_value=number_of_experiments) as bar:\n",
    "with tqdm(total=number_of_experiments, desc=\"experiment\") as bar:\n",
    "    for experiment_params in config_parser.getExperiments():\n",
    "        print(experiment_params)\n",
    "        experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "        # load images \n",
    "        if experiment_params['image_path'] == 'cifar-100-python':\n",
    "            datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        else:\n",
    "            datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "        train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "        architecture = {\n",
    "            \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "            \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "        }\n",
    "\n",
    "        # Loop through n trials\n",
    "        for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "            modelName = getModelName(experiment_params, i)\n",
    "            trialName = os.path.join(experimentPathAndName, modelName)\n",
    "            trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "            row_information = {\n",
    "                'experimentName': experimentName,\n",
    "                'modelName': modelName,\n",
    "                'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "                'experimentHash': experimentHash,\n",
    "                'trialHash': trialHash\n",
    "            }\n",
    "            row_information = {**row_information, **experiment_params} \n",
    "            print(row_information)\n",
    "\n",
    "            run = try_running(lambda : wandb.init(project='HGNN', group=experimentName+\"-\"+experimentHash, name=trialHash, config=row_information), WANDB_message) #, reinit=True\n",
    "\n",
    "            # Train/Load model\n",
    "            model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "\n",
    "            try_running(lambda : wandb.watch(model, log=\"all\"), WANDB_message)\n",
    "\n",
    "            if os.path.exists(CNN.getModelFile(trialName)):\n",
    "                print(\"Model {0} found!\".format(trialName))\n",
    "            else:\n",
    "                initModelPath = CNN.getInitModelFile(experimentPathAndName)\n",
    "                if os.path.exists(initModelPath):\n",
    "                    model.load_state_dict(torch.load(initModelPath))\n",
    "                    print(\"Init Model {0} found!\".format(initModelPath))\n",
    "                CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=detailed_reporting)\n",
    "\n",
    "            # Add to experiments file\n",
    "            if os.path.exists(experimentsFileNameAndPath):\n",
    "                experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "            else:\n",
    "                experiments_df = pd.DataFrame()\n",
    "\n",
    "            record_exists = not (experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName]).empty if not experiments_df.empty else False\n",
    "            if record_exists:\n",
    "                experiments_df.drop(experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName].index, inplace = True) \n",
    "\n",
    "            experiments_df = experiments_df.append(pd.DataFrame(row_information, index=[0]), ignore_index = True)\n",
    "            experiments_df.to_csv(experimentsFileNameAndPath, header=True, index=False)\n",
    "\n",
    "            try_running(lambda : run.finish(), WANDB_message)\n",
    "\n",
    "        bar.update()\n",
    "\n",
    "        experiment_index = experiment_index + 1\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "#     import argparse\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--cuda', required=True, type=int)\n",
    "#     parser.add_argument('--experiments', required=True)\n",
    "#     parser.add_argument('--data', required=True)\n",
    "#     parser.add_argument('--name', required=True)\n",
    "#     parser.add_argument('--detailed', required=False, action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "#     main(experimentName=args.name, experimentsPath=args.experiments, dataPath=args.data, device=args.cuda, detailed_reporting=args.detailed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
