{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath = \"/raid/elhamod/CIFAR_HGNN/experiments/\"\n",
    "dataPath = \"/raid/elhamod/\"\n",
    "experimentName = \"CIFAR_phylogeny_8_4_0_KLDiv\"\n",
    "device = 0\n",
    "detailed_reporting = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experimetnsFileName = \"experiments.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "experiment:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 0\n",
      "{'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': False, 'batchSize': 128, 'learning_rate': 5e-05, 'numOfTrials': 5, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0.01, 'unsupervisedOnTest': False, 'tl_model': 'CIFAR', 'link_layer': 'avgpool', 'adaptive_smoothing': False, 'adaptive_lambda': 0.015, 'adaptive_alpha': 0.8, 'noSpeciesBackprop': False, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating dataset...\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Loading dataset...\n",
      "/raid/elhamod/cifar-100-python/\n",
      "Files already downloaded and verified\n",
      "CIFAR normalization\n",
      "Creating dataset... Done.\n",
      "train/val =  40000 10000\n",
      "file /raid/elhamod/CIFAR_HGNN/experiments/CIFAR_phylogeny_8_4_0_KLDiv/trainingIndex.pkl written\n",
      "file /raid/elhamod/CIFAR_HGNN/experiments/CIFAR_phylogeny_8_4_0_KLDiv/valIndex.pkl written\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fb8d916d464f5698a752f979f8a9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_phylogeny_8_4_0_KLDiv', 'modelName': 'models/6b63c036ce74bd9c2360a1efc05e196e84807db2fdb81b09fbbdc01f', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '843245f82691b9ea95cc530cd93dd7d6b3a46cacb99e0ce433b37eab', 'trialHash': '6b63c036ce74bd9c2360a1efc05e196e84807db2fdb81b09fbbdc01f', 'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': False, 'batchSize': 128, 'learning_rate': 5e-05, 'numOfTrials': 5, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0.01, 'unsupervisedOnTest': False, 'tl_model': 'CIFAR', 'link_layer': 'avgpool', 'adaptive_smoothing': False, 'adaptive_lambda': 0.015, 'adaptive_alpha': 0.8, 'noSpeciesBackprop': False, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">6b63c036ce74bd9c2360a1efc05e196e84807db2fdb81b09fbbdc01f</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/ty8bg94z\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/ty8bg94z</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_132902-ty8bg94z</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/torch/nn/functional.py:2352: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [01:11<?, ?it/s, min_val_loss=inf, train=0.234, val=0.22, val_loss=4.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [01:11<9:52:28, 71.24s/it, min_val_loss=inf, train=0.234, val=0.22, val_loss=4.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [02:22<9:52:28, 71.24s/it, min_val_loss=4.55, train=0.492, val=0.461, val_loss=4.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [02:22<9:52:12, 71.35s/it, min_val_loss=4.55, train=0.492, val=0.461, val_loss=4.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:34<9:52:12, 71.35s/it, min_val_loss=2.17, train=0.65, val=0.609, val_loss=4.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [03:34<9:51:08, 71.36s/it, min_val_loss=2.17, train=0.65, val=0.609, val_loss=4.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [04:46<9:51:08, 71.36s/it, min_val_loss=1.64, train=0.738, val=0.691, val_loss=4.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [04:46<9:52:29, 71.67s/it, min_val_loss=1.64, train=0.738, val=0.691, val_loss=4.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [05:58<9:52:29, 71.67s/it, min_val_loss=1.45, train=0.802, val=0.742, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [05:58<9:51:29, 71.70s/it, min_val_loss=1.45, train=0.802, val=0.742, val_loss=4.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [07:10<9:51:29, 71.70s/it, min_val_loss=1.35, train=0.843, val=0.774, val_loss=4.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [07:10<9:51:23, 71.83s/it, min_val_loss=1.35, train=0.843, val=0.774, val_loss=4.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [08:22<9:51:23, 71.83s/it, min_val_loss=1.29, train=0.875, val=0.792, val_loss=4.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [08:22<9:50:09, 71.82s/it, min_val_loss=1.29, train=0.875, val=0.792, val_loss=4.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [09:34<9:50:09, 71.82s/it, min_val_loss=1.26, train=0.905, val=0.811, val_loss=4.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [09:34<9:50:18, 71.99s/it, min_val_loss=1.26, train=0.905, val=0.811, val_loss=4.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [10:46<9:50:18, 71.99s/it, min_val_loss=1.23, train=0.925, val=0.822, val_loss=4.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [10:46<9:48:22, 71.90s/it, min_val_loss=1.23, train=0.925, val=0.822, val_loss=4.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [11:58<9:48:22, 71.90s/it, min_val_loss=1.22, train=0.938, val=0.825, val_loss=4.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [11:58<9:47:02, 71.88s/it, min_val_loss=1.22, train=0.938, val=0.825, val_loss=4.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [13:10<9:47:02, 71.88s/it, min_val_loss=1.21, train=0.952, val=0.829, val_loss=4.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [13:10<9:45:45, 71.87s/it, min_val_loss=1.21, train=0.952, val=0.829, val_loss=4.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [14:21<9:45:45, 71.87s/it, min_val_loss=1.21, train=0.96, val=0.829, val_loss=4.17] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [14:21<9:44:15, 71.83s/it, min_val_loss=1.21, train=0.96, val=0.829, val_loss=4.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [15:33<9:44:15, 71.83s/it, min_val_loss=1.21, train=0.969, val=0.829, val_loss=4.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [15:33<9:42:10, 71.73s/it, min_val_loss=1.21, train=0.969, val=0.829, val_loss=4.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [16:44<9:42:10, 71.73s/it, min_val_loss=1.21, train=0.976, val=0.824, val_loss=4.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [16:44<9:39:37, 71.56s/it, min_val_loss=1.21, train=0.976, val=0.824, val_loss=4.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [17:55<9:39:37, 71.56s/it, min_val_loss=1.21, train=0.981, val=0.825, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [17:55<9:37:21, 71.43s/it, min_val_loss=1.21, train=0.981, val=0.825, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [19:06<9:37:21, 71.43s/it, min_val_loss=1.21, train=0.986, val=0.817, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [19:06<9:35:38, 71.36s/it, min_val_loss=1.21, train=0.986, val=0.817, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [20:19<9:35:38, 71.36s/it, min_val_loss=1.21, train=0.989, val=0.812, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [20:19<9:36:28, 71.61s/it, min_val_loss=1.21, train=0.989, val=0.812, val_loss=4.13]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   3%|▎         | 17/500 [20:23<9:39:23, 71.97s/it, min_val_loss=1.21, train=0.989, val=0.812, val_loss=4.13]\n",
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/ipykernel_launcher.py:109: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 60623<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.02MB of 0.02MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_132902-ty8bg94z/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_132902-ty8bg94z/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.00942</td></tr><tr><td>batch</td><td>5320</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>loss_fine</td><td>0.00942</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0.01</td></tr><tr><td>_step</td><td>5337</td></tr><tr><td>_runtime</td><td>1224</td></tr><tr><td>_timestamp</td><td>1619200166</td></tr><tr><td>validation_fine_f1</td><td>0.82936</td></tr><tr><td>training_fine_f1</td><td>0.98923</td></tr><tr><td>test_fine_f1</td><td>0.65875</td></tr><tr><td>validation_loss</td><td>4.12638</td></tr><tr><td>training_loss</td><td>3.91248</td></tr><tr><td>test_coarse_f1</td><td>0.75154</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▇▆▅▅▄▅▄▃▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>loss_fine</td><td>█▇▆▅▅▄▅▄▃▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>validation_fine_f1</td><td>▁▄▅▆▇▇███████████</td></tr><tr><td>training_fine_f1</td><td>▁▃▅▆▆▇▇▇▇████████</td></tr><tr><td>test_fine_f1</td><td>▁▄▆▆▇▇▇██████████</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▄▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>training_loss</td><td>█▇▇▆▅▅▄▄▄▃▃▂▂▂▂▁▁</td></tr><tr><td>test_coarse_f1</td><td>▁▄▆▆▇▇▇▇█████████</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">6b63c036ce74bd9c2360a1efc05e196e84807db2fdb81b09fbbdc01f</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/ty8bg94z\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/ty8bg94z</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_phylogeny_8_4_0_KLDiv', 'modelName': 'models/87dda0240edeb5f96948a543a1436b5bbf12ece329dfe4d05ab14866', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '843245f82691b9ea95cc530cd93dd7d6b3a46cacb99e0ce433b37eab', 'trialHash': '87dda0240edeb5f96948a543a1436b5bbf12ece329dfe4d05ab14866', 'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': False, 'batchSize': 128, 'learning_rate': 5e-05, 'numOfTrials': 5, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0.01, 'unsupervisedOnTest': False, 'tl_model': 'CIFAR', 'link_layer': 'avgpool', 'adaptive_smoothing': False, 'adaptive_lambda': 0.015, 'adaptive_alpha': 0.8, 'noSpeciesBackprop': False, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">87dda0240edeb5f96948a543a1436b5bbf12ece329dfe4d05ab14866</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/zbope3yf\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/zbope3yf</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_134934-zbope3yf</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [01:10<?, ?it/s, min_val_loss=inf, train=0.224, val=0.217, val_loss=4.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [01:10<9:48:10, 70.72s/it, min_val_loss=inf, train=0.224, val=0.217, val_loss=4.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [02:22<9:48:10, 70.72s/it, min_val_loss=4.6, train=0.477, val=0.451, val_loss=4.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [02:22<9:48:55, 70.96s/it, min_val_loss=4.6, train=0.477, val=0.451, val_loss=4.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:33<9:48:55, 70.96s/it, min_val_loss=2.22, train=0.633, val=0.598, val_loss=4.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [03:33<9:47:40, 70.95s/it, min_val_loss=2.22, train=0.633, val=0.598, val_loss=4.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [04:45<9:47:40, 70.95s/it, min_val_loss=1.67, train=0.729, val=0.684, val_loss=4.4] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [04:45<9:49:44, 71.34s/it, min_val_loss=1.67, train=0.729, val=0.684, val_loss=4.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [05:56<9:49:44, 71.34s/it, min_val_loss=1.46, train=0.8, val=0.739, val_loss=4.36] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [05:56<9:47:27, 71.21s/it, min_val_loss=1.46, train=0.8, val=0.739, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [07:08<9:47:27, 71.21s/it, min_val_loss=1.35, train=0.845, val=0.768, val_loss=4.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [07:08<9:48:39, 71.50s/it, min_val_loss=1.35, train=0.845, val=0.768, val_loss=4.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [08:20<9:48:39, 71.50s/it, min_val_loss=1.3, train=0.879, val=0.794, val_loss=4.29] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [08:20<9:48:42, 71.65s/it, min_val_loss=1.3, train=0.879, val=0.794, val_loss=4.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [09:33<9:48:42, 71.65s/it, min_val_loss=1.26, train=0.902, val=0.807, val_loss=4.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [09:33<9:49:52, 71.94s/it, min_val_loss=1.26, train=0.902, val=0.807, val_loss=4.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [10:44<9:49:52, 71.94s/it, min_val_loss=1.24, train=0.923, val=0.819, val_loss=4.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [10:44<9:47:33, 71.80s/it, min_val_loss=1.24, train=0.923, val=0.819, val_loss=4.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [11:56<9:47:33, 71.80s/it, min_val_loss=1.22, train=0.937, val=0.826, val_loss=4.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [11:56<9:46:02, 71.76s/it, min_val_loss=1.22, train=0.937, val=0.826, val_loss=4.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [13:07<9:46:02, 71.76s/it, min_val_loss=1.21, train=0.95, val=0.826, val_loss=4.2]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [13:07<9:44:34, 71.73s/it, min_val_loss=1.21, train=0.95, val=0.826, val_loss=4.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [14:19<9:44:34, 71.73s/it, min_val_loss=1.21, train=0.961, val=0.827, val_loss=4.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [14:19<9:43:25, 71.73s/it, min_val_loss=1.21, train=0.961, val=0.827, val_loss=4.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [15:31<9:43:25, 71.73s/it, min_val_loss=1.21, train=0.97, val=0.827, val_loss=4.16] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [15:31<9:41:55, 71.70s/it, min_val_loss=1.21, train=0.97, val=0.827, val_loss=4.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [16:42<9:41:55, 71.70s/it, min_val_loss=1.21, train=0.977, val=0.825, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [16:42<9:39:52, 71.59s/it, min_val_loss=1.21, train=0.977, val=0.825, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [17:53<9:39:52, 71.59s/it, min_val_loss=1.21, train=0.982, val=0.821, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [17:53<9:37:39, 71.46s/it, min_val_loss=1.21, train=0.982, val=0.821, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [19:05<9:37:39, 71.46s/it, min_val_loss=1.21, train=0.987, val=0.818, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [19:05<9:37:47, 71.63s/it, min_val_loss=1.21, train=0.987, val=0.818, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [20:16<9:37:47, 71.63s/it, min_val_loss=1.21, train=0.991, val=0.815, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [20:16<9:35:09, 71.45s/it, min_val_loss=1.21, train=0.991, val=0.815, val_loss=4.13]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   3%|▎         | 17/500 [20:20<9:38:09, 71.82s/it, min_val_loss=1.21, train=0.991, val=0.815, val_loss=4.13]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 73550<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.02MB of 0.02MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_134934-zbope3yf/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_134934-zbope3yf/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.00743</td></tr><tr><td>batch</td><td>5320</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>loss_fine</td><td>0.00743</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0.01</td></tr><tr><td>_step</td><td>5337</td></tr><tr><td>_runtime</td><td>1218</td></tr><tr><td>_timestamp</td><td>1619201392</td></tr><tr><td>validation_fine_f1</td><td>0.82709</td></tr><tr><td>training_fine_f1</td><td>0.99101</td></tr><tr><td>test_fine_f1</td><td>0.65157</td></tr><tr><td>validation_loss</td><td>4.13472</td></tr><tr><td>training_loss</td><td>3.92374</td></tr><tr><td>test_coarse_f1</td><td>0.75136</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▇▆▆▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>loss_fine</td><td>█▇▆▆▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>validation_fine_f1</td><td>▁▄▅▆▇▇███████████</td></tr><tr><td>training_fine_f1</td><td>▁▃▅▆▆▇▇▇▇████████</td></tr><tr><td>test_fine_f1</td><td>▁▄▅▆▇▇███████████</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▄▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▇▇▆▅▅▄▄▄▃▃▂▂▂▂▁▁</td></tr><tr><td>test_coarse_f1</td><td>▁▄▆▆▇▇▇▇█████████</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">87dda0240edeb5f96948a543a1436b5bbf12ece329dfe4d05ab14866</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/zbope3yf\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/zbope3yf</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_phylogeny_8_4_0_KLDiv', 'modelName': 'models/de9f7f06d738ee83c2cb7a5b415ad3709d174483b9eea7245e42c048', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '843245f82691b9ea95cc530cd93dd7d6b3a46cacb99e0ce433b37eab', 'trialHash': 'de9f7f06d738ee83c2cb7a5b415ad3709d174483b9eea7245e42c048', 'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': False, 'batchSize': 128, 'learning_rate': 5e-05, 'numOfTrials': 5, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0.01, 'unsupervisedOnTest': False, 'tl_model': 'CIFAR', 'link_layer': 'avgpool', 'adaptive_smoothing': False, 'adaptive_lambda': 0.015, 'adaptive_alpha': 0.8, 'noSpeciesBackprop': False, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">de9f7f06d738ee83c2cb7a5b415ad3709d174483b9eea7245e42c048</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2997orc5\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2997orc5</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_141000-2997orc5</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [01:11<?, ?it/s, min_val_loss=inf, train=0.222, val=0.211, val_loss=4.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [01:11<9:55:43, 71.63s/it, min_val_loss=inf, train=0.222, val=0.211, val_loss=4.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [02:23<9:55:43, 71.63s/it, min_val_loss=4.74, train=0.445, val=0.424, val_loss=4.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [02:23<9:54:33, 71.63s/it, min_val_loss=4.74, train=0.445, val=0.424, val_loss=4.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:34<9:54:33, 71.63s/it, min_val_loss=2.36, train=0.611, val=0.568, val_loss=4.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [03:34<9:51:53, 71.46s/it, min_val_loss=2.36, train=0.611, val=0.568, val_loss=4.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [04:46<9:51:53, 71.46s/it, min_val_loss=1.76, train=0.72, val=0.662, val_loss=4.4]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [04:46<9:52:09, 71.63s/it, min_val_loss=1.76, train=0.72, val=0.662, val_loss=4.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [05:59<9:52:09, 71.63s/it, min_val_loss=1.51, train=0.789, val=0.722, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [05:59<9:53:30, 71.94s/it, min_val_loss=1.51, train=0.789, val=0.722, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [07:11<9:53:30, 71.94s/it, min_val_loss=1.38, train=0.836, val=0.757, val_loss=4.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [07:11<9:54:15, 72.18s/it, min_val_loss=1.38, train=0.836, val=0.757, val_loss=4.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [08:23<9:54:15, 72.18s/it, min_val_loss=1.32, train=0.869, val=0.784, val_loss=4.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [08:23<9:52:56, 72.16s/it, min_val_loss=1.32, train=0.869, val=0.784, val_loss=4.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [09:36<9:52:56, 72.16s/it, min_val_loss=1.28, train=0.891, val=0.797, val_loss=4.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [09:36<9:52:17, 72.23s/it, min_val_loss=1.28, train=0.891, val=0.797, val_loss=4.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [10:47<9:52:17, 72.23s/it, min_val_loss=1.25, train=0.912, val=0.805, val_loss=4.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [10:47<9:49:13, 72.00s/it, min_val_loss=1.25, train=0.912, val=0.805, val_loss=4.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [12:00<9:49:13, 72.00s/it, min_val_loss=1.24, train=0.934, val=0.817, val_loss=4.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [12:00<9:49:04, 72.13s/it, min_val_loss=1.24, train=0.934, val=0.817, val_loss=4.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [13:13<9:49:04, 72.13s/it, min_val_loss=1.22, train=0.948, val=0.823, val_loss=4.2] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [13:13<9:50:22, 72.44s/it, min_val_loss=1.22, train=0.948, val=0.823, val_loss=4.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [14:26<9:50:22, 72.44s/it, min_val_loss=1.22, train=0.958, val=0.821, val_loss=4.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [14:26<9:49:55, 72.53s/it, min_val_loss=1.22, train=0.958, val=0.821, val_loss=4.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [15:38<9:49:55, 72.53s/it, min_val_loss=1.22, train=0.966, val=0.823, val_loss=4.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [15:38<9:48:45, 72.54s/it, min_val_loss=1.22, train=0.966, val=0.823, val_loss=4.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [16:51<9:48:45, 72.54s/it, min_val_loss=1.22, train=0.974, val=0.823, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [16:51<9:48:35, 72.66s/it, min_val_loss=1.22, train=0.974, val=0.823, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [18:03<9:48:35, 72.66s/it, min_val_loss=1.22, train=0.979, val=0.82, val_loss=4.14] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [18:03<9:44:55, 72.36s/it, min_val_loss=1.22, train=0.979, val=0.82, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [19:14<9:44:55, 72.36s/it, min_val_loss=1.22, train=0.986, val=0.81, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [19:14<9:42:07, 72.16s/it, min_val_loss=1.22, train=0.986, val=0.81, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [20:26<9:42:07, 72.16s/it, min_val_loss=1.22, train=0.988, val=0.81, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [20:26<9:39:35, 72.00s/it, min_val_loss=1.22, train=0.988, val=0.81, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [21:38<9:39:35, 72.00s/it, min_val_loss=1.22, train=0.991, val=0.807, val_loss=4.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [21:38<9:37:33, 71.90s/it, min_val_loss=1.22, train=0.991, val=0.807, val_loss=4.12]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   4%|▎         | 18/500 [21:42<9:41:11, 72.35s/it, min_val_loss=1.22, train=0.991, val=0.807, val_loss=4.12]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12260<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.02MB of 0.02MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_141000-2997orc5/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_141000-2997orc5/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.00754</td></tr><tr><td>batch</td><td>5633</td></tr><tr><td>epoch</td><td>17</td></tr><tr><td>loss_fine</td><td>0.00754</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0.01</td></tr><tr><td>_step</td><td>5651</td></tr><tr><td>_runtime</td><td>1299</td></tr><tr><td>_timestamp</td><td>1619202699</td></tr><tr><td>validation_fine_f1</td><td>0.82299</td></tr><tr><td>training_fine_f1</td><td>0.99149</td></tr><tr><td>test_fine_f1</td><td>0.65725</td></tr><tr><td>validation_loss</td><td>4.12436</td></tr><tr><td>training_loss</td><td>3.89542</td></tr><tr><td>test_coarse_f1</td><td>0.74695</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>loss_fine</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁▃▅▆▇▇████████████</td></tr><tr><td>training_fine_f1</td><td>▁▃▅▆▆▇▇▇▇▇████████</td></tr><tr><td>test_fine_f1</td><td>▁▄▅▆▇▇████████████</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▇▇▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>test_coarse_f1</td><td>▁▄▆▆▇▇▇███████████</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">de9f7f06d738ee83c2cb7a5b415ad3709d174483b9eea7245e42c048</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2997orc5\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2997orc5</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_phylogeny_8_4_0_KLDiv', 'modelName': 'models/c0e1b322b214f1f26521b965fdd28d4d92529ef9e0c70ee98b37868c', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '843245f82691b9ea95cc530cd93dd7d6b3a46cacb99e0ce433b37eab', 'trialHash': 'c0e1b322b214f1f26521b965fdd28d4d92529ef9e0c70ee98b37868c', 'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': False, 'batchSize': 128, 'learning_rate': 5e-05, 'numOfTrials': 5, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0.01, 'unsupervisedOnTest': False, 'tl_model': 'CIFAR', 'link_layer': 'avgpool', 'adaptive_smoothing': False, 'adaptive_lambda': 0.015, 'adaptive_alpha': 0.8, 'noSpeciesBackprop': False, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">c0e1b322b214f1f26521b965fdd28d4d92529ef9e0c70ee98b37868c</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/3dumpptm\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/3dumpptm</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_143149-3dumpptm</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [01:11<?, ?it/s, min_val_loss=inf, train=0.196, val=0.188, val_loss=4.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [01:11<9:52:49, 71.28s/it, min_val_loss=inf, train=0.196, val=0.188, val_loss=4.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [02:22<9:52:49, 71.28s/it, min_val_loss=5.32, train=0.427, val=0.399, val_loss=4.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [02:22<9:52:02, 71.33s/it, min_val_loss=5.32, train=0.427, val=0.399, val_loss=4.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:34<9:52:02, 71.33s/it, min_val_loss=2.51, train=0.602, val=0.563, val_loss=4.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [03:34<9:51:38, 71.43s/it, min_val_loss=2.51, train=0.602, val=0.563, val_loss=4.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [04:47<9:51:38, 71.43s/it, min_val_loss=1.77, train=0.714, val=0.666, val_loss=4.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [04:47<9:54:18, 71.89s/it, min_val_loss=1.77, train=0.714, val=0.666, val_loss=4.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [05:59<9:54:18, 71.89s/it, min_val_loss=1.5, train=0.788, val=0.729, val_loss=4.36] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [05:59<9:53:00, 71.88s/it, min_val_loss=1.5, train=0.788, val=0.729, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [07:11<9:53:00, 71.88s/it, min_val_loss=1.37, train=0.838, val=0.762, val_loss=4.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [07:11<9:52:48, 72.00s/it, min_val_loss=1.37, train=0.838, val=0.762, val_loss=4.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [08:23<9:52:48, 72.00s/it, min_val_loss=1.31, train=0.871, val=0.789, val_loss=4.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [08:23<9:51:41, 72.01s/it, min_val_loss=1.31, train=0.871, val=0.789, val_loss=4.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [09:36<9:51:41, 72.01s/it, min_val_loss=1.27, train=0.898, val=0.808, val_loss=4.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [09:36<9:52:17, 72.23s/it, min_val_loss=1.27, train=0.898, val=0.808, val_loss=4.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [10:48<9:52:17, 72.23s/it, min_val_loss=1.24, train=0.915, val=0.817, val_loss=4.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [10:48<9:50:50, 72.20s/it, min_val_loss=1.24, train=0.915, val=0.817, val_loss=4.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [12:01<9:50:50, 72.20s/it, min_val_loss=1.22, train=0.927, val=0.819, val_loss=4.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [12:01<9:50:55, 72.36s/it, min_val_loss=1.22, train=0.927, val=0.819, val_loss=4.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [13:13<9:50:55, 72.36s/it, min_val_loss=1.22, train=0.941, val=0.822, val_loss=4.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [13:13<9:48:59, 72.27s/it, min_val_loss=1.22, train=0.941, val=0.822, val_loss=4.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [14:25<9:48:59, 72.27s/it, min_val_loss=1.22, train=0.949, val=0.821, val_loss=4.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [14:25<9:47:36, 72.25s/it, min_val_loss=1.22, train=0.949, val=0.821, val_loss=4.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [15:36<9:47:36, 72.25s/it, min_val_loss=1.22, train=0.958, val=0.82, val_loss=4.15] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [15:36<9:44:46, 72.05s/it, min_val_loss=1.22, train=0.958, val=0.82, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [16:48<9:44:46, 72.05s/it, min_val_loss=1.22, train=0.964, val=0.818, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [16:48<9:42:00, 71.85s/it, min_val_loss=1.22, train=0.964, val=0.818, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [17:59<9:42:00, 71.85s/it, min_val_loss=1.22, train=0.97, val=0.822, val_loss=4.15] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [17:59<9:40:06, 71.77s/it, min_val_loss=1.22, train=0.97, val=0.822, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [19:12<9:40:06, 71.77s/it, min_val_loss=1.22, train=0.973, val=0.81, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [19:12<9:40:54, 72.01s/it, min_val_loss=1.22, train=0.973, val=0.81, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [20:23<9:40:54, 72.01s/it, min_val_loss=1.22, train=0.976, val=0.805, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [20:23<9:37:19, 71.72s/it, min_val_loss=1.22, train=0.976, val=0.805, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [21:35<9:37:19, 71.72s/it, min_val_loss=1.22, train=0.977, val=0.8, val_loss=4.13]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [21:35<9:36:11, 71.73s/it, min_val_loss=1.22, train=0.977, val=0.8, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [22:47<9:36:11, 71.73s/it, min_val_loss=1.22, train=0.981, val=0.799, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [22:47<9:35:02, 71.73s/it, min_val_loss=1.22, train=0.981, val=0.799, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [24:00<9:35:02, 71.73s/it, min_val_loss=1.22, train=0.982, val=0.795, val_loss=4.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [24:00<9:37:41, 72.21s/it, min_val_loss=1.22, train=0.982, val=0.795, val_loss=4.12]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   4%|▍         | 20/500 [24:04<9:37:45, 72.22s/it, min_val_loss=1.22, train=0.982, val=0.795, val_loss=4.12]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29422<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.02MB of 0.02MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_143149-3dumpptm/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_143149-3dumpptm/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.00786</td></tr><tr><td>batch</td><td>6259</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>loss_fine</td><td>0.00786</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0.01</td></tr><tr><td>_step</td><td>6279</td></tr><tr><td>_runtime</td><td>1442</td></tr><tr><td>_timestamp</td><td>1619204151</td></tr><tr><td>validation_fine_f1</td><td>0.82195</td></tr><tr><td>training_fine_f1</td><td>0.98227</td></tr><tr><td>test_fine_f1</td><td>0.65288</td></tr><tr><td>validation_loss</td><td>4.12436</td></tr><tr><td>training_loss</td><td>3.86722</td></tr><tr><td>test_coarse_f1</td><td>0.75229</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>loss_fine</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁▃▅▆▇▇██████████████</td></tr><tr><td>training_fine_f1</td><td>▁▃▅▆▆▇▇▇▇███████████</td></tr><tr><td>test_fine_f1</td><td>▁▃▅▆▇▇██████████████</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▇▇▆▆▅▅▄▄▄▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>test_coarse_f1</td><td>▁▄▆▇▇▇▇█████████████</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">c0e1b322b214f1f26521b965fdd28d4d92529ef9e0c70ee98b37868c</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/3dumpptm\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/3dumpptm</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'CIFAR_phylogeny_8_4_0_KLDiv', 'modelName': 'models/b3174b5ec8d793b030667df98f18c5c10f61617609f4490a100db479', 'datasetName': 'datasplits/dd10c35154ee995db5de0276a21ca1a15a77964ee53811d98f089e89', 'experimentHash': '843245f82691b9ea95cc530cd93dd7d6b3a46cacb99e0ce433b37eab', 'trialHash': 'b3174b5ec8d793b030667df98f18c5c10f61617609f4490a100db479', 'image_path': 'cifar-100-python', 'suffix': None, 'img_res': 32, 'augmented': False, 'batchSize': 128, 'learning_rate': 5e-05, 'numOfTrials': 5, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0.01, 'unsupervisedOnTest': False, 'tl_model': 'CIFAR', 'link_layer': 'avgpool', 'adaptive_smoothing': False, 'adaptive_lambda': 0.015, 'adaptive_alpha': 0.8, 'noSpeciesBackprop': False, 'phylogeny_loss': 'KLDiv', 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">b3174b5ec8d793b030667df98f18c5c10f61617609f4490a100db479</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2fplgnuz\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2fplgnuz</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_145600-2fplgnuz</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [01:11<?, ?it/s, min_val_loss=inf, train=0.224, val=0.218, val_loss=4.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [01:11<9:54:03, 71.43s/it, min_val_loss=inf, train=0.224, val=0.218, val_loss=4.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [02:23<9:54:03, 71.43s/it, min_val_loss=4.58, train=0.472, val=0.452, val_loss=4.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [02:23<9:55:23, 71.73s/it, min_val_loss=4.58, train=0.472, val=0.452, val_loss=4.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:35<9:55:23, 71.73s/it, min_val_loss=2.21, train=0.626, val=0.59, val_loss=4.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [03:35<9:52:42, 71.55s/it, min_val_loss=2.21, train=0.626, val=0.59, val_loss=4.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [04:47<9:52:42, 71.55s/it, min_val_loss=1.7, train=0.722, val=0.67, val_loss=4.41] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [04:47<9:53:08, 71.75s/it, min_val_loss=1.7, train=0.722, val=0.67, val_loss=4.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [06:01<9:53:08, 71.75s/it, min_val_loss=1.49, train=0.788, val=0.725, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [06:01<9:57:27, 72.42s/it, min_val_loss=1.49, train=0.788, val=0.725, val_loss=4.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [07:14<9:57:27, 72.42s/it, min_val_loss=1.38, train=0.838, val=0.765, val_loss=4.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [07:14<9:59:08, 72.77s/it, min_val_loss=1.38, train=0.838, val=0.765, val_loss=4.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [08:28<9:59:08, 72.77s/it, min_val_loss=1.31, train=0.873, val=0.791, val_loss=4.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [08:28<10:01:27, 73.20s/it, min_val_loss=1.31, train=0.873, val=0.791, val_loss=4.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [09:41<10:01:27, 73.20s/it, min_val_loss=1.26, train=0.899, val=0.805, val_loss=4.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [09:41<9:58:37, 73.00s/it, min_val_loss=1.26, train=0.899, val=0.805, val_loss=4.26] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [10:52<9:58:37, 73.00s/it, min_val_loss=1.24, train=0.916, val=0.81, val_loss=4.24] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [10:52<9:53:08, 72.48s/it, min_val_loss=1.24, train=0.916, val=0.81, val_loss=4.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [12:03<9:53:08, 72.48s/it, min_val_loss=1.23, train=0.936, val=0.82, val_loss=4.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [12:03<9:46:21, 71.80s/it, min_val_loss=1.23, train=0.936, val=0.82, val_loss=4.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [13:13<9:46:21, 71.80s/it, min_val_loss=1.22, train=0.949, val=0.824, val_loss=4.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [13:13<9:41:30, 71.35s/it, min_val_loss=1.22, train=0.949, val=0.824, val_loss=4.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [14:23<9:41:30, 71.35s/it, min_val_loss=1.21, train=0.96, val=0.825, val_loss=4.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [14:23<9:36:45, 70.91s/it, min_val_loss=1.21, train=0.96, val=0.825, val_loss=4.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [15:31<9:36:45, 70.91s/it, min_val_loss=1.21, train=0.967, val=0.828, val_loss=4.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [15:31<9:30:01, 70.23s/it, min_val_loss=1.21, train=0.967, val=0.828, val_loss=4.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [16:39<9:30:01, 70.23s/it, min_val_loss=1.21, train=0.975, val=0.825, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [16:39<9:22:30, 69.45s/it, min_val_loss=1.21, train=0.975, val=0.825, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [17:46<9:22:30, 69.45s/it, min_val_loss=1.21, train=0.981, val=0.823, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [17:46<9:16:30, 68.85s/it, min_val_loss=1.21, train=0.981, val=0.823, val_loss=4.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [18:51<9:16:30, 68.85s/it, min_val_loss=1.21, train=0.986, val=0.818, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [18:51<9:06:11, 67.71s/it, min_val_loss=1.21, train=0.986, val=0.818, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [19:57<9:06:11, 67.71s/it, min_val_loss=1.21, train=0.989, val=0.814, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [19:57<9:00:31, 67.15s/it, min_val_loss=1.21, train=0.989, val=0.814, val_loss=4.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [21:04<9:00:31, 67.15s/it, min_val_loss=1.21, train=0.992, val=0.812, val_loss=4.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [21:04<8:57:34, 66.92s/it, min_val_loss=1.21, train=0.992, val=0.812, val_loss=4.14]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   4%|▎         | 18/500 [21:08<9:26:01, 70.46s/it, min_val_loss=1.21, train=0.992, val=0.812, val_loss=4.14]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 56456<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.02MB of 0.02MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_145600-2fplgnuz/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210423_145600-2fplgnuz/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.00778</td></tr><tr><td>batch</td><td>5633</td></tr><tr><td>epoch</td><td>17</td></tr><tr><td>loss_fine</td><td>0.00778</td></tr><tr><td>loss_coarse</td><td>0</td></tr><tr><td>lambda_fine</td><td>1</td></tr><tr><td>lambda_coarse</td><td>0.01</td></tr><tr><td>_step</td><td>5651</td></tr><tr><td>_runtime</td><td>1266</td></tr><tr><td>_timestamp</td><td>1619205426</td></tr><tr><td>validation_fine_f1</td><td>0.82792</td></tr><tr><td>training_fine_f1</td><td>0.99248</td></tr><tr><td>test_fine_f1</td><td>0.65836</td></tr><tr><td>validation_loss</td><td>4.13552</td></tr><tr><td>training_loss</td><td>3.91287</td></tr><tr><td>test_coarse_f1</td><td>0.76095</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>loss_fine</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>validation_fine_f1</td><td>▁▄▅▆▇▇████████████</td></tr><tr><td>training_fine_f1</td><td>▁▃▅▆▆▇▇▇▇▇████████</td></tr><tr><td>test_fine_f1</td><td>▁▄▅▆▇▇████████████</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>training_loss</td><td>█▇▇▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>test_coarse_f1</td><td>▁▄▅▆▇▇▇▇▇█████████</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">b3174b5ec8d793b030667df98f18c5c10f61617609f4490a100db479</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2fplgnuz\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2fplgnuz</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment: 100%|██████████| 1/1 [1:48:14<00:00, 6494.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from myhelpers import config_plots, TrialStatistics\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "config_plots.global_settings()\n",
    "\n",
    "\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "# set cuda\n",
    "if device is not None:\n",
    "    print(\"using cuda\", device)\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# get experiment params\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "\n",
    "# init experiments file\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "\n",
    "paramsIterator = config_parser.getExperiments()  \n",
    "number_of_experiments = sum(1 for e in paramsIterator)  \n",
    "experiment_index = 0\n",
    "\n",
    "# Loop through experiments\n",
    "# with progressbar.ProgressBar(max_value=number_of_experiments) as bar:\n",
    "with tqdm(total=number_of_experiments, desc=\"experiment\") as bar:\n",
    "    for experiment_params in config_parser.getExperiments():\n",
    "        print(experiment_params)\n",
    "        experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "        # load images \n",
    "        datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "        train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "        architecture = {\n",
    "            \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "            \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "        }\n",
    "\n",
    "        # Loop through n trials\n",
    "        for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "            modelName = getModelName(experiment_params, i)\n",
    "            trialName = os.path.join(experimentPathAndName, modelName)\n",
    "            trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "            row_information = {\n",
    "                'experimentName': experimentName,\n",
    "                'modelName': modelName,\n",
    "                'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "                'experimentHash': experimentHash,\n",
    "                'trialHash': trialHash\n",
    "            }\n",
    "            row_information = {**row_information, **experiment_params} \n",
    "            print(row_information)\n",
    "\n",
    "            # Train/Load model\n",
    "            model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "\n",
    "            if os.path.exists(CNN.getModelFile(trialName)):\n",
    "                print(\"Model {0} found!\".format(trialName))\n",
    "            else:\n",
    "                initModelPath = CNN.getInitModelFile(experimentPathAndName)\n",
    "                if os.path.exists(initModelPath):\n",
    "                    model.load_state_dict(torch.load(initModelPath))\n",
    "                    print(\"Init Model {0} found!\".format(initModelPath))\n",
    "                CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=detailed_reporting)\n",
    "\n",
    "            # Add to experiments file\n",
    "            if os.path.exists(experimentsFileNameAndPath):\n",
    "                experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "            else:\n",
    "                experiments_df = pd.DataFrame()\n",
    "\n",
    "            record_exists = not (experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName]).empty if not experiments_df.empty else False\n",
    "            if record_exists:\n",
    "                experiments_df.drop(experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName].index, inplace = True) \n",
    "\n",
    "            experiments_df = experiments_df.append(pd.DataFrame(row_information, index=[0]), ignore_index = True)\n",
    "            experiments_df.to_csv(experimentsFileNameAndPath, header=True, index=False)\n",
    "\n",
    "        bar.update()\n",
    "\n",
    "        experiment_index = experiment_index + 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}