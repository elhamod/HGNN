{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath = \"/raid/elhamod/Fish/experiments/\" #\"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/Fish/\" # \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = \"Fish_notpretrained_experiments\" #\"Fish_tripletloss_alpha_lr_experiments\"\n",
    "device = 0\n",
    "detailed_reporting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "experiment:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 0\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': False, 'epochs': 80, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-triplet-hier-notpretrained', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ddeef3bf03431682b499ee31804414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_notpretrained_experiments', 'modelName': 'models/80287d15d196c7c025f3af3ecb36369c73af29d76785e0a726a38b34', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': 'a5f0afaccfa6c8cac8a91410a845173b44644af62693070f899ec0df', 'trialHash': '80287d15d196c7c025f3af3ecb36369c73af29d76785e0a726a38b34', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': False, 'epochs': 80, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-triplet-hier-notpretrained', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">80287d15d196c7c025f3af3ecb36369c73af29d76785e0a726a38b34</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/171c4zod\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/171c4zod</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210527_205316-171c4zod</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/80 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/80 [00:40<?, ?it/s, min_val_loss=inf, train=0.00139, val=0.00542, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 1/80 [00:40<53:33, 40.67s/it, min_val_loss=inf, train=0.00139, val=0.00542, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 1/80 [02:57<53:33, 40.67s/it, min_val_loss=inf, train=0.0023, val=0.00135, val_loss=3.64] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 2/80 [02:57<1:30:13, 69.41s/it, min_val_loss=inf, train=0.0023, val=0.00135, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 2/80 [05:19<1:30:13, 69.41s/it, min_val_loss=741, train=0.0353, val=0.0432, val_loss=3.62] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 3/80 [05:19<1:57:00, 91.18s/it, min_val_loss=741, train=0.0353, val=0.0432, val_loss=3.62]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 3/80 [07:46<1:57:00, 91.18s/it, min_val_loss=23.2, train=0.0715, val=0.0635, val_loss=3.59]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 4/80 [07:46<2:16:50, 108.04s/it, min_val_loss=23.2, train=0.0715, val=0.0635, val_loss=3.59]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 4/80 [10:10<2:16:50, 108.04s/it, min_val_loss=15.8, train=0.104, val=0.0904, val_loss=3.58] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 5/80 [10:10<2:28:28, 118.78s/it, min_val_loss=15.8, train=0.104, val=0.0904, val_loss=3.58]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 5/80 [12:32<2:28:28, 118.78s/it, min_val_loss=11.1, train=0.141, val=0.127, val_loss=3.58] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 6/80 [12:32<2:34:58, 125.65s/it, min_val_loss=11.1, train=0.141, val=0.127, val_loss=3.58]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 6/80 [14:54<2:34:58, 125.65s/it, min_val_loss=7.89, train=0.232, val=0.172, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 7/80 [14:54<2:38:58, 130.66s/it, min_val_loss=7.89, train=0.232, val=0.172, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 7/80 [17:17<2:38:58, 130.66s/it, min_val_loss=5.81, train=0.0967, val=0.0925, val_loss=3.58]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 8/80 [17:17<2:41:10, 134.31s/it, min_val_loss=5.81, train=0.0967, val=0.0925, val_loss=3.58]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 8/80 [19:33<2:41:10, 134.31s/it, min_val_loss=5.81, train=0.178, val=0.186, val_loss=3.53]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  11%|█▏        | 9/80 [19:33<2:39:36, 134.88s/it, min_val_loss=5.81, train=0.178, val=0.186, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  11%|█▏        | 9/80 [21:59<2:39:36, 134.88s/it, min_val_loss=5.38, train=0.298, val=0.25, val_loss=3.52] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 10/80 [21:59<2:41:24, 138.35s/it, min_val_loss=5.38, train=0.298, val=0.25, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 10/80 [24:19<2:41:24, 138.35s/it, min_val_loss=4, train=0.278, val=0.229, val_loss=3.49]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  14%|█▍        | 11/80 [24:19<2:39:31, 138.72s/it, min_val_loss=4, train=0.278, val=0.229, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  14%|█▍        | 11/80 [26:34<2:39:31, 138.72s/it, min_val_loss=4, train=0.24, val=0.226, val_loss=3.49] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 12/80 [26:34<2:35:57, 137.61s/it, min_val_loss=4, train=0.24, val=0.226, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 12/80 [28:51<2:35:57, 137.61s/it, min_val_loss=4, train=0.261, val=0.196, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  16%|█▋        | 13/80 [28:51<2:33:35, 137.54s/it, min_val_loss=4, train=0.261, val=0.196, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  16%|█▋        | 13/80 [31:08<2:33:35, 137.54s/it, min_val_loss=4, train=0.289, val=0.218, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 14/80 [31:08<2:31:10, 137.43s/it, min_val_loss=4, train=0.289, val=0.218, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 14/80 [33:26<2:31:10, 137.43s/it, min_val_loss=4, train=0.359, val=0.238, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  19%|█▉        | 15/80 [33:26<2:28:53, 137.43s/it, min_val_loss=4, train=0.359, val=0.238, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  19%|█▉        | 15/80 [35:42<2:28:53, 137.43s/it, min_val_loss=4, train=0.326, val=0.226, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 16/80 [35:42<2:26:11, 137.05s/it, min_val_loss=4, train=0.326, val=0.226, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 16/80 [37:58<2:26:11, 137.05s/it, min_val_loss=4, train=0.311, val=0.212, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  21%|██▏       | 17/80 [37:58<2:23:30, 136.68s/it, min_val_loss=4, train=0.311, val=0.212, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  21%|██▏       | 17/80 [40:19<2:23:30, 136.68s/it, min_val_loss=4, train=0.249, val=0.201, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 18/80 [40:19<2:22:34, 137.97s/it, min_val_loss=4, train=0.249, val=0.201, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 18/80 [42:33<2:22:34, 137.97s/it, min_val_loss=4, train=0.436, val=0.302, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  24%|██▍       | 19/80 [42:33<2:19:03, 136.77s/it, min_val_loss=4, train=0.436, val=0.302, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  24%|██▍       | 19/80 [44:55<2:19:03, 136.77s/it, min_val_loss=3.31, train=0.256, val=0.197, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 20/80 [44:55<2:18:14, 138.24s/it, min_val_loss=3.31, train=0.256, val=0.197, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 20/80 [47:11<2:18:14, 138.24s/it, min_val_loss=3.31, train=0.501, val=0.341, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  26%|██▋       | 21/80 [47:11<2:15:24, 137.70s/it, min_val_loss=3.31, train=0.501, val=0.341, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  26%|██▋       | 21/80 [49:31<2:15:24, 137.70s/it, min_val_loss=2.94, train=0.513, val=0.319, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 22/80 [49:31<2:13:48, 138.43s/it, min_val_loss=2.94, train=0.513, val=0.319, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 22/80 [51:49<2:13:48, 138.43s/it, min_val_loss=2.94, train=0.309, val=0.199, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  29%|██▉       | 23/80 [51:49<2:11:23, 138.30s/it, min_val_loss=2.94, train=0.309, val=0.199, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  29%|██▉       | 23/80 [54:07<2:11:23, 138.30s/it, min_val_loss=2.94, train=0.159, val=0.11, val_loss=3.5]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 24/80 [54:07<2:08:54, 138.11s/it, min_val_loss=2.94, train=0.159, val=0.11, val_loss=3.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 24/80 [56:21<2:08:54, 138.11s/it, min_val_loss=2.94, train=0.463, val=0.331, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  31%|███▏      | 25/80 [56:21<2:05:36, 137.03s/it, min_val_loss=2.94, train=0.463, val=0.331, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  31%|███▏      | 25/80 [58:36<2:05:36, 137.03s/it, min_val_loss=2.94, train=0.448, val=0.286, val_loss=3.4] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 26/80 [58:36<2:02:37, 136.26s/it, min_val_loss=2.94, train=0.448, val=0.286, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 26/80 [1:00:51<2:02:37, 136.26s/it, min_val_loss=2.94, train=0.542, val=0.355, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  34%|███▍      | 27/80 [1:00:51<2:00:10, 136.05s/it, min_val_loss=2.94, train=0.542, val=0.355, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  34%|███▍      | 27/80 [1:03:10<2:00:10, 136.05s/it, min_val_loss=2.82, train=0.442, val=0.268, val_loss=3.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 28/80 [1:03:10<1:58:29, 136.72s/it, min_val_loss=2.82, train=0.442, val=0.268, val_loss=3.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 28/80 [1:05:27<1:58:29, 136.72s/it, min_val_loss=2.82, train=0.574, val=0.358, val_loss=3.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  36%|███▋      | 29/80 [1:05:27<1:56:23, 136.94s/it, min_val_loss=2.82, train=0.574, val=0.358, val_loss=3.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  36%|███▋      | 29/80 [1:07:44<1:56:23, 136.94s/it, min_val_loss=2.79, train=0.506, val=0.308, val_loss=3.4] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 30/80 [1:07:44<1:54:07, 136.96s/it, min_val_loss=2.79, train=0.506, val=0.308, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 30/80 [1:09:59<1:54:07, 136.96s/it, min_val_loss=2.79, train=0.309, val=0.176, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  39%|███▉      | 31/80 [1:09:59<1:51:22, 136.38s/it, min_val_loss=2.79, train=0.309, val=0.176, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  39%|███▉      | 31/80 [1:12:12<1:51:22, 136.38s/it, min_val_loss=2.79, train=0.308, val=0.169, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 32/80 [1:12:12<1:48:16, 135.34s/it, min_val_loss=2.79, train=0.308, val=0.169, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 32/80 [1:14:29<1:48:16, 135.34s/it, min_val_loss=2.79, train=0.398, val=0.23, val_loss=3.42] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  41%|████▏     | 33/80 [1:14:29<1:46:18, 135.71s/it, min_val_loss=2.79, train=0.398, val=0.23, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  41%|████▏     | 33/80 [1:16:44<1:46:18, 135.71s/it, min_val_loss=2.79, train=0.596, val=0.295, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 34/80 [1:16:44<1:44:04, 135.75s/it, min_val_loss=2.79, train=0.596, val=0.295, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 34/80 [1:19:02<1:44:04, 135.75s/it, min_val_loss=2.79, train=0.549, val=0.281, val_loss=3.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  44%|████▍     | 35/80 [1:19:02<1:42:09, 136.22s/it, min_val_loss=2.79, train=0.549, val=0.281, val_loss=3.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  44%|████▍     | 35/80 [1:21:20<1:42:09, 136.22s/it, min_val_loss=2.79, train=0.647, val=0.32, val_loss=3.37] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 36/80 [1:21:20<1:40:27, 136.99s/it, min_val_loss=2.79, train=0.647, val=0.32, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 36/80 [1:23:39<1:40:27, 136.99s/it, min_val_loss=2.79, train=0.763, val=0.363, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  46%|████▋     | 37/80 [1:23:39<1:38:33, 137.53s/it, min_val_loss=2.79, train=0.763, val=0.363, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  46%|████▋     | 37/80 [1:26:04<1:38:33, 137.53s/it, min_val_loss=2.75, train=0.751, val=0.348, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 38/80 [1:26:04<1:37:43, 139.60s/it, min_val_loss=2.75, train=0.751, val=0.348, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 38/80 [1:28:25<1:37:43, 139.60s/it, min_val_loss=2.75, train=0.557, val=0.25, val_loss=3.39] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  49%|████▉     | 39/80 [1:28:25<1:35:48, 140.22s/it, min_val_loss=2.75, train=0.557, val=0.25, val_loss=3.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  49%|████▉     | 39/80 [1:30:47<1:35:48, 140.22s/it, min_val_loss=2.75, train=0.855, val=0.404, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 40/80 [1:30:47<1:33:41, 140.54s/it, min_val_loss=2.75, train=0.855, val=0.404, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 40/80 [1:33:47<1:33:41, 140.54s/it, min_val_loss=2.47, train=0.902, val=0.422, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  51%|█████▏    | 41/80 [1:33:47<1:39:04, 152.41s/it, min_val_loss=2.47, train=0.902, val=0.422, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  51%|█████▏    | 41/80 [1:36:37<1:39:04, 152.41s/it, min_val_loss=2.37, train=0.945, val=0.458, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 42/80 [1:36:37<1:40:00, 157.90s/it, min_val_loss=2.37, train=0.945, val=0.458, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 42/80 [1:39:07<1:40:00, 157.90s/it, min_val_loss=2.18, train=0.637, val=0.321, val_loss=3.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  54%|█████▍    | 43/80 [1:39:07<1:35:49, 155.38s/it, min_val_loss=2.18, train=0.637, val=0.321, val_loss=3.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  54%|█████▍    | 43/80 [1:41:31<1:35:49, 155.38s/it, min_val_loss=2.18, train=0.633, val=0.282, val_loss=3.4] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 44/80 [1:41:31<1:31:11, 151.99s/it, min_val_loss=2.18, train=0.633, val=0.282, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 44/80 [1:43:53<1:31:11, 151.99s/it, min_val_loss=2.18, train=0.827, val=0.361, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  56%|█████▋    | 45/80 [1:43:53<1:26:50, 148.86s/it, min_val_loss=2.18, train=0.827, val=0.361, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  56%|█████▋    | 45/80 [1:46:17<1:26:50, 148.86s/it, min_val_loss=2.18, train=0.79, val=0.396, val_loss=3.3] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 46/80 [1:46:17<1:23:38, 147.59s/it, min_val_loss=2.18, train=0.79, val=0.396, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 46/80 [1:48:40<1:23:38, 147.59s/it, min_val_loss=2.18, train=0.642, val=0.289, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  59%|█████▉    | 47/80 [1:48:40<1:20:26, 146.25s/it, min_val_loss=2.18, train=0.642, val=0.289, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  59%|█████▉    | 47/80 [1:51:02<1:20:26, 146.25s/it, min_val_loss=2.18, train=0.816, val=0.346, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 48/80 [1:51:02<1:17:17, 144.92s/it, min_val_loss=2.18, train=0.816, val=0.346, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 48/80 [1:53:26<1:17:17, 144.92s/it, min_val_loss=2.18, train=0.867, val=0.375, val_loss=3.3] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  61%|██████▏   | 49/80 [1:53:26<1:14:43, 144.64s/it, min_val_loss=2.18, train=0.867, val=0.375, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  61%|██████▏   | 49/80 [1:55:49<1:14:43, 144.64s/it, min_val_loss=2.18, train=0.87, val=0.431, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 50/80 [1:55:49<1:12:01, 144.06s/it, min_val_loss=2.18, train=0.87, val=0.431, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 50/80 [1:58:13<1:12:01, 144.06s/it, min_val_loss=2.18, train=0.989, val=0.502, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  64%|██████▍   | 51/80 [1:58:13<1:09:36, 144.00s/it, min_val_loss=2.18, train=0.989, val=0.502, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  64%|██████▍   | 51/80 [2:00:42<1:09:36, 144.00s/it, min_val_loss=1.99, train=0.965, val=0.505, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 52/80 [2:00:42<1:07:59, 145.68s/it, min_val_loss=1.99, train=0.965, val=0.505, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 52/80 [2:03:50<1:07:59, 145.68s/it, min_val_loss=1.98, train=0.903, val=0.4, val_loss=3.27]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  66%|██████▋   | 53/80 [2:03:50<1:11:12, 158.26s/it, min_val_loss=1.98, train=0.903, val=0.4, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  66%|██████▋   | 53/80 [2:06:23<1:11:12, 158.26s/it, min_val_loss=1.98, train=0.979, val=0.52, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 54/80 [2:06:23<1:07:50, 156.56s/it, min_val_loss=1.98, train=0.979, val=0.52, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 54/80 [2:08:52<1:07:50, 156.56s/it, min_val_loss=1.92, train=0.874, val=0.436, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  69%|██████▉   | 55/80 [2:08:52<1:04:17, 154.32s/it, min_val_loss=1.92, train=0.874, val=0.436, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  69%|██████▉   | 55/80 [2:11:11<1:04:17, 154.32s/it, min_val_loss=1.92, train=0.953, val=0.403, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 56/80 [2:11:11<59:55, 149.82s/it, min_val_loss=1.92, train=0.953, val=0.403, val_loss=3.25]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 56/80 [2:13:32<59:55, 149.82s/it, min_val_loss=1.92, train=0.704, val=0.325, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  71%|███████▏  | 57/80 [2:13:32<56:27, 147.30s/it, min_val_loss=1.92, train=0.704, val=0.325, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  71%|███████▏  | 57/80 [2:15:53<56:27, 147.30s/it, min_val_loss=1.92, train=0.752, val=0.289, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 58/80 [2:15:53<53:14, 145.21s/it, min_val_loss=1.92, train=0.752, val=0.289, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 58/80 [2:18:16<53:14, 145.21s/it, min_val_loss=1.92, train=0.732, val=0.31, val_loss=3.35] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  74%|███████▍  | 59/80 [2:18:16<50:34, 144.49s/it, min_val_loss=1.92, train=0.732, val=0.31, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  74%|███████▍  | 59/80 [2:20:40<50:34, 144.49s/it, min_val_loss=1.92, train=0.652, val=0.223, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 60/80 [2:20:40<48:07, 144.36s/it, min_val_loss=1.92, train=0.652, val=0.223, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 60/80 [2:23:04<48:07, 144.36s/it, min_val_loss=1.92, train=0.897, val=0.386, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  76%|███████▋  | 61/80 [2:23:04<45:40, 144.24s/it, min_val_loss=1.92, train=0.897, val=0.386, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  76%|███████▋  | 61/80 [2:25:27<45:40, 144.24s/it, min_val_loss=1.92, train=0.908, val=0.414, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 62/80 [2:25:27<43:13, 144.07s/it, min_val_loss=1.92, train=0.908, val=0.414, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 62/80 [2:27:47<43:13, 144.07s/it, min_val_loss=1.92, train=0.889, val=0.432, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  79%|███████▉  | 63/80 [2:27:47<40:26, 142.74s/it, min_val_loss=1.92, train=0.889, val=0.432, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  79%|███████▉  | 63/80 [2:30:07<40:26, 142.74s/it, min_val_loss=1.92, train=0.844, val=0.355, val_loss=3.3] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 64/80 [2:30:07<37:51, 141.99s/it, min_val_loss=1.92, train=0.844, val=0.355, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 64/80 [2:32:29<37:51, 141.99s/it, min_val_loss=1.92, train=0.74, val=0.316, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  81%|████████▏ | 65/80 [2:32:29<35:30, 142.06s/it, min_val_loss=1.92, train=0.74, val=0.316, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  81%|████████▏ | 65/80 [2:34:53<35:30, 142.06s/it, min_val_loss=1.92, train=0.974, val=0.464, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 66/80 [2:34:53<33:15, 142.53s/it, min_val_loss=1.92, train=0.974, val=0.464, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 66/80 [2:37:17<33:15, 142.53s/it, min_val_loss=1.92, train=0.998, val=0.538, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  84%|████████▍ | 67/80 [2:37:17<30:59, 143.01s/it, min_val_loss=1.92, train=0.998, val=0.538, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  84%|████████▍ | 67/80 [2:39:46<30:59, 143.01s/it, min_val_loss=1.86, train=0.998, val=0.611, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 68/80 [2:39:46<28:57, 144.82s/it, min_val_loss=1.86, train=0.998, val=0.611, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 68/80 [2:42:13<28:57, 144.82s/it, min_val_loss=1.64, train=1, val=0.603, val_loss=3.14]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  86%|████████▋ | 69/80 [2:42:13<26:38, 145.34s/it, min_val_loss=1.64, train=1, val=0.603, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  86%|████████▋ | 69/80 [2:44:37<26:38, 145.34s/it, min_val_loss=1.64, train=0.998, val=0.588, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 70/80 [2:44:37<24:09, 144.95s/it, min_val_loss=1.64, train=0.998, val=0.588, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 70/80 [2:46:58<24:09, 144.95s/it, min_val_loss=1.64, train=0.999, val=0.585, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  89%|████████▉ | 71/80 [2:46:58<21:35, 143.96s/it, min_val_loss=1.64, train=0.999, val=0.585, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  89%|████████▉ | 71/80 [2:49:19<21:35, 143.96s/it, min_val_loss=1.64, train=1, val=0.599, val_loss=3.14]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 72/80 [2:49:19<19:04, 143.01s/it, min_val_loss=1.64, train=1, val=0.599, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 72/80 [2:51:41<19:04, 143.01s/it, min_val_loss=1.64, train=0.985, val=0.502, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  91%|█████████▏| 73/80 [2:51:41<16:39, 142.73s/it, min_val_loss=1.64, train=0.985, val=0.502, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  91%|█████████▏| 73/80 [2:54:03<16:39, 142.73s/it, min_val_loss=1.64, train=0.998, val=0.577, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 74/80 [2:54:03<14:14, 142.39s/it, min_val_loss=1.64, train=0.998, val=0.577, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 74/80 [2:56:30<14:14, 142.39s/it, min_val_loss=1.64, train=1, val=0.596, val_loss=3.13]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  94%|█████████▍| 75/80 [2:56:30<11:58, 143.69s/it, min_val_loss=1.64, train=1, val=0.596, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  94%|█████████▍| 75/80 [2:58:51<11:58, 143.69s/it, min_val_loss=1.64, train=0.999, val=0.575, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 76/80 [2:58:51<09:31, 142.99s/it, min_val_loss=1.64, train=0.999, val=0.575, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 76/80 [3:01:19<09:31, 142.99s/it, min_val_loss=1.64, train=1, val=0.601, val_loss=3.13]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  96%|█████████▋| 77/80 [3:01:19<07:13, 144.40s/it, min_val_loss=1.64, train=1, val=0.601, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  96%|█████████▋| 77/80 [3:03:43<07:13, 144.40s/it, min_val_loss=1.64, train=1, val=0.581, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 78/80 [3:03:43<04:48, 144.49s/it, min_val_loss=1.64, train=1, val=0.581, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 78/80 [3:06:06<04:48, 144.49s/it, min_val_loss=1.64, train=0.993, val=0.536, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  99%|█████████▉| 79/80 [3:06:06<02:23, 143.99s/it, min_val_loss=1.64, train=0.993, val=0.536, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  99%|█████████▉| 79/80 [3:08:29<02:23, 143.99s/it, min_val_loss=1.64, train=1, val=0.62, val_loss=3.11]     \u001b[A\u001b[A\n",
      "\n",
      "iteration: 100%|██████████| 80/80 [3:09:07<00:00, 141.84s/it, min_val_loss=1.64, train=1, val=0.62, val_loss=3.11]\u001b[A\u001b[A\n",
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/ipykernel_launcher.py:113: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 71824<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa709c9eabb34e049355dbf8055ed502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210527_205316-171c4zod/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210527_205316-171c4zod/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>79</td></tr><tr><td>learning rate</td><td>0.00025</td></tr><tr><td>validation_fine_f1</td><td>0.61954</td></tr><tr><td>training_fine_f1</td><td>1.0</td></tr><tr><td>training_fine_acc</td><td>1.0</td></tr><tr><td>test_fine_f1</td><td>0.6338</td></tr><tr><td>test_fine_acc</td><td>0.63684</td></tr><tr><td>validation_loss</td><td>3.10976</td></tr><tr><td>_runtime</td><td>11315</td></tr><tr><td>_timestamp</td><td>1622174511</td></tr><tr><td>_step</td><td>869</td></tr><tr><td>loss</td><td>0.04063</td></tr><tr><td>batch</td><td>799</td></tr><tr><td>loss_fine</td><td>0.00233</td></tr><tr><td>lambda_fine</td><td>0.82812</td></tr><tr><td>loss_layer2</td><td>0.17073</td></tr><tr><td>lambda_layer2</td><td>0.14137</td></tr><tr><td>nonzerotriplets_layer2</td><td>134</td></tr><tr><td>loss_layer4</td><td>0.47735</td></tr><tr><td>lambda_layer4</td><td>0.03052</td></tr><tr><td>nonzerotriplets_layer4</td><td>38</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>████████████████████████████████▃▃▃▃▃▃▃▁</td></tr><tr><td>validation_fine_f1</td><td>▁▁▂▃▃▄▃▄▃▄▅▃▅▅▅▃▄▄▅▄▆▅▅▄▅▇▅▆▅▄▅▆▅▇██▇███</td></tr><tr><td>training_fine_f1</td><td>▁▁▂▃▂▃▃▄▃▄▄▃▄▅▅▃▄▅▆▅▇▅▇▅▇█▇▇▆▆▇▇▆███████</td></tr><tr><td>training_fine_acc</td><td>▁▁▂▃▂▃▃▄▃▄▅▃▄▅▅▃▄▅▆▅▇▆▇▅▇█▇▇▆▆▇▇▆███████</td></tr><tr><td>test_fine_f1</td><td>▁▁▂▃▃▄▃▄▃▅▅▃▅▅▅▃▃▄▅▄▆▅▆▄▆▇▆▆▄▅▅▆▅▇██▇▇██</td></tr><tr><td>test_fine_acc</td><td>▁▂▂▄▃▄▄▄▄▅▅▄▅▅▆▄▄▅▆▅▇▅▆▄▆▇▆▆▅▅▆▇▅▇██▇███</td></tr><tr><td>validation_loss</td><td>██▇▇▇▆▆▅▆▅▅▆▄▄▄▅▅▅▄▅▃▄▄▅▄▂▃▃▄▄▃▂▅▂▁▁▂▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▇▆▆▅▅▄▄▄▄▃▄▃▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▇▆▆▆▅▅▄▄▃▃▃▃▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▂▁▁▃▅▅▄▇▆▇▇██▇▇█▆▇▇▇▇▇▆▆▆▆█▆▇▇</td></tr><tr><td>loss_layer2</td><td>▇▆▆▆▅▅▇▅▄▆▅▃▅▄█▆▅▄▂▄▃▃▄▃▂▃▁▂▂▂▂▂▁▁▁▁▄▁▁▁</td></tr><tr><td>lambda_layer2</td><td>█▇▆▅▇▇▂▆█▃▅▇▄▅▁▂▂▁▃▂▂▂▁▂▂▂▃▂▂▂▃▂▃▃▃▃▁▃▃▃</td></tr><tr><td>nonzerotriplets_layer2</td><td>█████████▃██▃▂▂███▇▂▂▂▁▁▃▂▄▃▁▁▁▁▄▁▄▂▇▁▂▁</td></tr><tr><td>loss_layer4</td><td>█▆▅▄▅▅▃▄▄▃▃▃▂▃▄▃▂▂▃▁▂▂▂▃▂▃▂▂▃▂▂▂▂▃▂▁▁▁▃▃</td></tr><tr><td>lambda_layer4</td><td>▁▃▄▅▃▂█▃▂▇▄▂▆▂▅▄▄▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">80287d15d196c7c025f3af3ecb36369c73af29d76785e0a726a38b34</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/171c4zod\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/171c4zod</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  33%|███▎      | 1/3 [3:09:17<6:18:35, 11357.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': False, 'epochs': 80, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-BB-notpretrained', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8024bb7d20944f8a0e402b2f881fac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_notpretrained_experiments', 'modelName': 'models/33ee681a7cebc7f84b51b7d1c74cbce191ffeb27ecec3e22de24c0e1', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '6168ab49cc478cb33b9b8db750fb3636399ce8050c10d91444c668f6', 'trialHash': '33ee681a7cebc7f84b51b7d1c74cbce191ffeb27ecec3e22de24c0e1', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': False, 'epochs': 80, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-BB-notpretrained', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">33ee681a7cebc7f84b51b7d1c74cbce191ffeb27ecec3e22de24c0e1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1mllwkra\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1mllwkra</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210528_000233-1mllwkra</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/80 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/80 [00:53<?, ?it/s, min_val_loss=inf, train=0.00135, val=0.00135, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 1/80 [00:53<1:10:11, 53.30s/it, min_val_loss=inf, train=0.00135, val=0.00135, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 1/80 [02:09<1:10:11, 53.30s/it, min_val_loss=inf, train=0.00724, val=0.00901, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 2/80 [02:09<1:18:09, 60.12s/it, min_val_loss=inf, train=0.00724, val=0.00901, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 2/80 [03:32<1:18:09, 60.12s/it, min_val_loss=111, train=0.0189, val=0.0165, val_loss=3.62]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 3/80 [03:32<1:26:01, 67.03s/it, min_val_loss=111, train=0.0189, val=0.0165, val_loss=3.62]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 3/80 [04:50<1:26:01, 67.03s/it, min_val_loss=60.7, train=0.0731, val=0.0854, val_loss=3.6]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 4/80 [04:50<1:29:06, 70.35s/it, min_val_loss=60.7, train=0.0731, val=0.0854, val_loss=3.6]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 4/80 [06:09<1:29:06, 70.35s/it, min_val_loss=11.7, train=0.0866, val=0.0785, val_loss=3.59]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 5/80 [06:09<1:31:16, 73.02s/it, min_val_loss=11.7, train=0.0866, val=0.0785, val_loss=3.59]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 5/80 [07:29<1:31:16, 73.02s/it, min_val_loss=11.7, train=0.0978, val=0.0847, val_loss=3.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 6/80 [07:29<1:32:42, 75.17s/it, min_val_loss=11.7, train=0.0978, val=0.0847, val_loss=3.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 6/80 [08:47<1:32:42, 75.17s/it, min_val_loss=11.7, train=0.0747, val=0.0548, val_loss=3.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 7/80 [08:47<1:32:11, 75.77s/it, min_val_loss=11.7, train=0.0747, val=0.0548, val_loss=3.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 7/80 [10:06<1:32:11, 75.77s/it, min_val_loss=11.7, train=0.151, val=0.132, val_loss=3.53]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 8/80 [10:06<1:32:08, 76.79s/it, min_val_loss=11.7, train=0.151, val=0.132, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 8/80 [11:25<1:32:08, 76.79s/it, min_val_loss=7.58, train=0.185, val=0.159, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  11%|█▏        | 9/80 [11:25<1:31:35, 77.40s/it, min_val_loss=7.58, train=0.185, val=0.159, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  11%|█▏        | 9/80 [12:44<1:31:35, 77.40s/it, min_val_loss=6.31, train=0.281, val=0.245, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 10/80 [12:44<1:31:07, 78.11s/it, min_val_loss=6.31, train=0.281, val=0.245, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 10/80 [14:02<1:31:07, 78.11s/it, min_val_loss=4.08, train=0.175, val=0.153, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  14%|█▍        | 11/80 [14:02<1:29:44, 78.03s/it, min_val_loss=4.08, train=0.175, val=0.153, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  14%|█▍        | 11/80 [15:20<1:29:44, 78.03s/it, min_val_loss=4.08, train=0.278, val=0.203, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 12/80 [15:20<1:28:11, 77.81s/it, min_val_loss=4.08, train=0.278, val=0.203, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 12/80 [16:38<1:28:11, 77.81s/it, min_val_loss=4.08, train=0.446, val=0.291, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  16%|█▋        | 13/80 [16:38<1:26:57, 77.87s/it, min_val_loss=4.08, train=0.446, val=0.291, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  16%|█▋        | 13/80 [17:57<1:26:57, 77.87s/it, min_val_loss=3.44, train=0.218, val=0.139, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 14/80 [17:57<1:26:03, 78.23s/it, min_val_loss=3.44, train=0.218, val=0.139, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 14/80 [19:17<1:26:03, 78.23s/it, min_val_loss=3.44, train=0.315, val=0.225, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  19%|█▉        | 15/80 [19:17<1:25:20, 78.78s/it, min_val_loss=3.44, train=0.315, val=0.225, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  19%|█▉        | 15/80 [20:32<1:25:20, 78.78s/it, min_val_loss=3.44, train=0.259, val=0.186, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 16/80 [20:32<1:22:52, 77.70s/it, min_val_loss=3.44, train=0.259, val=0.186, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 16/80 [21:54<1:22:52, 77.70s/it, min_val_loss=3.44, train=0.264, val=0.199, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  21%|██▏       | 17/80 [21:54<1:22:59, 79.03s/it, min_val_loss=3.44, train=0.264, val=0.199, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  21%|██▏       | 17/80 [23:08<1:22:59, 79.03s/it, min_val_loss=3.44, train=0.251, val=0.183, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 18/80 [23:08<1:20:14, 77.65s/it, min_val_loss=3.44, train=0.251, val=0.183, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 18/80 [24:27<1:20:14, 77.65s/it, min_val_loss=3.44, train=0.356, val=0.273, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  24%|██▍       | 19/80 [24:27<1:19:13, 77.93s/it, min_val_loss=3.44, train=0.356, val=0.273, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  24%|██▍       | 19/80 [25:41<1:19:13, 77.93s/it, min_val_loss=3.44, train=0.243, val=0.184, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 20/80 [25:41<1:16:47, 76.79s/it, min_val_loss=3.44, train=0.243, val=0.184, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 20/80 [27:01<1:16:47, 76.79s/it, min_val_loss=3.44, train=0.223, val=0.159, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  26%|██▋       | 21/80 [27:01<1:16:32, 77.83s/it, min_val_loss=3.44, train=0.223, val=0.159, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  26%|██▋       | 21/80 [28:18<1:16:32, 77.83s/it, min_val_loss=3.44, train=0.356, val=0.228, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 22/80 [28:18<1:14:57, 77.54s/it, min_val_loss=3.44, train=0.356, val=0.228, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 22/80 [29:34<1:14:57, 77.54s/it, min_val_loss=3.44, train=0.526, val=0.324, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  29%|██▉       | 23/80 [29:34<1:13:14, 77.10s/it, min_val_loss=3.44, train=0.526, val=0.324, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  29%|██▉       | 23/80 [30:52<1:13:14, 77.10s/it, min_val_loss=3.09, train=0.515, val=0.346, val_loss=3.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 24/80 [30:52<1:12:10, 77.33s/it, min_val_loss=3.09, train=0.515, val=0.346, val_loss=3.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 24/80 [32:11<1:12:10, 77.33s/it, min_val_loss=2.89, train=0.265, val=0.15, val_loss=3.46] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  31%|███▏      | 25/80 [32:11<1:11:12, 77.68s/it, min_val_loss=2.89, train=0.265, val=0.15, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  31%|███▏      | 25/80 [33:31<1:11:12, 77.68s/it, min_val_loss=2.89, train=0.385, val=0.229, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 26/80 [33:31<1:10:37, 78.48s/it, min_val_loss=2.89, train=0.385, val=0.229, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 26/80 [34:47<1:10:37, 78.48s/it, min_val_loss=2.89, train=0.352, val=0.183, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  34%|███▍      | 27/80 [34:47<1:08:31, 77.58s/it, min_val_loss=2.89, train=0.352, val=0.183, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  34%|███▍      | 27/80 [36:08<1:08:31, 77.58s/it, min_val_loss=2.89, train=0.487, val=0.274, val_loss=3.4] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 28/80 [36:08<1:08:20, 78.86s/it, min_val_loss=2.89, train=0.487, val=0.274, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 28/80 [37:26<1:08:20, 78.86s/it, min_val_loss=2.89, train=0.616, val=0.296, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  36%|███▋      | 29/80 [37:26<1:06:39, 78.42s/it, min_val_loss=2.89, train=0.616, val=0.296, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  36%|███▋      | 29/80 [38:44<1:06:39, 78.42s/it, min_val_loss=2.89, train=0.583, val=0.32, val_loss=3.37] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 30/80 [38:44<1:05:21, 78.42s/it, min_val_loss=2.89, train=0.583, val=0.32, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 30/80 [40:02<1:05:21, 78.42s/it, min_val_loss=2.89, train=0.56, val=0.284, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  39%|███▉      | 31/80 [40:02<1:03:59, 78.35s/it, min_val_loss=2.89, train=0.56, val=0.284, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  39%|███▉      | 31/80 [41:20<1:03:59, 78.35s/it, min_val_loss=2.89, train=0.276, val=0.157, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 32/80 [41:20<1:02:31, 78.15s/it, min_val_loss=2.89, train=0.276, val=0.157, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 32/80 [42:37<1:02:31, 78.15s/it, min_val_loss=2.89, train=0.381, val=0.21, val_loss=3.46] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  41%|████▏     | 33/80 [42:37<1:00:59, 77.85s/it, min_val_loss=2.89, train=0.381, val=0.21, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  41%|████▏     | 33/80 [43:52<1:00:59, 77.85s/it, min_val_loss=2.89, train=0.592, val=0.247, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 34/80 [43:52<59:01, 76.98s/it, min_val_loss=2.89, train=0.592, val=0.247, val_loss=3.41]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 34/80 [45:11<59:01, 76.98s/it, min_val_loss=2.89, train=0.669, val=0.347, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  44%|████▍     | 35/80 [45:11<58:06, 77.48s/it, min_val_loss=2.89, train=0.669, val=0.347, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  44%|████▍     | 35/80 [46:29<58:06, 77.48s/it, min_val_loss=2.88, train=0.794, val=0.341, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 36/80 [46:29<56:57, 77.68s/it, min_val_loss=2.88, train=0.794, val=0.341, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 36/80 [47:49<56:57, 77.68s/it, min_val_loss=2.88, train=0.831, val=0.398, val_loss=3.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  46%|████▋     | 37/80 [47:49<56:10, 78.39s/it, min_val_loss=2.88, train=0.831, val=0.398, val_loss=3.26]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  46%|████▋     | 37/80 [49:06<56:10, 78.39s/it, min_val_loss=2.51, train=0.882, val=0.44, val_loss=3.29] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 38/80 [49:06<54:31, 77.90s/it, min_val_loss=2.51, train=0.882, val=0.44, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 38/80 [50:24<54:31, 77.90s/it, min_val_loss=2.27, train=0.776, val=0.379, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  49%|████▉     | 39/80 [50:24<53:13, 77.89s/it, min_val_loss=2.27, train=0.776, val=0.379, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  49%|████▉     | 39/80 [51:43<53:13, 77.89s/it, min_val_loss=2.27, train=0.855, val=0.405, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 40/80 [51:43<52:13, 78.34s/it, min_val_loss=2.27, train=0.855, val=0.405, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 40/80 [53:03<52:13, 78.34s/it, min_val_loss=2.27, train=0.397, val=0.175, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  51%|█████▏    | 41/80 [53:03<51:11, 78.76s/it, min_val_loss=2.27, train=0.397, val=0.175, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  51%|█████▏    | 41/80 [54:24<51:11, 78.76s/it, min_val_loss=2.27, train=0.772, val=0.347, val_loss=3.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 42/80 [54:24<50:20, 79.49s/it, min_val_loss=2.27, train=0.772, val=0.347, val_loss=3.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 42/80 [55:42<50:20, 79.49s/it, min_val_loss=2.27, train=0.673, val=0.314, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  54%|█████▍    | 43/80 [55:42<48:39, 78.92s/it, min_val_loss=2.27, train=0.673, val=0.314, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  54%|█████▍    | 43/80 [57:00<48:39, 78.92s/it, min_val_loss=2.27, train=0.491, val=0.245, val_loss=3.4] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 44/80 [57:00<47:16, 78.80s/it, min_val_loss=2.27, train=0.491, val=0.245, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 44/80 [58:18<47:16, 78.80s/it, min_val_loss=2.27, train=0.37, val=0.186, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  56%|█████▋    | 45/80 [58:18<45:46, 78.46s/it, min_val_loss=2.27, train=0.37, val=0.186, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  56%|█████▋    | 45/80 [59:36<45:46, 78.46s/it, min_val_loss=2.27, train=0.621, val=0.305, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 46/80 [59:36<44:24, 78.38s/it, min_val_loss=2.27, train=0.621, val=0.305, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 46/80 [1:00:50<44:24, 78.38s/it, min_val_loss=2.27, train=0.908, val=0.409, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  59%|█████▉    | 47/80 [1:00:50<42:21, 77.01s/it, min_val_loss=2.27, train=0.908, val=0.409, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  59%|█████▉    | 47/80 [1:02:07<42:21, 77.01s/it, min_val_loss=2.27, train=0.914, val=0.441, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 48/80 [1:02:07<41:02, 76.94s/it, min_val_loss=2.27, train=0.914, val=0.441, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 48/80 [1:03:25<41:02, 76.94s/it, min_val_loss=2.27, train=0.906, val=0.428, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  61%|██████▏   | 49/80 [1:03:25<39:56, 77.31s/it, min_val_loss=2.27, train=0.906, val=0.428, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  61%|██████▏   | 49/80 [1:04:41<39:56, 77.31s/it, min_val_loss=2.27, train=0.877, val=0.412, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 50/80 [1:04:41<38:32, 77.08s/it, min_val_loss=2.27, train=0.877, val=0.412, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 50/80 [1:06:02<38:32, 77.08s/it, min_val_loss=2.27, train=0.755, val=0.342, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  64%|██████▍   | 51/80 [1:06:02<37:43, 78.04s/it, min_val_loss=2.27, train=0.755, val=0.342, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  64%|██████▍   | 51/80 [1:07:16<37:43, 78.04s/it, min_val_loss=2.27, train=0.939, val=0.415, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 52/80 [1:07:16<35:58, 77.09s/it, min_val_loss=2.27, train=0.939, val=0.415, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 52/80 [1:08:36<35:58, 77.09s/it, min_val_loss=2.27, train=0.378, val=0.226, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  66%|██████▋   | 53/80 [1:08:36<35:02, 77.88s/it, min_val_loss=2.27, train=0.378, val=0.226, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  66%|██████▋   | 53/80 [1:09:52<35:02, 77.88s/it, min_val_loss=2.27, train=0.362, val=0.155, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 54/80 [1:09:52<33:31, 77.36s/it, min_val_loss=2.27, train=0.362, val=0.155, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 54/80 [1:11:10<33:31, 77.36s/it, min_val_loss=2.27, train=0.513, val=0.214, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  69%|██████▉   | 55/80 [1:11:10<32:20, 77.61s/it, min_val_loss=2.27, train=0.513, val=0.214, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  69%|██████▉   | 55/80 [1:12:29<32:20, 77.61s/it, min_val_loss=2.27, train=0.388, val=0.194, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 56/80 [1:12:29<31:09, 77.88s/it, min_val_loss=2.27, train=0.388, val=0.194, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 56/80 [1:13:43<31:09, 77.88s/it, min_val_loss=2.27, train=0.449, val=0.201, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  71%|███████▏  | 57/80 [1:13:43<29:22, 76.62s/it, min_val_loss=2.27, train=0.449, val=0.201, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  71%|███████▏  | 57/80 [1:14:59<29:22, 76.62s/it, min_val_loss=2.27, train=0.513, val=0.184, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 58/80 [1:14:59<28:06, 76.66s/it, min_val_loss=2.27, train=0.513, val=0.184, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 58/80 [1:16:13<28:06, 76.66s/it, min_val_loss=2.27, train=0.389, val=0.204, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  74%|███████▍  | 59/80 [1:16:13<26:33, 75.88s/it, min_val_loss=2.27, train=0.389, val=0.204, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  74%|███████▍  | 59/80 [1:17:32<26:33, 75.88s/it, min_val_loss=2.27, train=0.913, val=0.464, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 60/80 [1:17:32<25:30, 76.53s/it, min_val_loss=2.27, train=0.913, val=0.464, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 60/80 [1:18:51<25:30, 76.53s/it, min_val_loss=2.16, train=0.949, val=0.472, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  76%|███████▋  | 61/80 [1:18:51<24:29, 77.36s/it, min_val_loss=2.16, train=0.949, val=0.472, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  76%|███████▋  | 61/80 [1:20:10<24:29, 77.36s/it, min_val_loss=2.12, train=0.998, val=0.542, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 62/80 [1:20:10<23:19, 77.77s/it, min_val_loss=2.12, train=0.998, val=0.542, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 62/80 [1:21:28<23:19, 77.77s/it, min_val_loss=1.84, train=0.996, val=0.537, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  79%|███████▉  | 63/80 [1:21:28<22:07, 78.11s/it, min_val_loss=1.84, train=0.996, val=0.537, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  79%|███████▉  | 63/80 [1:22:47<22:07, 78.11s/it, min_val_loss=1.84, train=1, val=0.6, val_loss=3.13]      \u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 64/80 [1:22:47<20:50, 78.16s/it, min_val_loss=1.84, train=1, val=0.6, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 64/80 [1:24:04<20:50, 78.16s/it, min_val_loss=1.67, train=1, val=0.575, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  81%|████████▏ | 65/80 [1:24:04<19:28, 77.92s/it, min_val_loss=1.67, train=1, val=0.575, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  81%|████████▏ | 65/80 [1:25:17<19:28, 77.92s/it, min_val_loss=1.67, train=1, val=0.616, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 66/80 [1:25:17<17:49, 76.43s/it, min_val_loss=1.67, train=1, val=0.616, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 66/80 [1:26:36<17:49, 76.43s/it, min_val_loss=1.62, train=1, val=0.596, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  84%|████████▍ | 67/80 [1:26:36<16:44, 77.30s/it, min_val_loss=1.62, train=1, val=0.596, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  84%|████████▍ | 67/80 [1:27:53<16:44, 77.30s/it, min_val_loss=1.62, train=1, val=0.595, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 68/80 [1:27:53<15:25, 77.08s/it, min_val_loss=1.62, train=1, val=0.595, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 68/80 [1:29:05<15:25, 77.08s/it, min_val_loss=1.62, train=1, val=0.627, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  86%|████████▋ | 69/80 [1:29:05<13:50, 75.49s/it, min_val_loss=1.62, train=1, val=0.627, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  86%|████████▋ | 69/80 [1:30:23<13:50, 75.49s/it, min_val_loss=1.6, train=1, val=0.62, val_loss=3.12]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 70/80 [1:30:23<12:41, 76.19s/it, min_val_loss=1.6, train=1, val=0.62, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 70/80 [1:31:38<12:41, 76.19s/it, min_val_loss=1.6, train=0.999, val=0.613, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  89%|████████▉ | 71/80 [1:31:38<11:22, 75.88s/it, min_val_loss=1.6, train=0.999, val=0.613, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  89%|████████▉ | 71/80 [1:32:53<11:22, 75.88s/it, min_val_loss=1.6, train=1, val=0.604, val_loss=3.11]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 72/80 [1:32:53<10:06, 75.77s/it, min_val_loss=1.6, train=1, val=0.604, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 72/80 [1:34:11<10:06, 75.77s/it, min_val_loss=1.6, train=1, val=0.623, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  91%|█████████▏| 73/80 [1:34:11<08:54, 76.34s/it, min_val_loss=1.6, train=1, val=0.623, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  91%|█████████▏| 73/80 [1:35:28<08:54, 76.34s/it, min_val_loss=1.6, train=1, val=0.627, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 74/80 [1:35:28<07:39, 76.66s/it, min_val_loss=1.6, train=1, val=0.627, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 74/80 [1:36:43<07:39, 76.66s/it, min_val_loss=1.59, train=1, val=0.636, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  94%|█████████▍| 75/80 [1:36:43<06:20, 76.13s/it, min_val_loss=1.59, train=1, val=0.636, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  94%|█████████▍| 75/80 [1:37:59<06:20, 76.13s/it, min_val_loss=1.57, train=1, val=0.596, val_loss=3.1] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 76/80 [1:37:59<05:04, 76.18s/it, min_val_loss=1.57, train=1, val=0.596, val_loss=3.1]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 76/80 [1:39:11<05:04, 76.18s/it, min_val_loss=1.57, train=1, val=0.626, val_loss=3.1]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  96%|█████████▋| 77/80 [1:39:11<03:44, 74.74s/it, min_val_loss=1.57, train=1, val=0.626, val_loss=3.1]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  96%|█████████▋| 77/80 [1:40:28<03:44, 74.74s/it, min_val_loss=1.57, train=1, val=0.62, val_loss=3.1] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 78/80 [1:40:28<02:30, 75.43s/it, min_val_loss=1.57, train=1, val=0.62, val_loss=3.1]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 78/80 [1:41:44<02:30, 75.43s/it, min_val_loss=1.57, train=1, val=0.614, val_loss=3.1]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  99%|█████████▉| 79/80 [1:41:44<01:15, 75.71s/it, min_val_loss=1.57, train=1, val=0.614, val_loss=3.1]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  99%|█████████▉| 79/80 [1:43:03<01:15, 75.71s/it, min_val_loss=1.57, train=1, val=0.604, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration: 100%|██████████| 80/80 [1:43:29<00:00, 77.61s/it, min_val_loss=1.57, train=1, val=0.604, val_loss=3.12]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30402<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bad3bd6c94c4e54814df52770b34709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210528_000233-1mllwkra/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210528_000233-1mllwkra/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>79</td></tr><tr><td>learning rate</td><td>0.0005</td></tr><tr><td>validation_fine_f1</td><td>0.63559</td></tr><tr><td>training_fine_f1</td><td>1.0</td></tr><tr><td>training_fine_acc</td><td>1.0</td></tr><tr><td>test_fine_f1</td><td>0.64268</td></tr><tr><td>test_fine_acc</td><td>0.64211</td></tr><tr><td>validation_loss</td><td>3.12076</td></tr><tr><td>_runtime</td><td>6187</td></tr><tr><td>_timestamp</td><td>1622180740</td></tr><tr><td>_step</td><td>869</td></tr><tr><td>loss</td><td>0.00215</td></tr><tr><td>batch</td><td>799</td></tr><tr><td>loss_fine</td><td>0.00215</td></tr><tr><td>lambda_fine</td><td>1.0</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>█████████████████████████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁▁▂▂▃▃▄▃▃▄▃▅▃▃▄▄▃▅▅▅▃▄▃▆▆▅▃▃▃▃▆▇▇███████</td></tr><tr><td>training_fine_f1</td><td>▁▁▂▂▂▂▄▃▃▃▃▅▃▃▅▅▄▆▇▆▄▆▄▇▇▆▄▅▄▄██████████</td></tr><tr><td>training_fine_acc</td><td>▁▁▂▂▂▂▄▃▃▄▃▅▃▄▅▅▄▆▇▆▄▆▄▇▇▆▄▄▄▄██████████</td></tr><tr><td>test_fine_f1</td><td>▁▁▂▂▃▃▅▄▃▄▃▅▃▃▅▄▃▅▆▅▃▄▃▆▆▅▃▄▃▃▆▇████████</td></tr><tr><td>test_fine_acc</td><td>▁▁▂▂▃▃▅▄▄▅▃▅▃▄▅▅▃▅▆▅▃▄▃▆▇▅▄▄▃▄▆▇████████</td></tr><tr><td>validation_loss</td><td>██▇▇▆▆▆▅▅▅▆▅▆▆▅▄▆▄▃▄▆▄▆▃▃▄▅▆▅▆▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▆▅▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▇▆▅▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">33ee681a7cebc7f84b51b7d1c74cbce191ffeb27ecec3e22de24c0e1</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1mllwkra\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1mllwkra</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  67%|██████▋   | 2/3 [4:52:53<2:43:35, 9815.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': False, 'epochs': 80, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-triplet-normal-notpretrained', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39acef7f09944481ac56070146de1a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_notpretrained_experiments', 'modelName': 'models/602c939fd16c07bde265fedff080d9947041107e19611e38b75dc59c', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '55e4c549d7c31faa68c85ddb89e94403838975f22df65c02916386e9', 'trialHash': '602c939fd16c07bde265fedff080d9947041107e19611e38b75dc59c', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 128, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': False, 'epochs': 80, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-triplet-normal-notpretrained', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.31 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">602c939fd16c07bde265fedff080d9947041107e19611e38b75dc59c</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2gws5aro\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2gws5aro</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210528_014609-2gws5aro</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/80 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/80 [00:49<?, ?it/s, min_val_loss=inf, train=0.00131, val=0.00135, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 1/80 [00:49<1:04:50, 49.24s/it, min_val_loss=inf, train=0.00131, val=0.00135, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 1/80 [03:11<1:04:50, 49.24s/it, min_val_loss=inf, train=0.00362, val=0.00502, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 2/80 [03:11<1:40:08, 77.04s/it, min_val_loss=inf, train=0.00362, val=0.00502, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 2/80 [05:39<1:40:08, 77.04s/it, min_val_loss=199, train=0.0205, val=0.0311, val_loss=3.63]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 3/80 [05:39<2:06:25, 98.51s/it, min_val_loss=199, train=0.0205, val=0.0311, val_loss=3.63]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 3/80 [08:02<2:06:25, 98.51s/it, min_val_loss=32.2, train=0.0438, val=0.0328, val_loss=3.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 4/80 [08:02<2:21:25, 111.65s/it, min_val_loss=32.2, train=0.0438, val=0.0328, val_loss=3.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 4/80 [10:33<2:21:25, 111.65s/it, min_val_loss=30.5, train=0.0738, val=0.0568, val_loss=3.6] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 5/80 [10:33<2:34:24, 123.52s/it, min_val_loss=30.5, train=0.0738, val=0.0568, val_loss=3.6]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 5/80 [12:57<2:34:24, 123.52s/it, min_val_loss=17.6, train=0.0922, val=0.0753, val_loss=3.59]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 6/80 [12:57<2:40:08, 129.85s/it, min_val_loss=17.6, train=0.0922, val=0.0753, val_loss=3.59]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 6/80 [15:25<2:40:08, 129.85s/it, min_val_loss=13.3, train=0.141, val=0.146, val_loss=3.58]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 7/80 [15:25<2:44:34, 135.26s/it, min_val_loss=13.3, train=0.141, val=0.146, val_loss=3.58]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 7/80 [17:48<2:44:34, 135.26s/it, min_val_loss=6.87, train=0.164, val=0.129, val_loss=3.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 8/80 [17:48<2:45:08, 137.61s/it, min_val_loss=6.87, train=0.164, val=0.129, val_loss=3.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 8/80 [20:07<2:45:08, 137.61s/it, min_val_loss=6.87, train=0.195, val=0.178, val_loss=3.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  11%|█▏        | 9/80 [20:07<2:43:20, 138.04s/it, min_val_loss=6.87, train=0.195, val=0.178, val_loss=3.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  11%|█▏        | 9/80 [22:31<2:43:20, 138.04s/it, min_val_loss=5.63, train=0.191, val=0.18, val_loss=3.53] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 10/80 [22:31<2:43:02, 139.75s/it, min_val_loss=5.63, train=0.191, val=0.18, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 10/80 [24:56<2:43:02, 139.75s/it, min_val_loss=5.56, train=0.189, val=0.132, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  14%|█▍        | 11/80 [24:56<2:42:32, 141.34s/it, min_val_loss=5.56, train=0.189, val=0.132, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  14%|█▍        | 11/80 [27:18<2:42:32, 141.34s/it, min_val_loss=5.56, train=0.28, val=0.221, val_loss=3.5]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 12/80 [27:18<2:40:29, 141.60s/it, min_val_loss=5.56, train=0.28, val=0.221, val_loss=3.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 12/80 [29:46<2:40:29, 141.60s/it, min_val_loss=4.53, train=0.214, val=0.18, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  16%|█▋        | 13/80 [29:46<2:40:08, 143.41s/it, min_val_loss=4.53, train=0.214, val=0.18, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  16%|█▋        | 13/80 [32:04<2:40:08, 143.41s/it, min_val_loss=4.53, train=0.203, val=0.154, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 14/80 [32:04<2:35:54, 141.73s/it, min_val_loss=4.53, train=0.203, val=0.154, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 14/80 [34:25<2:35:54, 141.73s/it, min_val_loss=4.53, train=0.297, val=0.239, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  19%|█▉        | 15/80 [34:25<2:33:14, 141.45s/it, min_val_loss=4.53, train=0.297, val=0.239, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  19%|█▉        | 15/80 [36:52<2:33:14, 141.45s/it, min_val_loss=4.18, train=0.346, val=0.272, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 16/80 [36:52<2:32:51, 143.30s/it, min_val_loss=4.18, train=0.346, val=0.272, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 16/80 [39:16<2:32:51, 143.30s/it, min_val_loss=3.68, train=0.259, val=0.205, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  21%|██▏       | 17/80 [39:16<2:30:29, 143.32s/it, min_val_loss=3.68, train=0.259, val=0.205, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  21%|██▏       | 17/80 [41:35<2:30:29, 143.32s/it, min_val_loss=3.68, train=0.359, val=0.318, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 18/80 [41:35<2:26:49, 142.09s/it, min_val_loss=3.68, train=0.359, val=0.318, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 18/80 [44:04<2:26:49, 142.09s/it, min_val_loss=3.14, train=0.381, val=0.279, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  24%|██▍       | 19/80 [44:04<2:26:33, 144.16s/it, min_val_loss=3.14, train=0.381, val=0.279, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  24%|██▍       | 19/80 [46:27<2:26:33, 144.16s/it, min_val_loss=3.14, train=0.157, val=0.13, val_loss=3.52] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 20/80 [46:27<2:23:51, 143.85s/it, min_val_loss=3.14, train=0.157, val=0.13, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 20/80 [48:44<2:23:51, 143.85s/it, min_val_loss=3.14, train=0.411, val=0.272, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  26%|██▋       | 21/80 [48:44<2:19:33, 141.92s/it, min_val_loss=3.14, train=0.411, val=0.272, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  26%|██▋       | 21/80 [51:09<2:19:33, 141.92s/it, min_val_loss=3.14, train=0.483, val=0.313, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 22/80 [51:09<2:17:58, 142.74s/it, min_val_loss=3.14, train=0.483, val=0.313, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 22/80 [53:30<2:17:58, 142.74s/it, min_val_loss=3.14, train=0.454, val=0.317, val_loss=3.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  29%|██▉       | 23/80 [53:30<2:15:07, 142.23s/it, min_val_loss=3.14, train=0.454, val=0.317, val_loss=3.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  29%|██▉       | 23/80 [55:49<2:15:07, 142.23s/it, min_val_loss=3.14, train=0.175, val=0.132, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 24/80 [55:49<2:11:53, 141.31s/it, min_val_loss=3.14, train=0.175, val=0.132, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 24/80 [58:04<2:11:53, 141.31s/it, min_val_loss=3.14, train=0.382, val=0.247, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  31%|███▏      | 25/80 [58:04<2:07:49, 139.44s/it, min_val_loss=3.14, train=0.382, val=0.247, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  31%|███▏      | 25/80 [1:00:21<2:07:49, 139.44s/it, min_val_loss=3.14, train=0.358, val=0.24, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 26/80 [1:00:21<2:04:37, 138.47s/it, min_val_loss=3.14, train=0.358, val=0.24, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 26/80 [1:02:40<2:04:37, 138.47s/it, min_val_loss=3.14, train=0.576, val=0.356, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  34%|███▍      | 27/80 [1:02:40<2:02:30, 138.69s/it, min_val_loss=3.14, train=0.576, val=0.356, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  34%|███▍      | 27/80 [1:05:01<2:02:30, 138.69s/it, min_val_loss=2.81, train=0.458, val=0.3, val_loss=3.4]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 28/80 [1:05:01<2:00:56, 139.56s/it, min_val_loss=2.81, train=0.458, val=0.3, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 28/80 [1:07:20<2:00:56, 139.56s/it, min_val_loss=2.81, train=0.293, val=0.195, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  36%|███▋      | 29/80 [1:07:20<1:58:22, 139.26s/it, min_val_loss=2.81, train=0.293, val=0.195, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  36%|███▋      | 29/80 [1:09:39<1:58:22, 139.26s/it, min_val_loss=2.81, train=0.156, val=0.109, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 30/80 [1:09:39<1:55:56, 139.13s/it, min_val_loss=2.81, train=0.156, val=0.109, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 30/80 [1:11:54<1:55:56, 139.13s/it, min_val_loss=2.81, train=0.276, val=0.175, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  39%|███▉      | 31/80 [1:11:54<1:52:45, 138.08s/it, min_val_loss=2.81, train=0.276, val=0.175, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  39%|███▉      | 31/80 [1:14:09<1:52:45, 138.08s/it, min_val_loss=2.81, train=0.253, val=0.162, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 32/80 [1:14:09<1:49:31, 136.92s/it, min_val_loss=2.81, train=0.253, val=0.162, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 32/80 [1:16:27<1:49:31, 136.92s/it, min_val_loss=2.81, train=0.372, val=0.22, val_loss=3.43] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  41%|████▏     | 33/80 [1:16:27<1:47:36, 137.37s/it, min_val_loss=2.81, train=0.372, val=0.22, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  41%|████▏     | 33/80 [1:18:45<1:47:36, 137.37s/it, min_val_loss=2.81, train=0.192, val=0.14, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 34/80 [1:18:45<1:45:25, 137.51s/it, min_val_loss=2.81, train=0.192, val=0.14, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 34/80 [1:20:57<1:45:25, 137.51s/it, min_val_loss=2.81, train=0.401, val=0.262, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  44%|████▍     | 35/80 [1:20:57<1:42:00, 136.01s/it, min_val_loss=2.81, train=0.401, val=0.262, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  44%|████▍     | 35/80 [1:23:15<1:42:00, 136.01s/it, min_val_loss=2.81, train=0.0959, val=0.0856, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 36/80 [1:23:15<1:40:03, 136.44s/it, min_val_loss=2.81, train=0.0959, val=0.0856, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 36/80 [1:25:35<1:40:03, 136.44s/it, min_val_loss=2.81, train=0.51, val=0.251, val_loss=3.36]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:  46%|████▋     | 37/80 [1:25:35<1:38:40, 137.68s/it, min_val_loss=2.81, train=0.51, val=0.251, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  46%|████▋     | 37/80 [1:27:54<1:38:40, 137.68s/it, min_val_loss=2.81, train=0.294, val=0.169, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 38/80 [1:27:54<1:36:35, 137.99s/it, min_val_loss=2.81, train=0.294, val=0.169, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 38/80 [1:30:16<1:36:35, 137.99s/it, min_val_loss=2.81, train=0.893, val=0.46, val_loss=3.25] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  49%|████▉     | 39/80 [1:30:16<1:35:03, 139.12s/it, min_val_loss=2.81, train=0.893, val=0.46, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  49%|████▉     | 39/80 [1:32:42<1:35:03, 139.12s/it, min_val_loss=2.18, train=0.918, val=0.471, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 40/80 [1:32:42<1:34:07, 141.19s/it, min_val_loss=2.18, train=0.918, val=0.471, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 40/80 [1:35:04<1:34:07, 141.19s/it, min_val_loss=2.12, train=0.812, val=0.392, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  51%|█████▏    | 41/80 [1:35:04<1:31:54, 141.40s/it, min_val_loss=2.12, train=0.812, val=0.392, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  51%|█████▏    | 41/80 [1:37:23<1:31:54, 141.40s/it, min_val_loss=2.12, train=0.961, val=0.489, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 42/80 [1:37:23<1:29:05, 140.68s/it, min_val_loss=2.12, train=0.961, val=0.489, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 42/80 [1:39:48<1:29:05, 140.68s/it, min_val_loss=2.05, train=0.974, val=0.536, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  54%|█████▍    | 43/80 [1:39:48<1:27:37, 142.10s/it, min_val_loss=2.05, train=0.974, val=0.536, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  54%|█████▍    | 43/80 [1:42:08<1:27:37, 142.10s/it, min_val_loss=1.87, train=0.975, val=0.502, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 44/80 [1:42:08<1:24:56, 141.58s/it, min_val_loss=1.87, train=0.975, val=0.502, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 44/80 [1:44:22<1:24:56, 141.58s/it, min_val_loss=1.87, train=0.989, val=0.537, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  56%|█████▋    | 45/80 [1:44:22<1:21:13, 139.24s/it, min_val_loss=1.87, train=0.989, val=0.537, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  56%|█████▋    | 45/80 [1:46:48<1:21:13, 139.24s/it, min_val_loss=1.86, train=0.989, val=0.562, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 46/80 [1:46:48<1:19:56, 141.09s/it, min_val_loss=1.86, train=0.989, val=0.562, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 46/80 [1:49:06<1:19:56, 141.09s/it, min_val_loss=1.78, train=0.988, val=0.553, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  59%|█████▉    | 47/80 [1:49:06<1:17:08, 140.26s/it, min_val_loss=1.78, train=0.988, val=0.553, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  59%|█████▉    | 47/80 [1:51:22<1:17:08, 140.26s/it, min_val_loss=1.78, train=0.993, val=0.579, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 48/80 [1:51:22<1:14:10, 139.07s/it, min_val_loss=1.78, train=0.993, val=0.579, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 48/80 [1:53:46<1:14:10, 139.07s/it, min_val_loss=1.73, train=0.983, val=0.538, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  61%|██████▏   | 49/80 [1:53:46<1:12:37, 140.56s/it, min_val_loss=1.73, train=0.983, val=0.538, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  61%|██████▏   | 49/80 [1:56:06<1:12:37, 140.56s/it, min_val_loss=1.73, train=0.958, val=0.477, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 50/80 [1:56:06<1:10:12, 140.42s/it, min_val_loss=1.73, train=0.958, val=0.477, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 50/80 [1:58:21<1:10:12, 140.42s/it, min_val_loss=1.73, train=0.99, val=0.558, val_loss=3.17] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  64%|██████▍   | 51/80 [1:58:21<1:07:05, 138.82s/it, min_val_loss=1.73, train=0.99, val=0.558, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  64%|██████▍   | 51/80 [2:00:38<1:07:05, 138.82s/it, min_val_loss=1.73, train=0.998, val=0.567, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 52/80 [2:00:38<1:04:23, 137.97s/it, min_val_loss=1.73, train=0.998, val=0.567, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 52/80 [2:02:54<1:04:23, 137.97s/it, min_val_loss=1.73, train=0.984, val=0.503, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  66%|██████▋   | 53/80 [2:02:54<1:01:49, 137.40s/it, min_val_loss=1.73, train=0.984, val=0.503, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  66%|██████▋   | 53/80 [2:05:13<1:01:49, 137.40s/it, min_val_loss=1.73, train=0.897, val=0.441, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 54/80 [2:05:13<59:52, 138.16s/it, min_val_loss=1.73, train=0.897, val=0.441, val_loss=3.24]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 54/80 [2:07:29<59:52, 138.16s/it, min_val_loss=1.73, train=0.862, val=0.413, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  69%|██████▉   | 55/80 [2:07:29<57:12, 137.28s/it, min_val_loss=1.73, train=0.862, val=0.413, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  69%|██████▉   | 55/80 [2:09:42<57:12, 137.28s/it, min_val_loss=1.73, train=0.944, val=0.474, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 56/80 [2:09:42<54:27, 136.16s/it, min_val_loss=1.73, train=0.944, val=0.474, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 56/80 [2:12:01<54:27, 136.16s/it, min_val_loss=1.73, train=0.96, val=0.478, val_loss=3.21] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  71%|███████▏  | 57/80 [2:12:01<52:26, 136.80s/it, min_val_loss=1.73, train=0.96, val=0.478, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  71%|███████▏  | 57/80 [2:14:14<52:26, 136.80s/it, min_val_loss=1.73, train=0.861, val=0.429, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 58/80 [2:14:14<49:50, 135.92s/it, min_val_loss=1.73, train=0.861, val=0.429, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 58/80 [2:16:33<49:50, 135.92s/it, min_val_loss=1.73, train=0.872, val=0.35, val_loss=3.29] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  74%|███████▍  | 59/80 [2:16:33<47:49, 136.66s/it, min_val_loss=1.73, train=0.872, val=0.35, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  74%|███████▍  | 59/80 [2:18:47<47:49, 136.66s/it, min_val_loss=1.73, train=0.998, val=0.533, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 60/80 [2:18:47<45:19, 135.97s/it, min_val_loss=1.73, train=0.998, val=0.533, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 60/80 [2:21:04<45:19, 135.97s/it, min_val_loss=1.73, train=0.999, val=0.536, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  76%|███████▋  | 61/80 [2:21:04<43:07, 136.19s/it, min_val_loss=1.73, train=0.999, val=0.536, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  76%|███████▋  | 61/80 [2:23:21<43:07, 136.19s/it, min_val_loss=1.73, train=1, val=0.622, val_loss=3.14]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 62/80 [2:23:21<40:56, 136.48s/it, min_val_loss=1.73, train=1, val=0.622, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 62/80 [2:25:41<40:56, 136.48s/it, min_val_loss=1.61, train=1, val=0.608, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  79%|███████▉  | 63/80 [2:25:41<38:56, 137.47s/it, min_val_loss=1.61, train=1, val=0.608, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  79%|███████▉  | 63/80 [2:27:54<38:56, 137.47s/it, min_val_loss=1.61, train=1, val=0.618, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 64/80 [2:27:54<36:21, 136.32s/it, min_val_loss=1.61, train=1, val=0.618, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 64/80 [2:30:12<36:21, 136.32s/it, min_val_loss=1.61, train=1, val=0.61, val_loss=3.12] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  81%|████████▏ | 65/80 [2:30:12<34:10, 136.67s/it, min_val_loss=1.61, train=1, val=0.61, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  81%|████████▏ | 65/80 [2:32:31<34:10, 136.67s/it, min_val_loss=1.61, train=1, val=0.613, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 66/80 [2:32:32<32:05, 137.54s/it, min_val_loss=1.61, train=1, val=0.613, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 66/80 [2:34:46<32:05, 137.54s/it, min_val_loss=1.61, train=0.999, val=0.613, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  84%|████████▍ | 67/80 [2:34:46<29:36, 136.67s/it, min_val_loss=1.61, train=0.999, val=0.613, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  84%|████████▍ | 67/80 [2:37:06<29:36, 136.67s/it, min_val_loss=1.61, train=1, val=0.601, val_loss=3.12]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 68/80 [2:37:06<27:31, 137.60s/it, min_val_loss=1.61, train=1, val=0.601, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 68/80 [2:39:20<27:31, 137.60s/it, min_val_loss=1.61, train=0.999, val=0.63, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  86%|████████▋ | 69/80 [2:39:20<25:01, 136.50s/it, min_val_loss=1.61, train=0.999, val=0.63, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  86%|████████▋ | 69/80 [2:41:44<25:01, 136.50s/it, min_val_loss=1.59, train=0.998, val=0.601, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 70/80 [2:41:44<23:07, 138.76s/it, min_val_loss=1.59, train=0.998, val=0.601, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 70/80 [2:44:05<23:07, 138.76s/it, min_val_loss=1.59, train=0.999, val=0.622, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  89%|████████▉ | 71/80 [2:44:05<20:54, 139.39s/it, min_val_loss=1.59, train=0.999, val=0.622, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  89%|████████▉ | 71/80 [2:46:28<20:54, 139.39s/it, min_val_loss=1.59, train=0.999, val=0.556, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 72/80 [2:46:28<18:43, 140.48s/it, min_val_loss=1.59, train=0.999, val=0.556, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 72/80 [2:48:51<18:43, 140.48s/it, min_val_loss=1.59, train=0.999, val=0.561, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  91%|█████████▏| 73/80 [2:48:51<16:29, 141.31s/it, min_val_loss=1.59, train=0.999, val=0.561, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  91%|█████████▏| 73/80 [2:51:12<16:29, 141.31s/it, min_val_loss=1.59, train=0.998, val=0.644, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 74/80 [2:51:12<14:07, 141.30s/it, min_val_loss=1.59, train=0.998, val=0.644, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 74/80 [2:53:41<14:07, 141.30s/it, min_val_loss=1.55, train=1, val=0.625, val_loss=3.11]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  94%|█████████▍| 75/80 [2:53:41<11:57, 143.56s/it, min_val_loss=1.55, train=1, val=0.625, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  94%|█████████▍| 75/80 [2:56:00<11:57, 143.56s/it, min_val_loss=1.55, train=1, val=0.632, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 76/80 [2:56:00<09:28, 142.16s/it, min_val_loss=1.55, train=1, val=0.632, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 76/80 [2:58:20<09:28, 142.16s/it, min_val_loss=1.55, train=1, val=0.632, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  96%|█████████▋| 77/80 [2:58:20<07:04, 141.56s/it, min_val_loss=1.55, train=1, val=0.632, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  96%|█████████▋| 77/80 [3:00:42<07:04, 141.56s/it, min_val_loss=1.55, train=1, val=0.631, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 78/80 [3:00:42<04:43, 141.71s/it, min_val_loss=1.55, train=1, val=0.631, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 78/80 [3:03:04<04:43, 141.71s/it, min_val_loss=1.55, train=1, val=0.615, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  99%|█████████▉| 79/80 [3:03:04<02:21, 141.78s/it, min_val_loss=1.55, train=1, val=0.615, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  99%|█████████▉| 79/80 [3:05:27<02:21, 141.78s/it, min_val_loss=1.55, train=1, val=0.607, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration: 100%|██████████| 80/80 [3:05:59<00:00, 139.49s/it, min_val_loss=1.55, train=1, val=0.607, val_loss=3.11]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 64722<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b45a72a847840f5830c4b921df42716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210528_014609-2gws5aro/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210528_014609-2gws5aro/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>79</td></tr><tr><td>learning rate</td><td>0.00025</td></tr><tr><td>validation_fine_f1</td><td>0.64362</td></tr><tr><td>training_fine_f1</td><td>1.0</td></tr><tr><td>training_fine_acc</td><td>1.0</td></tr><tr><td>test_fine_f1</td><td>0.64354</td></tr><tr><td>test_fine_acc</td><td>0.63947</td></tr><tr><td>validation_loss</td><td>3.11215</td></tr><tr><td>_runtime</td><td>11131</td></tr><tr><td>_timestamp</td><td>1622191900</td></tr><tr><td>_step</td><td>869</td></tr><tr><td>loss</td><td>0.01949</td></tr><tr><td>batch</td><td>799</td></tr><tr><td>loss_fine</td><td>0.00479</td></tr><tr><td>lambda_fine</td><td>0.98105</td></tr><tr><td>loss_layer2</td><td>0.73772</td></tr><tr><td>lambda_layer2</td><td>0.01232</td></tr><tr><td>nonzerotriplets_layer2</td><td>38</td></tr><tr><td>loss_layer4</td><td>0.86182</td></tr><tr><td>lambda_layer4</td><td>0.00662</td></tr><tr><td>nonzerotriplets_layer4</td><td>38</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>███████████████████▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁▁▂▃▃▂▃▄▃▄▄▅▄▅▃▃▃▄▄▆▅▇▇▇▇▇▇▆▆▅▇█████▇███</td></tr><tr><td>training_fine_f1</td><td>▁▁▂▂▂▂▂▃▃▄▄▄▄▅▃▃▄▄▅▇▇██████▇█▇██████████</td></tr><tr><td>training_fine_acc</td><td>▁▁▂▂▃▃▃▃▃▄▄▄▄▅▃▃▄▄▅▇▇██████▇█▇██████████</td></tr><tr><td>test_fine_f1</td><td>▁▁▁▂▃▃▃▄▃▄▄▅▄▅▃▃▃▄▄▆▅▇▇▇▇▇▇▅▆▅▇█▇███▇███</td></tr><tr><td>test_fine_acc</td><td>▁▁▂▃▃▃▄▄▄▅▄▅▄▅▄▄▄▄▅▆▆▇▇▇▇▇▇▆▇▅▇█▇███▇███</td></tr><tr><td>validation_loss</td><td>██▇▇▇▇▆▆▆▅▅▅▅▄▆▆▅▅▄▃▃▂▂▂▂▂▂▃▂▃▂▁▁▁▁▁▂▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>██▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>██▇▆▆▅▅▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▂▁▂▃▃▅▇▅▇▇███████████▇█████████</td></tr><tr><td>loss_layer2</td><td>█▆▆▄▄▄▄▃▄▄▄▄▅▄▃▄▃▃▄▄▃▄▂▃▄▄▃▄▂▃▃▃▃▃▁▃▃▂▃▃</td></tr><tr><td>lambda_layer2</td><td>▃█▂█▄▅▄▅▇▄▂▅▂▂▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer4</td><td>▆█▄▆▄▅▄▄▅▅▃▄▄▃▄▅▃▂▃▃▂▃▂▃▂▂▂▂▂▃▁▃▂▂▂▃▃▂▂▃</td></tr><tr><td>lambda_layer4</td><td>█▃█▃▆▅▆▅▃▅█▄▅▆▂▂▄▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nonzerotriplets_layer4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">602c939fd16c07bde265fedff080d9947041107e19611e38b75dc59c</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2gws5aro\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2gws5aro</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment: 100%|██████████| 3/3 [7:59:00<00:00, 9580.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    print('wandb not found')\n",
    "\n",
    "from myhelpers import config_plots, TrialStatistics\n",
    "from myhelpers.try_warning import try_running\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "config_plots.global_settings()\n",
    "\n",
    "experimetnsFileName = \"experiments.csv\"\n",
    "WANDB_message=\"wandb not working\"\n",
    "\n",
    "\n",
    "# For logging to server\n",
    "try_running(lambda : wandb.login(), WANDB_message)\n",
    "\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "# set cuda\n",
    "if device is not None:\n",
    "    print(\"using cuda\", device)\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# get experiment params\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "\n",
    "# init experiments file\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "\n",
    "paramsIterator = config_parser.getExperiments()  \n",
    "number_of_experiments = sum(1 for e in paramsIterator)  \n",
    "experiment_index = 0\n",
    "\n",
    "# Loop through experiments\n",
    "# with progressbar.ProgressBar(max_value=number_of_experiments) as bar:\n",
    "with tqdm(total=number_of_experiments, desc=\"experiment\") as bar:\n",
    "    for experiment_params in config_parser.getExperiments():\n",
    "        print(experiment_params)\n",
    "        experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "        # load images \n",
    "        if experiment_params['image_path'] == 'cifar-100-python':\n",
    "            datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        else:\n",
    "            datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "        train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "        architecture = {\n",
    "            \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "            \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "        }\n",
    "\n",
    "        # Loop through n trials\n",
    "        for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "            modelName = getModelName(experiment_params, i)\n",
    "            trialName = os.path.join(experimentPathAndName, modelName)\n",
    "            trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "            row_information = {\n",
    "                'experimentName': experimentName,\n",
    "                'modelName': modelName,\n",
    "                'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "                'experimentHash': experimentHash,\n",
    "                'trialHash': trialHash\n",
    "            }\n",
    "            row_information = {**row_information, **experiment_params} \n",
    "            print(row_information)\n",
    "\n",
    "            run = try_running(lambda : wandb.init(project='HGNN', group=experimentName+\"-\"+experimentHash, name=trialHash, config=row_information), WANDB_message) #, reinit=True\n",
    "\n",
    "            # Train/Load model\n",
    "            model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "            \n",
    "#             from torchsummary import summary\n",
    "\n",
    "#             summary(model, (3, 224, 224))\n",
    "\n",
    "#             try_running(lambda : wandb.watch(model, log=\"all\"), WANDB_message)\n",
    "\n",
    "            if os.path.exists(CNN.getModelFile(trialName)):\n",
    "                print(\"Model {0} found!\".format(trialName))\n",
    "            else:\n",
    "                initModelPath = CNN.getInitModelFile(experimentPathAndName)\n",
    "                if os.path.exists(initModelPath):\n",
    "                    model.load_state_dict(torch.load(initModelPath))\n",
    "                    print(\"Init Model {0} found!\".format(initModelPath))\n",
    "                CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=detailed_reporting)\n",
    "\n",
    "            # Add to experiments file\n",
    "            if os.path.exists(experimentsFileNameAndPath):\n",
    "                experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "            else:\n",
    "                experiments_df = pd.DataFrame()\n",
    "\n",
    "            record_exists = not (experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName]).empty if not experiments_df.empty else False\n",
    "            if record_exists:\n",
    "                experiments_df.drop(experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName].index, inplace = True) \n",
    "\n",
    "            experiments_df = experiments_df.append(pd.DataFrame(row_information, index=[0]), ignore_index = True)\n",
    "            experiments_df.to_csv(experimentsFileNameAndPath, header=True, index=False)\n",
    "\n",
    "            try_running(lambda : run.finish(), WANDB_message)\n",
    "\n",
    "        bar.update()\n",
    "\n",
    "        experiment_index = experiment_index + 1\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "#     import argparse\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--cuda', required=True, type=int)\n",
    "#     parser.add_argument('--experiments', required=True)\n",
    "#     parser.add_argument('--data', required=True)\n",
    "#     parser.add_argument('--name', required=True)\n",
    "#     parser.add_argument('--detailed', required=False, action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "#     main(experimentName=args.name, experimentsPath=args.experiments, dataPath=args.data, device=args.cuda, detailed_reporting=args.detailed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
