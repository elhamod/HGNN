{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath = \"/raid/elhamod/Fish/experiments/\" #\"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/Fish/\" # \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = \"Fish_phylogeny_tripletloss_new_archi_full\" #\"Fish_tripletloss_alpha_lr_experiments\"\n",
    "device = 0\n",
    "detailed_reporting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "experiment:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 0\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish alpha 0.1', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1411fe1394014523a9ed7481c33cbd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_phylogeny_tripletloss_new_archi_full', 'modelName': 'models/78fcadef7f9d40a4d9d12789ca7ca4c756b8660495e53b25b334cad4', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '64166d8fa41454b367214676437e28256afe8cd18d661d637880cca7', 'trialHash': '78fcadef7f9d40a4d9d12789ca7ca4c756b8660495e53b25b334cad4', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish alpha 0.1', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">78fcadef7f9d40a4d9d12789ca7ca4c756b8660495e53b25b334cad4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/3s5ynrq8\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/3s5ynrq8</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_010507-3s5ynrq8</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:38<?, ?it/s, min_val_loss=inf, train=0.00565, val=0.00652, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [00:38<5:17:06, 38.13s/it, min_val_loss=inf, train=0.00565, val=0.00652, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [03:30<5:17:06, 38.13s/it, min_val_loss=inf, train=0.168, val=0.109, val_loss=3.6]     \u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:30<10:49:36, 78.27s/it, min_val_loss=inf, train=0.168, val=0.109, val_loss=3.6]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [06:23<10:49:36, 78.27s/it, min_val_loss=9.16, train=0.307, val=0.279, val_loss=3.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [06:23<14:44:35, 106.79s/it, min_val_loss=9.16, train=0.307, val=0.279, val_loss=3.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [09:23<14:44:35, 106.79s/it, min_val_loss=3.58, train=0.344, val=0.268, val_loss=3.54]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [09:23<17:43:40, 128.67s/it, min_val_loss=3.58, train=0.344, val=0.268, val_loss=3.54]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [12:16<17:43:40, 128.67s/it, min_val_loss=3.58, train=0.349, val=0.331, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [12:16<19:31:11, 141.96s/it, min_val_loss=3.58, train=0.349, val=0.331, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [15:05<19:31:11, 141.96s/it, min_val_loss=3.02, train=0.346, val=0.324, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [15:05<20:36:57, 150.24s/it, min_val_loss=3.02, train=0.346, val=0.324, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [17:58<20:36:57, 150.24s/it, min_val_loss=3.02, train=0.394, val=0.309, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [17:58<21:29:05, 156.89s/it, min_val_loss=3.02, train=0.394, val=0.309, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [20:54<21:29:05, 156.89s/it, min_val_loss=3.02, train=0.406, val=0.356, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [20:54<22:14:55, 162.80s/it, min_val_loss=3.02, train=0.406, val=0.356, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [23:53<22:14:55, 162.80s/it, min_val_loss=2.81, train=0.435, val=0.358, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [23:53<22:50:49, 167.51s/it, min_val_loss=2.81, train=0.435, val=0.358, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [26:45<22:50:49, 167.51s/it, min_val_loss=2.79, train=0.467, val=0.376, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [26:45<23:00:06, 168.99s/it, min_val_loss=2.79, train=0.467, val=0.376, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [29:44<23:00:06, 168.99s/it, min_val_loss=2.66, train=0.544, val=0.468, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [29:44<23:20:47, 171.88s/it, min_val_loss=2.66, train=0.544, val=0.468, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [32:39<23:20:47, 171.88s/it, min_val_loss=2.14, train=0.526, val=0.477, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [32:39<23:27:25, 173.04s/it, min_val_loss=2.14, train=0.526, val=0.477, val_loss=3.43]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [35:36<23:27:25, 173.04s/it, min_val_loss=2.1, train=0.491, val=0.354, val_loss=3.5]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [35:36<23:31:52, 173.95s/it, min_val_loss=2.1, train=0.491, val=0.354, val_loss=3.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [38:27<23:31:52, 173.95s/it, min_val_loss=2.1, train=0.628, val=0.492, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [38:27<23:21:55, 173.08s/it, min_val_loss=2.1, train=0.628, val=0.492, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [41:23<23:21:55, 173.08s/it, min_val_loss=2.03, train=0.684, val=0.518, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [41:23<23:27:30, 174.12s/it, min_val_loss=2.03, train=0.684, val=0.518, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [44:14<23:27:30, 174.12s/it, min_val_loss=1.93, train=0.627, val=0.491, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [44:14<23:17:10, 173.20s/it, min_val_loss=1.93, train=0.627, val=0.491, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [47:08<23:17:10, 173.20s/it, min_val_loss=1.93, train=0.636, val=0.564, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [47:08<23:14:59, 173.29s/it, min_val_loss=1.93, train=0.636, val=0.564, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [50:02<23:14:59, 173.29s/it, min_val_loss=1.77, train=0.654, val=0.509, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [50:02<23:15:28, 173.71s/it, min_val_loss=1.77, train=0.654, val=0.509, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [52:53<23:15:28, 173.71s/it, min_val_loss=1.77, train=0.664, val=0.504, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [52:53<23:06:09, 172.91s/it, min_val_loss=1.77, train=0.664, val=0.504, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [55:48<23:06:09, 172.91s/it, min_val_loss=1.77, train=0.757, val=0.591, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [55:48<23:08:27, 173.56s/it, min_val_loss=1.77, train=0.757, val=0.591, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [58:44<23:08:27, 173.56s/it, min_val_loss=1.69, train=0.705, val=0.563, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [58:44<23:09:07, 174.00s/it, min_val_loss=1.69, train=0.705, val=0.563, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [1:01:33<23:09:07, 174.00s/it, min_val_loss=1.69, train=0.732, val=0.551, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:01:33<22:54:15, 172.50s/it, min_val_loss=1.69, train=0.732, val=0.551, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:04:28<22:54:15, 172.50s/it, min_val_loss=1.69, train=0.813, val=0.589, val_loss=3.3] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:04:28<22:57:28, 173.27s/it, min_val_loss=1.69, train=0.813, val=0.589, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:07:20<22:57:28, 173.27s/it, min_val_loss=1.69, train=0.786, val=0.595, val_loss=3.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [1:07:20<22:52:44, 173.04s/it, min_val_loss=1.69, train=0.786, val=0.595, val_loss=3.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [1:10:17<22:52:44, 173.04s/it, min_val_loss=1.68, train=0.845, val=0.614, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [1:10:17<22:59:13, 174.22s/it, min_val_loss=1.68, train=0.845, val=0.614, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [1:13:14<22:59:13, 174.22s/it, min_val_loss=1.63, train=0.8, val=0.605, val_loss=3.3]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [1:13:14<23:02:17, 174.97s/it, min_val_loss=1.63, train=0.8, val=0.605, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [1:16:00<23:02:17, 174.97s/it, min_val_loss=1.63, train=0.767, val=0.612, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [1:16:00<22:39:08, 172.41s/it, min_val_loss=1.63, train=0.767, val=0.612, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [1:18:51<22:39:08, 172.41s/it, min_val_loss=1.63, train=0.834, val=0.644, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [1:18:51<22:31:50, 171.84s/it, min_val_loss=1.63, train=0.834, val=0.644, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [1:21:47<22:31:50, 171.84s/it, min_val_loss=1.55, train=0.838, val=0.663, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [1:21:47<22:38:42, 173.08s/it, min_val_loss=1.55, train=0.838, val=0.663, val_loss=3.29]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [1:24:42<22:38:42, 173.08s/it, min_val_loss=1.51, train=0.872, val=0.71, val_loss=3.25] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [1:24:42<22:42:04, 173.88s/it, min_val_loss=1.51, train=0.872, val=0.71, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [1:27:37<22:42:04, 173.88s/it, min_val_loss=1.41, train=0.859, val=0.68, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [1:27:37<22:39:53, 173.97s/it, min_val_loss=1.41, train=0.859, val=0.68, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [1:30:30<22:39:53, 173.97s/it, min_val_loss=1.41, train=0.885, val=0.698, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [1:30:30<22:34:30, 173.66s/it, min_val_loss=1.41, train=0.885, val=0.698, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [1:33:19<22:34:30, 173.66s/it, min_val_loss=1.41, train=0.887, val=0.639, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [1:33:19<22:22:18, 172.46s/it, min_val_loss=1.41, train=0.887, val=0.639, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [1:36:13<22:22:18, 172.46s/it, min_val_loss=1.41, train=0.746, val=0.563, val_loss=3.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [1:36:13<22:23:34, 172.99s/it, min_val_loss=1.41, train=0.746, val=0.563, val_loss=3.32]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [1:39:05<22:23:34, 172.99s/it, min_val_loss=1.41, train=0.878, val=0.65, val_loss=3.24] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 35/500 [1:39:05<22:17:34, 172.59s/it, min_val_loss=1.41, train=0.878, val=0.65, val_loss=3.24]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   7%|▋         | 35/500 [1:39:16<21:58:49, 170.17s/it, min_val_loss=1.41, train=0.878, val=0.65, val_loss=3.24]\n",
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/ipykernel_launcher.py:109: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 56462<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_010507-3s5ynrq8/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_010507-3s5ynrq8/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>34</td></tr><tr><td>learning rate</td><td>0.005</td></tr><tr><td>validation_fine_f1</td><td>0.70977</td></tr><tr><td>training_fine_f1</td><td>0.878</td></tr><tr><td>test_fine_f1</td><td>0.69415</td></tr><tr><td>validation_loss</td><td>3.23857</td></tr><tr><td>_runtime</td><td>5952</td></tr><tr><td>_timestamp</td><td>1620715459</td></tr><tr><td>_step</td><td>680</td></tr><tr><td>loss</td><td>0.12029</td></tr><tr><td>batch</td><td>664</td></tr><tr><td>loss_fine</td><td>0.9351</td></tr><tr><td>lambda_fine</td><td>0.10017</td></tr><tr><td>loss_layer2</td><td>0.0</td></tr><tr><td>lambda_layer2</td><td>0.75395</td></tr><tr><td>nonzerotriplets_layer2</td><td>1</td></tr><tr><td>loss_layer4</td><td>0.1825</td></tr><tr><td>lambda_layer4</td><td>0.14588</td></tr><tr><td>nonzerotriplets_layer4</td><td>5</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>learning rate</td><td>█████████████████████████████████▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁▂▄▄▄▄▄▄▅▅▆▆▄▆▆▆▇▆▆▇▇▆▇▇▇▇▇▇████▇▇▇</td></tr><tr><td>training_fine_f1</td><td>▁▂▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█▇▇██████▇█</td></tr><tr><td>test_fine_f1</td><td>▁▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▆▇▇▇█▇▇██████▇█</td></tr><tr><td>validation_loss</td><td>█▇▇▆▆▆▆▆▆▆▅▅▆▄▄▄▄▃▄▃▃▃▂▃▂▂▂▂▂▂▁▁▁▃▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▆▅▆▃▅▅▅▆▅▃▅▅▄▂▄▂▅▄▄▂▄▂▂▄▁▁▄▃▄▄▃▁▁▃▁▃▁▄</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▆▅▅▅▄▄▄▄▅▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▁█▁▂</td></tr><tr><td>loss_layer2</td><td>▃▃▃▃▃▁▃▃▃▃▃▁█▃▃▁▃▁▃▃▃▁▃▁▁▃▁▁▃▃▃▃▃▁▁▃▁▃▁▄</td></tr><tr><td>lambda_layer2</td><td>▅▅▅▅▅█▅▅▅▅▅█▁▅▅█▅█▅▅▅█▅██▅██▅▅▅▅▅██▅█▅█▃</td></tr><tr><td>nonzerotriplets_layer2</td><td>█▃▃▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer4</td><td>▅▅▄▃▄▃▄▃▄▃▃▅▄▂▃▃▅▆█▄▁▄▄▃▆▂▅▂▃▂▃▂▂▄▆▅▃▅▄▅</td></tr><tr><td>lambda_layer4</td><td>▄▄▄▄▄▁▄▄▄▄▄▁█▄▄▁▄▁▄▄▄▁▄▁▁▄▁▁▄▄▄▄▄▁▁▄▁▄▁▆</td></tr><tr><td>nonzerotriplets_layer4</td><td>█▅▇▆▆▅▅▇▃▃▆▄▂▆▅▂▅▅▂▁▂▅▇▅▂▅▂▄▄▅▃▇▂▆▃▂▃▅▃▃</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">78fcadef7f9d40a4d9d12789ca7ca4c756b8660495e53b25b334cad4</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/3s5ynrq8\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/3s5ynrq8</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "experiment:  14%|█▍        | 1/7 [1:39:26<9:56:41, 5966.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish alpha 0.5', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51b465dd38b408a9a42a04d99bda72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_phylogeny_tripletloss_new_archi_full', 'modelName': 'models/b29caa2f775f847892d4a5c6df3ec49adb696ab87fdcaa3aa1453374', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '54ea69374bf971d458bd8dcb95cabdbf97037ddde3583f6d5fafa29c', 'trialHash': 'b29caa2f775f847892d4a5c6df3ec49adb696ab87fdcaa3aa1453374', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish alpha 0.5', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">b29caa2f775f847892d4a5c6df3ec49adb696ab87fdcaa3aa1453374</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/3fncy1lf\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/3fncy1lf</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_024433-3fncy1lf</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:37<?, ?it/s, min_val_loss=inf, train=0.00206, val=0.00155, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [00:37<5:14:29, 37.82s/it, min_val_loss=inf, train=0.00206, val=0.00155, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [03:33<5:14:29, 37.82s/it, min_val_loss=inf, train=0.474, val=0.38, val_loss=3.48]     \u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:33<10:57:14, 79.19s/it, min_val_loss=inf, train=0.474, val=0.38, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [06:31<10:57:14, 79.19s/it, min_val_loss=2.63, train=0.61, val=0.497, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [06:31<15:01:38, 108.85s/it, min_val_loss=2.63, train=0.61, val=0.497, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [09:25<15:01:38, 108.85s/it, min_val_loss=2.01, train=0.769, val=0.601, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [09:25<17:42:05, 128.48s/it, min_val_loss=2.01, train=0.769, val=0.601, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [12:20<17:42:05, 128.48s/it, min_val_loss=1.67, train=0.885, val=0.637, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [12:20<19:33:47, 142.28s/it, min_val_loss=1.67, train=0.885, val=0.637, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [15:13<19:33:47, 142.28s/it, min_val_loss=1.57, train=0.922, val=0.677, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [15:13<20:48:49, 151.68s/it, min_val_loss=1.57, train=0.922, val=0.677, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [18:11<20:48:49, 151.68s/it, min_val_loss=1.48, train=0.949, val=0.7, val_loss=3.23]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [18:11<21:50:35, 159.51s/it, min_val_loss=1.48, train=0.949, val=0.7, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [21:07<21:50:35, 159.51s/it, min_val_loss=1.43, train=0.954, val=0.706, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [21:07<22:27:28, 164.33s/it, min_val_loss=1.43, train=0.954, val=0.706, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [24:07<22:27:28, 164.33s/it, min_val_loss=1.42, train=0.94, val=0.66, val_loss=3.23]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [24:07<23:02:40, 168.96s/it, min_val_loss=1.42, train=0.94, val=0.66, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [26:59<23:02:40, 168.96s/it, min_val_loss=1.42, train=0.863, val=0.587, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [26:59<23:09:19, 170.12s/it, min_val_loss=1.42, train=0.863, val=0.587, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [29:55<23:09:19, 170.12s/it, min_val_loss=1.42, train=0.96, val=0.681, val_loss=3.16] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [29:55<23:18:45, 171.63s/it, min_val_loss=1.42, train=0.96, val=0.681, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [32:50<23:18:45, 171.63s/it, min_val_loss=1.42, train=0.981, val=0.728, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [32:50<23:25:47, 172.84s/it, min_val_loss=1.42, train=0.981, val=0.728, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [35:43<23:25:47, 172.84s/it, min_val_loss=1.37, train=0.981, val=0.717, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [35:43<23:22:51, 172.84s/it, min_val_loss=1.37, train=0.981, val=0.717, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [38:36<23:22:51, 172.84s/it, min_val_loss=1.37, train=0.987, val=0.728, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [38:36<23:20:15, 172.87s/it, min_val_loss=1.37, train=0.987, val=0.728, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [41:27<23:20:15, 172.87s/it, min_val_loss=1.37, train=0.996, val=0.764, val_loss=3.1] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [41:27<23:11:45, 172.18s/it, min_val_loss=1.37, train=0.996, val=0.764, val_loss=3.1]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [44:24<23:11:45, 172.18s/it, min_val_loss=1.31, train=0.959, val=0.642, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [44:24<23:20:26, 173.61s/it, min_val_loss=1.31, train=0.959, val=0.642, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [47:13<23:20:26, 173.61s/it, min_val_loss=1.31, train=0.983, val=0.696, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [47:13<23:06:35, 172.25s/it, min_val_loss=1.31, train=0.983, val=0.696, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [50:04<23:06:35, 172.25s/it, min_val_loss=1.31, train=0.995, val=0.762, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [50:04<23:02:16, 172.07s/it, min_val_loss=1.31, train=0.995, val=0.762, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [52:57<23:02:16, 172.07s/it, min_val_loss=1.31, train=0.998, val=0.767, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [52:57<23:02:07, 172.41s/it, min_val_loss=1.31, train=0.998, val=0.767, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [55:54<23:02:07, 172.41s/it, min_val_loss=1.3, train=0.985, val=0.7, val_loss=3.14]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [55:54<23:10:06, 173.76s/it, min_val_loss=1.3, train=0.985, val=0.7, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [58:50<23:10:06, 173.76s/it, min_val_loss=1.3, train=0.998, val=0.773, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [58:50<23:11:50, 174.34s/it, min_val_loss=1.3, train=0.998, val=0.773, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [1:01:44<23:11:50, 174.34s/it, min_val_loss=1.29, train=0.998, val=0.772, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:01:44<23:08:28, 174.28s/it, min_val_loss=1.29, train=0.998, val=0.772, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:04:40<23:08:28, 174.28s/it, min_val_loss=1.29, train=0.998, val=0.719, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:04:40<23:10:00, 174.84s/it, min_val_loss=1.29, train=0.998, val=0.719, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:07:36<23:10:00, 174.84s/it, min_val_loss=1.29, train=0.996, val=0.776, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [1:07:36<23:07:54, 174.95s/it, min_val_loss=1.29, train=0.996, val=0.776, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [1:10:34<23:07:54, 174.95s/it, min_val_loss=1.29, train=0.999, val=0.772, val_loss=3.04]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [1:10:34<23:12:16, 175.87s/it, min_val_loss=1.29, train=0.999, val=0.772, val_loss=3.04]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [1:13:28<23:12:16, 175.87s/it, min_val_loss=1.29, train=0.999, val=0.771, val_loss=3.04]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [1:13:28<23:05:06, 175.33s/it, min_val_loss=1.29, train=0.999, val=0.771, val_loss=3.04]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [1:16:20<23:05:06, 175.33s/it, min_val_loss=1.29, train=0.997, val=0.755, val_loss=3.05]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [1:16:20<22:56:07, 174.56s/it, min_val_loss=1.29, train=0.997, val=0.755, val_loss=3.05]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [1:19:08<22:56:07, 174.56s/it, min_val_loss=1.29, train=0.973, val=0.731, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [1:19:08<22:37:47, 172.60s/it, min_val_loss=1.29, train=0.973, val=0.731, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [1:22:01<22:37:47, 172.60s/it, min_val_loss=1.29, train=0.998, val=0.8, val_loss=3.01]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [1:22:01<22:35:17, 172.65s/it, min_val_loss=1.29, train=0.998, val=0.8, val_loss=3.01]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [1:24:55<22:35:17, 172.65s/it, min_val_loss=1.25, train=1, val=0.817, val_loss=3]     \u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [1:24:55<22:35:27, 173.04s/it, min_val_loss=1.25, train=1, val=0.817, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [1:27:51<22:35:27, 173.04s/it, min_val_loss=1.22, train=1, val=0.803, val_loss=3.01]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [1:27:51<22:39:18, 173.90s/it, min_val_loss=1.22, train=1, val=0.803, val_loss=3.01]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [1:30:45<22:39:18, 173.90s/it, min_val_loss=1.22, train=1, val=0.803, val_loss=3.02]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [1:30:45<22:36:33, 173.92s/it, min_val_loss=1.22, train=1, val=0.803, val_loss=3.02]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [1:33:40<22:36:33, 173.92s/it, min_val_loss=1.22, train=0.999, val=0.796, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [1:33:40<22:35:19, 174.13s/it, min_val_loss=1.22, train=0.999, val=0.796, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [1:36:33<22:35:19, 174.13s/it, min_val_loss=1.22, train=0.999, val=0.795, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [1:36:33<22:30:19, 173.86s/it, min_val_loss=1.22, train=0.999, val=0.795, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [1:39:27<22:30:19, 173.86s/it, min_val_loss=1.22, train=1, val=0.819, val_loss=2.99] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 35/500 [1:39:27<22:29:12, 174.09s/it, min_val_loss=1.22, train=1, val=0.819, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 35/500 [1:42:22<22:29:12, 174.09s/it, min_val_loss=1.22, train=1, val=0.821, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 36/500 [1:42:22<22:26:24, 174.10s/it, min_val_loss=1.22, train=1, val=0.821, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 36/500 [1:45:18<22:26:24, 174.10s/it, min_val_loss=1.22, train=1, val=0.805, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 37/500 [1:45:18<22:27:50, 174.67s/it, min_val_loss=1.22, train=1, val=0.805, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 37/500 [1:48:06<22:27:50, 174.67s/it, min_val_loss=1.22, train=1, val=0.816, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 38/500 [1:48:06<22:10:23, 172.78s/it, min_val_loss=1.22, train=1, val=0.816, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 38/500 [1:51:00<22:10:23, 172.78s/it, min_val_loss=1.22, train=1, val=0.803, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 39/500 [1:51:00<22:09:51, 173.08s/it, min_val_loss=1.22, train=1, val=0.803, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 39/500 [1:53:52<22:09:51, 173.08s/it, min_val_loss=1.22, train=1, val=0.834, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 40/500 [1:53:52<22:04:36, 172.78s/it, min_val_loss=1.22, train=1, val=0.834, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 40/500 [1:56:49<22:04:36, 172.78s/it, min_val_loss=1.2, train=1, val=0.824, val_loss=2.99] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 41/500 [1:56:49<22:10:50, 173.97s/it, min_val_loss=1.2, train=1, val=0.824, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 41/500 [1:59:43<22:10:50, 173.97s/it, min_val_loss=1.2, train=1, val=0.825, val_loss=2.98]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 42/500 [1:59:43<22:08:22, 174.02s/it, min_val_loss=1.2, train=1, val=0.825, val_loss=2.98]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 42/500 [2:02:38<22:08:22, 174.02s/it, min_val_loss=1.2, train=1, val=0.827, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▊         | 43/500 [2:02:38<22:09:15, 174.52s/it, min_val_loss=1.2, train=1, val=0.827, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▊         | 43/500 [2:05:34<22:09:15, 174.52s/it, min_val_loss=1.2, train=1, val=0.819, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 44/500 [2:05:34<22:09:26, 174.93s/it, min_val_loss=1.2, train=1, val=0.819, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 44/500 [2:08:28<22:09:26, 174.93s/it, min_val_loss=1.2, train=1, val=0.816, val_loss=2.99]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   9%|▉         | 45/500 [2:08:28<22:03:20, 174.51s/it, min_val_loss=1.2, train=1, val=0.816, val_loss=2.99]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   9%|▉         | 45/500 [2:08:38<21:40:42, 171.52s/it, min_val_loss=1.2, train=1, val=0.816, val_loss=2.99]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 71948<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_024433-3fncy1lf/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_024433-3fncy1lf/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>44</td></tr><tr><td>learning rate</td><td>0.00125</td></tr><tr><td>validation_fine_f1</td><td>0.8336</td></tr><tr><td>training_fine_f1</td><td>1.0</td></tr><tr><td>test_fine_f1</td><td>0.84794</td></tr><tr><td>validation_loss</td><td>2.99454</td></tr><tr><td>_runtime</td><td>7711</td></tr><tr><td>_timestamp</td><td>1620723184</td></tr><tr><td>_step</td><td>880</td></tr><tr><td>loss</td><td>0.10625</td></tr><tr><td>batch</td><td>854</td></tr><tr><td>loss_fine</td><td>0.13786</td></tr><tr><td>lambda_fine</td><td>0.63156</td></tr><tr><td>loss_layer2</td><td>0.0</td></tr><tr><td>lambda_layer2</td><td>0.26211</td></tr><tr><td>nonzerotriplets_layer2</td><td>1</td></tr><tr><td>loss_layer4</td><td>0.18043</td></tr><tr><td>lambda_layer4</td><td>0.10633</td></tr><tr><td>nonzerotriplets_layer4</td><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>learning rate</td><td>████████████████████████▄▄▄▄▄▄▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>validation_fine_f1</td><td>▁▄▅▆▆▇▇▇▆▇▇▇▇▇▆▇▇▇▇▇▇█▇▇▇███████████████</td></tr><tr><td>training_fine_f1</td><td>▁▄▅▆▇▇██▇███████████████████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▄▅▆▆▇▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇███████████████</td></tr><tr><td>validation_loss</td><td>█▆▆▅▅▄▄▃▄▃▃▃▂▂▄▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▂▂▂▃▄▃▃▃▃▃▃▆▄▆▅▅▆▄▆▄▆▆▆█▇▅▇▅▆▇▆▆▅▅▅</td></tr><tr><td>loss_layer2</td><td>▇█████▇█▇█▁▁▁▁▁▁█▇▁█▁▁▁▁▁▁▁▁█▁▁▁▁▁█▁▁▁▁▁</td></tr><tr><td>lambda_layer2</td><td>▅▅▅▅▅▅▅▅▅▄██████▂▄▆▃▆▆▇▆▇▅▅▅▁▅▃▅▆▅▁▅▅▃▃▃</td></tr><tr><td>nonzerotriplets_layer2</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer4</td><td>█▇████▇███▇█▇▇▇▇██▇█▇▇█▇▆█▇▇██▁▇▇▇██▇▁▁▁</td></tr><tr><td>lambda_layer4</td><td>████▇▇█▇▇▆▂▂▂▃▂▂▅▅▁▅▂▁▂▁▃▁▂▂▃▁▅▁▂▁▄▁▂▅▅▅</td></tr><tr><td>nonzerotriplets_layer4</td><td>▄█▇▅▃▄▅▂▃▅▄▅▃▁▄▆▂▂▃▂▃▃▂▃▁▃▄▁▂▁▁▃▂▁▂▃▂▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">b29caa2f775f847892d4a5c6df3ec49adb696ab87fdcaa3aa1453374</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/3fncy1lf\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/3fncy1lf</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "experiment:  29%|██▊       | 2/7 [3:48:12<9:01:11, 6494.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.8, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish alpha 0.8', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3d3a0403e946a6a17b71b8c55186e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_phylogeny_tripletloss_new_archi_full', 'modelName': 'models/edebf26c8066d83a2cce0cff6d478d3afdbe584376ba86b66fb03461', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': 'b00875994a7e534d15ba11565b57a9121fb514f365c8a5b30583f1d1', 'trialHash': 'edebf26c8066d83a2cce0cff6d478d3afdbe584376ba86b66fb03461', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.8, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish alpha 0.8', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">edebf26c8066d83a2cce0cff6d478d3afdbe584376ba86b66fb03461</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1f0zgn53\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1f0zgn53</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_045318-1f0zgn53</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:40<?, ?it/s, min_val_loss=inf, train=0.00458, val=0.00614, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [00:40<5:33:45, 40.13s/it, min_val_loss=inf, train=0.00458, val=0.00614, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [03:32<5:33:45, 40.13s/it, min_val_loss=inf, train=0.519, val=0.39, val_loss=3.46]     \u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:32<11:03:22, 79.92s/it, min_val_loss=inf, train=0.519, val=0.39, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [06:28<11:03:22, 79.92s/it, min_val_loss=2.56, train=0.787, val=0.623, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [06:28<14:59:21, 108.57s/it, min_val_loss=2.56, train=0.787, val=0.623, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [09:25<14:59:21, 108.57s/it, min_val_loss=1.6, train=0.854, val=0.594, val_loss=3.33] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [09:25<17:47:43, 129.16s/it, min_val_loss=1.6, train=0.854, val=0.594, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [12:19<17:47:43, 129.16s/it, min_val_loss=1.6, train=0.959, val=0.713, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [12:19<19:36:08, 142.56s/it, min_val_loss=1.6, train=0.959, val=0.713, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [15:13<19:36:08, 142.56s/it, min_val_loss=1.4, train=0.996, val=0.749, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [15:13<20:51:33, 152.01s/it, min_val_loss=1.4, train=0.996, val=0.749, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [18:10<20:51:33, 152.01s/it, min_val_loss=1.33, train=0.985, val=0.743, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [18:10<21:50:26, 159.48s/it, min_val_loss=1.33, train=0.985, val=0.743, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [21:02<21:50:26, 159.48s/it, min_val_loss=1.33, train=0.997, val=0.776, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [21:02<22:20:01, 163.42s/it, min_val_loss=1.33, train=0.997, val=0.776, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [24:00<22:20:01, 163.42s/it, min_val_loss=1.29, train=0.999, val=0.775, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [24:00<22:52:00, 167.66s/it, min_val_loss=1.29, train=0.999, val=0.775, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [26:55<22:52:00, 167.66s/it, min_val_loss=1.29, train=0.998, val=0.762, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [26:55<23:06:50, 169.82s/it, min_val_loss=1.29, train=0.998, val=0.762, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [29:44<23:06:50, 169.82s/it, min_val_loss=1.29, train=0.999, val=0.765, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [29:44<23:03:16, 169.73s/it, min_val_loss=1.29, train=0.999, val=0.765, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [32:34<23:03:16, 169.73s/it, min_val_loss=1.29, train=0.999, val=0.779, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [32:34<23:00:03, 169.68s/it, min_val_loss=1.29, train=0.999, val=0.779, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [35:27<23:00:03, 169.68s/it, min_val_loss=1.28, train=0.99, val=0.751, val_loss=3.11] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [35:27<23:06:21, 170.80s/it, min_val_loss=1.28, train=0.99, val=0.751, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [38:21<23:06:21, 170.80s/it, min_val_loss=1.28, train=0.997, val=0.785, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [38:21<23:10:28, 171.66s/it, min_val_loss=1.28, train=0.997, val=0.785, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [41:18<23:10:28, 171.66s/it, min_val_loss=1.27, train=1, val=0.767, val_loss=3.09]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [41:18<23:20:40, 173.28s/it, min_val_loss=1.27, train=1, val=0.767, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [44:10<23:20:40, 173.28s/it, min_val_loss=1.27, train=0.998, val=0.788, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [44:10<23:14:22, 172.86s/it, min_val_loss=1.27, train=0.998, val=0.788, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [47:06<23:14:22, 172.86s/it, min_val_loss=1.27, train=0.998, val=0.76, val_loss=3.09] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [47:06<23:18:09, 173.68s/it, min_val_loss=1.27, train=0.998, val=0.76, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [49:59<23:18:09, 173.68s/it, min_val_loss=1.27, train=1, val=0.797, val_loss=3.05]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [49:59<23:15:46, 173.75s/it, min_val_loss=1.27, train=1, val=0.797, val_loss=3.05]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [53:01<23:15:46, 173.75s/it, min_val_loss=1.25, train=0.998, val=0.762, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [53:01<23:30:38, 175.96s/it, min_val_loss=1.25, train=0.998, val=0.762, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [55:52<23:30:38, 175.96s/it, min_val_loss=1.25, train=1, val=0.777, val_loss=3.06]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [55:52<23:17:31, 174.69s/it, min_val_loss=1.25, train=1, val=0.777, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [58:45<23:17:31, 174.69s/it, min_val_loss=1.25, train=0.999, val=0.759, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [58:45<23:09:25, 174.04s/it, min_val_loss=1.25, train=0.999, val=0.759, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [1:01:36<23:09:25, 174.04s/it, min_val_loss=1.25, train=0.998, val=0.787, val_loss=3.05]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:01:36<22:59:02, 173.10s/it, min_val_loss=1.25, train=0.998, val=0.787, val_loss=3.05]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:04:26<22:59:02, 173.10s/it, min_val_loss=1.25, train=1, val=0.79, val_loss=3.04]     \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:04:26<22:49:47, 172.30s/it, min_val_loss=1.25, train=1, val=0.79, val_loss=3.04]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   5%|▍         | 23/500 [1:04:36<22:19:46, 168.52s/it, min_val_loss=1.25, train=1, val=0.79, val_loss=3.04]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20383<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_045318-1f0zgn53/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_045318-1f0zgn53/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>22</td></tr><tr><td>learning rate</td><td>0.005</td></tr><tr><td>validation_fine_f1</td><td>0.79696</td></tr><tr><td>training_fine_f1</td><td>1.0</td></tr><tr><td>test_fine_f1</td><td>0.82957</td></tr><tr><td>validation_loss</td><td>3.03674</td></tr><tr><td>_runtime</td><td>3869</td></tr><tr><td>_timestamp</td><td>1620727068</td></tr><tr><td>_step</td><td>440</td></tr><tr><td>loss</td><td>0.13149</td></tr><tr><td>batch</td><td>436</td></tr><tr><td>loss_fine</td><td>0.14085</td></tr><tr><td>lambda_fine</td><td>0.86147</td></tr><tr><td>loss_layer2</td><td>0.0</td></tr><tr><td>lambda_layer2</td><td>0.08147</td></tr><tr><td>nonzerotriplets_layer2</td><td>1</td></tr><tr><td>loss_layer4</td><td>0.17798</td></tr><tr><td>lambda_layer4</td><td>0.05707</td></tr><tr><td>nonzerotriplets_layer4</td><td>3</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>learning rate</td><td>█████████████████████▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁▄▆▆▇██████████████████</td></tr><tr><td>training_fine_f1</td><td>▁▅▇▇███████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▄▆▆▇██████████████████</td></tr><tr><td>validation_loss</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▅▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▆▅▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▂▂▄▄▅▆▆▄▅▇▆█▇▆▆▇▇▇▆█▇█▇█▇▇█▆▆▇▆▇▇▅█▇</td></tr><tr><td>loss_layer2</td><td>▇▇█▇█▁█▇█▇█▁▁▇▁██▇▁▇██▁▇▁█▁███▇▁▁▁▁▁▇▁█▁</td></tr><tr><td>lambda_layer2</td><td>▇▇▆▆▆█▄▅▄▃▃▆▆▃▅▂▂▁▅▃▂▂▅▂▄▂▄▂▂▂▂▃▅▄▃▄▁▆▂▅</td></tr><tr><td>nonzerotriplets_layer2</td><td>██▂▃▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer4</td><td>███▇█▇██▇▇▇▇██▇▇█▁▇█▇▇▇▇███▇▇▇▇▁██▁▇▁▇▇█</td></tr><tr><td>lambda_layer4</td><td>████▇▅▆▆▅▄▄▃▂▃▂▃▃▆▂▃▃▃▁▂▁▃▁▃▃▃▂▄▁▁▄▁▅▂▃▁</td></tr><tr><td>nonzerotriplets_layer4</td><td>▇██▅▅▅▆▅▇▇▅▅▅▅▅▅▃▁▃▁▂▅▄▆▄▂▃▂▂▂▅▁▂▂▁▁▁▂▃▂</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">edebf26c8066d83a2cce0cff6d478d3afdbe584376ba86b66fb03461</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1f0zgn53\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1f0zgn53</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "experiment:  43%|████▎     | 3/7 [4:52:55<6:20:43, 5710.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.1, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish lr 0.1', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72df735c7674d15aae6afc96c4a4d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_phylogeny_tripletloss_new_archi_full', 'modelName': 'models/7711c57fd32fdd7281a6c89c09e47bff36095c4aff330e615fd8d727', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '858a992afd440386cc28dca45a6b03ce60cd28476bae2bd0e9f1e607', 'trialHash': '7711c57fd32fdd7281a6c89c09e47bff36095c4aff330e615fd8d727', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.1, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish lr 0.1', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">7711c57fd32fdd7281a6c89c09e47bff36095c4aff330e615fd8d727</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/mt4hprsw\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/mt4hprsw</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_055802-mt4hprsw</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:38<?, ?it/s, min_val_loss=inf, train=0.00522, val=0.00527, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [00:38<5:23:09, 38.86s/it, min_val_loss=inf, train=0.00522, val=0.00527, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [03:33<5:23:09, 38.86s/it, min_val_loss=inf, train=0.0493, val=0.042, val_loss=3.62]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:33<11:00:19, 79.56s/it, min_val_loss=inf, train=0.0493, val=0.042, val_loss=3.62]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [06:28<11:00:19, 79.56s/it, min_val_loss=23.8, train=0.181, val=0.159, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [06:28<14:56:06, 108.18s/it, min_val_loss=23.8, train=0.181, val=0.159, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [09:25<14:56:06, 108.18s/it, min_val_loss=6.3, train=0.216, val=0.139, val_loss=3.53] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [09:25<17:44:44, 128.80s/it, min_val_loss=6.3, train=0.216, val=0.139, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [12:17<17:44:44, 128.80s/it, min_val_loss=6.3, train=0.192, val=0.177, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [12:17<19:31:01, 141.94s/it, min_val_loss=6.3, train=0.192, val=0.177, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [15:15<19:31:01, 141.94s/it, min_val_loss=5.64, train=0.154, val=0.146, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [15:15<20:57:19, 152.71s/it, min_val_loss=5.64, train=0.154, val=0.146, val_loss=3.52]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [18:07<20:57:19, 152.71s/it, min_val_loss=5.64, train=0.229, val=0.199, val_loss=3.5] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [18:07<21:42:11, 158.48s/it, min_val_loss=5.64, train=0.229, val=0.199, val_loss=3.5]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [20:59<21:42:11, 158.48s/it, min_val_loss=5.04, train=0.335, val=0.29, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [20:59<22:11:43, 162.41s/it, min_val_loss=5.04, train=0.335, val=0.29, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [23:55<22:11:43, 162.41s/it, min_val_loss=3.45, train=0.384, val=0.297, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [23:55<22:43:23, 166.61s/it, min_val_loss=3.45, train=0.384, val=0.297, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [26:49<22:43:23, 166.61s/it, min_val_loss=3.36, train=0.219, val=0.184, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [26:49<22:57:34, 168.68s/it, min_val_loss=3.36, train=0.219, val=0.184, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [29:44<22:57:34, 168.68s/it, min_val_loss=3.36, train=0.388, val=0.308, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [29:44<23:10:31, 170.62s/it, min_val_loss=3.36, train=0.388, val=0.308, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [32:39<23:10:31, 170.62s/it, min_val_loss=3.24, train=0.486, val=0.362, val_loss=3.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [32:39<23:17:56, 171.88s/it, min_val_loss=3.24, train=0.486, val=0.362, val_loss=3.39]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [35:36<23:17:56, 171.88s/it, min_val_loss=2.76, train=0.524, val=0.444, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [35:36<23:29:40, 173.68s/it, min_val_loss=2.76, train=0.524, val=0.444, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [38:32<23:29:40, 173.68s/it, min_val_loss=2.25, train=0.389, val=0.29, val_loss=3.44] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [38:32<23:31:27, 174.25s/it, min_val_loss=2.25, train=0.389, val=0.29, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [41:25<23:31:27, 174.25s/it, min_val_loss=2.25, train=0.502, val=0.359, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [41:25<23:26:18, 173.98s/it, min_val_loss=2.25, train=0.502, val=0.359, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [44:22<23:26:18, 173.98s/it, min_val_loss=2.25, train=0.551, val=0.448, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [44:22<23:28:53, 174.66s/it, min_val_loss=2.25, train=0.551, val=0.448, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [47:16<23:28:53, 174.66s/it, min_val_loss=2.23, train=0.559, val=0.441, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [47:16<23:25:29, 174.59s/it, min_val_loss=2.23, train=0.559, val=0.441, val_loss=3.31]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [50:08<23:25:29, 174.59s/it, min_val_loss=2.23, train=0.506, val=0.391, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [50:08<23:16:46, 173.87s/it, min_val_loss=2.23, train=0.506, val=0.391, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [53:00<23:16:46, 173.87s/it, min_val_loss=2.23, train=0.556, val=0.413, val_loss=3.3] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [53:00<23:08:12, 173.17s/it, min_val_loss=2.23, train=0.556, val=0.413, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [55:51<23:08:12, 173.17s/it, min_val_loss=2.23, train=0.498, val=0.382, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [55:51<23:01:40, 172.71s/it, min_val_loss=2.23, train=0.498, val=0.382, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [58:45<23:01:40, 172.71s/it, min_val_loss=2.23, train=0.739, val=0.557, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [58:45<23:00:42, 172.95s/it, min_val_loss=2.23, train=0.739, val=0.557, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [1:01:39<23:00:42, 172.95s/it, min_val_loss=1.8, train=0.792, val=0.571, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:01:39<23:01:26, 173.40s/it, min_val_loss=1.8, train=0.792, val=0.571, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:04:34<23:01:26, 173.40s/it, min_val_loss=1.75, train=0.777, val=0.575, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:04:34<23:00:14, 173.61s/it, min_val_loss=1.75, train=0.777, val=0.575, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:07:32<23:00:14, 173.61s/it, min_val_loss=1.74, train=0.832, val=0.566, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [1:07:32<23:09:53, 175.20s/it, min_val_loss=1.74, train=0.832, val=0.566, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [1:10:24<23:09:53, 175.20s/it, min_val_loss=1.74, train=0.804, val=0.578, val_loss=3.2] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [1:10:24<22:59:12, 174.22s/it, min_val_loss=1.74, train=0.804, val=0.578, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [1:13:15<22:59:12, 174.22s/it, min_val_loss=1.73, train=0.853, val=0.621, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [1:13:15<22:48:29, 173.23s/it, min_val_loss=1.73, train=0.853, val=0.621, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [1:16:11<22:48:29, 173.23s/it, min_val_loss=1.61, train=0.849, val=0.624, val_loss=3.2] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [1:16:11<22:51:40, 174.00s/it, min_val_loss=1.61, train=0.849, val=0.624, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [1:19:02<22:51:40, 174.00s/it, min_val_loss=1.6, train=0.857, val=0.576, val_loss=3.2] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [1:19:02<22:42:06, 173.15s/it, min_val_loss=1.6, train=0.857, val=0.576, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [1:21:51<22:42:06, 173.15s/it, min_val_loss=1.6, train=0.909, val=0.637, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [1:21:51<22:28:51, 171.83s/it, min_val_loss=1.6, train=0.909, val=0.637, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [1:24:49<22:28:51, 171.83s/it, min_val_loss=1.57, train=0.862, val=0.592, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [1:24:49<22:39:46, 173.59s/it, min_val_loss=1.57, train=0.862, val=0.592, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [1:27:43<22:39:46, 173.59s/it, min_val_loss=1.57, train=0.906, val=0.607, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [1:27:43<22:39:04, 173.87s/it, min_val_loss=1.57, train=0.906, val=0.607, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [1:30:36<22:39:04, 173.87s/it, min_val_loss=1.57, train=0.927, val=0.622, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [1:30:36<22:32:48, 173.44s/it, min_val_loss=1.57, train=0.927, val=0.622, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [1:33:27<22:32:48, 173.44s/it, min_val_loss=1.57, train=0.882, val=0.564, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [1:33:27<22:24:14, 172.71s/it, min_val_loss=1.57, train=0.882, val=0.564, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [1:36:21<22:24:14, 172.71s/it, min_val_loss=1.57, train=0.925, val=0.612, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [1:36:21<22:25:29, 173.24s/it, min_val_loss=1.57, train=0.925, val=0.612, val_loss=3.16]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   7%|▋         | 34/500 [1:36:32<22:03:05, 170.36s/it, min_val_loss=1.57, train=0.925, val=0.612, val_loss=3.16]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30380<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_055802-mt4hprsw/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_055802-mt4hprsw/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>33</td></tr><tr><td>learning rate</td><td>0.025</td></tr><tr><td>validation_fine_f1</td><td>0.63737</td></tr><tr><td>training_fine_f1</td><td>0.92464</td></tr><tr><td>test_fine_f1</td><td>0.61514</td></tr><tr><td>validation_loss</td><td>3.16062</td></tr><tr><td>_runtime</td><td>5785</td></tr><tr><td>_timestamp</td><td>1620732867</td></tr><tr><td>_step</td><td>660</td></tr><tr><td>loss</td><td>0.08106</td></tr><tr><td>batch</td><td>645</td></tr><tr><td>loss_fine</td><td>0.50598</td></tr><tr><td>lambda_fine</td><td>0.10785</td></tr><tr><td>loss_layer2</td><td>0.0</td></tr><tr><td>lambda_layer2</td><td>0.74551</td></tr><tr><td>nonzerotriplets_layer2</td><td>1</td></tr><tr><td>loss_layer4</td><td>0.18068</td></tr><tr><td>lambda_layer4</td><td>0.14664</td></tr><tr><td>nonzerotriplets_layer4</td><td>3</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>███████████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁▁▃▂▃▃▃▄▄▃▄▅▆▄▅▆▆▅▆▅▇▇▇▇▇██▇█▇██▇█</td></tr><tr><td>training_fine_f1</td><td>▁▁▂▃▂▂▃▄▄▃▄▅▅▄▅▅▅▅▅▅▇▇▇▇▇▇▇▇██████</td></tr><tr><td>test_fine_f1</td><td>▁▁▃▃▃▂▃▄▅▃▅▅▆▄▅▆▆▅▆▅▇▇▇█▇████▇████</td></tr><tr><td>validation_loss</td><td>██▇▆▆▆▆▅▅▆▅▄▄▅▅▄▃▄▃▄▂▂▂▂▂▁▂▂▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>██▇▇▆▆▇▇▆▆▆▅▅▅▅▃▅▂▅▄▂▅▅▅▂▄▂▂▂▄▁▁▄▄▄▄▁▄▄▄</td></tr><tr><td>batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▇▆▆▅▅▆▆▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▁▁▂▁▁▂▁▂▂▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▂▁▃▃▃▂▂▂█</td></tr><tr><td>loss_layer2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_layer2</td><td>▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅█▅▅█▅▅▅█▁███▅██▅▅▅▅█▅▅▅</td></tr><tr><td>nonzerotriplets_layer2</td><td>█▇▃▂▃▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer4</td><td>▃▄▆▅▂▅▆▅▃▄▄▂▁▃▅▄▆▅▅▄▇▂▇▄▁▇▅▅▄▄▃█▃▇▃▅▅▅▅▅</td></tr><tr><td>lambda_layer4</td><td>▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▁▄▁▄▄▁▄▄▄▁█▁▁▁▄▁▁▄▄▄▄▁▄▄▄</td></tr><tr><td>nonzerotriplets_layer4</td><td>▅▇█▆▅▆█▃▆▇▄▆▃▆▆▃▅▆▂▆▄▆▅▆▅▂▅▃▄▅▃▁▂▃▅▄▆▄▃▃</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">7711c57fd32fdd7281a6c89c09e47bff36095c4aff330e615fd8d727</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/mt4hprsw\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/mt4hprsw</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "experiment:  57%|█████▋    | 4/7 [6:29:35<4:46:53, 5737.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish lr 0.01', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2f035889be42f2a5dc563cf730fa00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_phylogeny_tripletloss_new_archi_full', 'modelName': 'models/a68727a46b2913194bdaa7ed977c51aad2130bd058da8a824def7b96', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '9487ac2535ecbeb43364c31fce816b28080d239b3180b1fa7b2ffe59', 'trialHash': 'a68727a46b2913194bdaa7ed977c51aad2130bd058da8a824def7b96', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.01, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish lr 0.01', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">a68727a46b2913194bdaa7ed977c51aad2130bd058da8a824def7b96</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2sfcgvy4\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2sfcgvy4</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_073444-2sfcgvy4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:38<?, ?it/s, min_val_loss=inf, train=0.00252, val=0.00397, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [00:38<5:22:34, 38.79s/it, min_val_loss=inf, train=0.00252, val=0.00397, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [03:31<5:22:34, 38.79s/it, min_val_loss=inf, train=0.0883, val=0.085, val_loss=3.61]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:31<10:54:40, 78.88s/it, min_val_loss=inf, train=0.0883, val=0.085, val_loss=3.61]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [06:25<10:54:40, 78.88s/it, min_val_loss=11.8, train=0.227, val=0.197, val_loss=3.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [06:25<14:49:56, 107.44s/it, min_val_loss=11.8, train=0.227, val=0.197, val_loss=3.57]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [09:19<14:49:56, 107.44s/it, min_val_loss=5.08, train=0.386, val=0.348, val_loss=3.54]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [09:19<17:33:08, 127.40s/it, min_val_loss=5.08, train=0.386, val=0.348, val_loss=3.54]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [12:15<17:33:08, 127.40s/it, min_val_loss=2.87, train=0.199, val=0.164, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [12:15<19:32:58, 142.18s/it, min_val_loss=2.87, train=0.199, val=0.164, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [15:03<19:32:58, 142.18s/it, min_val_loss=2.87, train=0.346, val=0.26, val_loss=3.54] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [15:03<20:33:11, 149.78s/it, min_val_loss=2.87, train=0.346, val=0.26, val_loss=3.54]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [17:57<20:33:11, 149.78s/it, min_val_loss=2.87, train=0.513, val=0.428, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [17:57<21:30:09, 157.02s/it, min_val_loss=2.87, train=0.513, val=0.428, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [20:52<21:30:09, 157.02s/it, min_val_loss=2.34, train=0.506, val=0.39, val_loss=3.46] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [20:52<22:10:56, 162.31s/it, min_val_loss=2.34, train=0.506, val=0.39, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [23:47<22:10:56, 162.31s/it, min_val_loss=2.34, train=0.527, val=0.459, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [23:47<22:40:11, 166.21s/it, min_val_loss=2.34, train=0.527, val=0.459, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [26:42<22:40:11, 166.21s/it, min_val_loss=2.18, train=0.455, val=0.357, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [26:42<23:00:33, 169.05s/it, min_val_loss=2.18, train=0.455, val=0.357, val_loss=3.47]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [29:37<23:00:33, 169.05s/it, min_val_loss=2.18, train=0.508, val=0.447, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [29:37<23:11:55, 170.79s/it, min_val_loss=2.18, train=0.508, val=0.447, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [32:27<23:11:55, 170.79s/it, min_val_loss=2.18, train=0.514, val=0.395, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [32:27<23:06:13, 170.44s/it, min_val_loss=2.18, train=0.514, val=0.395, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [35:19<23:06:13, 170.44s/it, min_val_loss=2.18, train=0.595, val=0.57, val_loss=3.41] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [35:19<23:07:43, 170.97s/it, min_val_loss=2.18, train=0.595, val=0.57, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [38:14<23:07:43, 170.97s/it, min_val_loss=1.76, train=0.664, val=0.547, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [38:14<23:14:42, 172.19s/it, min_val_loss=1.76, train=0.664, val=0.547, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [41:09<23:14:42, 172.19s/it, min_val_loss=1.76, train=0.698, val=0.555, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [41:09<23:18:24, 173.00s/it, min_val_loss=1.76, train=0.698, val=0.555, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [44:00<23:18:24, 173.00s/it, min_val_loss=1.76, train=0.707, val=0.638, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [44:00<23:10:57, 172.43s/it, min_val_loss=1.76, train=0.707, val=0.638, val_loss=3.38]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [46:54<23:10:57, 172.43s/it, min_val_loss=1.57, train=0.686, val=0.568, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [46:54<23:10:17, 172.71s/it, min_val_loss=1.57, train=0.686, val=0.568, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [49:47<23:10:17, 172.71s/it, min_val_loss=1.57, train=0.692, val=0.551, val_loss=3.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [49:47<23:09:53, 173.02s/it, min_val_loss=1.57, train=0.692, val=0.551, val_loss=3.34]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [52:38<23:09:53, 173.02s/it, min_val_loss=1.57, train=0.717, val=0.6, val_loss=3.35]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [52:38<23:01:42, 172.35s/it, min_val_loss=1.57, train=0.717, val=0.6, val_loss=3.35]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 19/500 [55:30<23:01:42, 172.35s/it, min_val_loss=1.57, train=0.802, val=0.642, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [55:30<22:58:04, 172.26s/it, min_val_loss=1.57, train=0.802, val=0.642, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 20/500 [58:26<22:58:04, 172.26s/it, min_val_loss=1.56, train=0.654, val=0.533, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [58:26<23:04:15, 173.39s/it, min_val_loss=1.56, train=0.654, val=0.533, val_loss=3.37]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 21/500 [1:01:16<23:04:15, 173.39s/it, min_val_loss=1.56, train=0.661, val=0.509, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:01:16<22:53:48, 172.44s/it, min_val_loss=1.56, train=0.661, val=0.509, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▍         | 22/500 [1:04:10<22:53:48, 172.44s/it, min_val_loss=1.56, train=0.634, val=0.485, val_loss=3.4] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:04:10<22:53:12, 172.73s/it, min_val_loss=1.56, train=0.634, val=0.485, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 23/500 [1:07:04<22:53:12, 172.73s/it, min_val_loss=1.56, train=0.747, val=0.572, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [1:07:04<22:53:21, 173.11s/it, min_val_loss=1.56, train=0.747, val=0.572, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▍         | 24/500 [1:10:00<22:53:21, 173.11s/it, min_val_loss=1.56, train=0.843, val=0.666, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [1:10:00<22:58:07, 174.08s/it, min_val_loss=1.56, train=0.843, val=0.666, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 25/500 [1:12:56<22:58:07, 174.08s/it, min_val_loss=1.5, train=0.908, val=0.712, val_loss=3.24] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [1:12:56<23:00:07, 174.70s/it, min_val_loss=1.5, train=0.908, val=0.712, val_loss=3.24]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 26/500 [1:15:52<23:00:07, 174.70s/it, min_val_loss=1.4, train=0.915, val=0.721, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [1:15:52<22:59:18, 174.96s/it, min_val_loss=1.4, train=0.915, val=0.721, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 27/500 [1:18:48<22:59:18, 174.96s/it, min_val_loss=1.39, train=0.928, val=0.734, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [1:18:48<22:59:14, 175.33s/it, min_val_loss=1.39, train=0.928, val=0.734, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 28/500 [1:21:46<22:59:14, 175.33s/it, min_val_loss=1.36, train=0.918, val=0.706, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [1:21:46<23:01:30, 175.99s/it, min_val_loss=1.36, train=0.918, val=0.706, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 29/500 [1:24:38<23:01:30, 175.99s/it, min_val_loss=1.36, train=0.883, val=0.692, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [1:24:38<22:50:08, 174.91s/it, min_val_loss=1.36, train=0.883, val=0.692, val_loss=3.23]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 30/500 [1:27:27<22:50:08, 174.91s/it, min_val_loss=1.36, train=0.904, val=0.703, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [1:27:27<22:33:59, 173.22s/it, min_val_loss=1.36, train=0.904, val=0.703, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▌         | 31/500 [1:30:17<22:33:59, 173.22s/it, min_val_loss=1.36, train=0.92, val=0.754, val_loss=3.2]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [1:30:17<22:23:47, 172.28s/it, min_val_loss=1.36, train=0.92, val=0.754, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   6%|▋         | 32/500 [1:33:06<22:23:47, 172.28s/it, min_val_loss=1.33, train=0.934, val=0.735, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [1:33:06<22:13:12, 171.29s/it, min_val_loss=1.33, train=0.934, val=0.735, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 33/500 [1:35:57<22:13:12, 171.29s/it, min_val_loss=1.33, train=0.928, val=0.738, val_loss=3.2] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [1:35:57<22:08:47, 171.09s/it, min_val_loss=1.33, train=0.928, val=0.738, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 34/500 [1:38:51<22:08:47, 171.09s/it, min_val_loss=1.33, train=0.942, val=0.72, val_loss=3.2] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 35/500 [1:38:51<22:12:49, 171.98s/it, min_val_loss=1.33, train=0.942, val=0.72, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 35/500 [1:41:41<22:12:49, 171.98s/it, min_val_loss=1.33, train=0.963, val=0.754, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 36/500 [1:41:41<22:04:37, 171.29s/it, min_val_loss=1.33, train=0.963, val=0.754, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 36/500 [1:44:30<22:04:37, 171.29s/it, min_val_loss=1.33, train=0.96, val=0.771, val_loss=3.17] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 37/500 [1:44:30<21:56:36, 170.62s/it, min_val_loss=1.33, train=0.96, val=0.771, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   7%|▋         | 37/500 [1:47:18<21:56:36, 170.62s/it, min_val_loss=1.3, train=0.967, val=0.748, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 38/500 [1:47:18<21:49:04, 170.01s/it, min_val_loss=1.3, train=0.967, val=0.748, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 38/500 [1:50:10<21:49:04, 170.01s/it, min_val_loss=1.3, train=0.974, val=0.748, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 39/500 [1:50:10<21:49:01, 170.37s/it, min_val_loss=1.3, train=0.974, val=0.748, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 39/500 [1:53:00<21:49:01, 170.37s/it, min_val_loss=1.3, train=0.981, val=0.755, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 40/500 [1:53:00<21:47:16, 170.51s/it, min_val_loss=1.3, train=0.981, val=0.755, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 40/500 [1:55:51<21:47:16, 170.51s/it, min_val_loss=1.3, train=0.979, val=0.754, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 41/500 [1:55:51<21:45:01, 170.59s/it, min_val_loss=1.3, train=0.979, val=0.754, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 41/500 [1:58:40<21:45:01, 170.59s/it, min_val_loss=1.3, train=0.981, val=0.765, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 42/500 [1:58:40<21:37:49, 170.02s/it, min_val_loss=1.3, train=0.981, val=0.765, val_loss=3.15]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "total number of epochs:  41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteration:   8%|▊         | 42/500 [1:58:50<21:35:54, 169.77s/it, min_val_loss=1.3, train=0.981, val=0.765, val_loss=3.15]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 45221<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_073444-2sfcgvy4/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_073444-2sfcgvy4/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>41</td></tr><tr><td>learning rate</td><td>0.00125</td></tr><tr><td>validation_fine_f1</td><td>0.77073</td></tr><tr><td>training_fine_f1</td><td>0.98106</td></tr><tr><td>test_fine_f1</td><td>0.78414</td></tr><tr><td>validation_loss</td><td>3.15337</td></tr><tr><td>_runtime</td><td>7123</td></tr><tr><td>_timestamp</td><td>1620740007</td></tr><tr><td>_step</td><td>820</td></tr><tr><td>loss</td><td>0.22349</td></tr><tr><td>batch</td><td>797</td></tr><tr><td>loss_fine</td><td>0.6049</td></tr><tr><td>lambda_fine</td><td>0.10948</td></tr><tr><td>loss_layer2</td><td>0.18957</td></tr><tr><td>lambda_layer2</td><td>0.39841</td></tr><tr><td>nonzerotriplets_layer2</td><td>1</td></tr><tr><td>loss_layer4</td><td>0.1661</td></tr><tr><td>lambda_layer4</td><td>0.49211</td></tr><tr><td>nonzerotriplets_layer4</td><td>2</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>██████████████████████▄▄▄▄▄▄▄▄▄▄▄▄▂▂▂▂▂▁</td></tr><tr><td>validation_fine_f1</td><td>▁▂▃▄▂▃▅▅▅▄▅▅▆▆▆▇▆▆▆▇▆▅▆▇▇██▇▇▇██████████</td></tr><tr><td>training_fine_f1</td><td>▁▂▃▄▂▃▅▅▅▄▅▅▅▆▆▆▆▆▆▇▆▆▆▇▇███▇▇██████████</td></tr><tr><td>test_fine_f1</td><td>▁▂▃▃▃▄▅▅▅▄▅▅▆▆▆▆▆▆▆▇▅▅▆▇▇▇█▇▇▇█▇████████</td></tr><tr><td>validation_loss</td><td>██▇▇▇▇▆▅▆▆▆▅▅▅▄▄▄▄▄▃▄▅▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▆▆▅▃▅▅▅▅▅▅▅▄▂▂▄▂▄▂▂▂▂▂▁▁▂▁▂▄▁▁▁▁▁▃▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▇▆▅▅▅▄▄▄▄▃▄▄▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▃▂▄▂▅█▅▂▇▆</td></tr><tr><td>loss_layer2</td><td>█▇███▁▇█▇██▇▇█▁▁▇▁█▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▇█▁▁▁</td></tr><tr><td>lambda_layer2</td><td>▄▄▄▄▄▇▄▄▄▄▄▄▄▄▇▇▄▇▄▇▇▇▇▇▇▇█▇▇▄▇▇▇▇▇▄▁▇▇▇</td></tr><tr><td>nonzerotriplets_layer2</td><td>█▃▃▃▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_layer4</td><td>▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆█▆▆▆▆▆▆▆▆▆▁▆▆▅</td></tr><tr><td>lambda_layer4</td><td>▅▅▅▅▅▂▅▅▅▅▅▅▅▅▂▂▅▂▅▂▂▂▂▂▂▂▁▂▂▅▂▂▂▂▂▅█▂▂▂</td></tr><tr><td>nonzerotriplets_layer4</td><td>▅▄▃▅▂▅▃█▅▅▅▅▅▅▅▇▃▃▄▃▄▃▃▅▃▃▁▃▂▃▃▂▃▃▃▁▁▃▃▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">a68727a46b2913194bdaa7ed977c51aad2130bd058da8a824def7b96</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2sfcgvy4\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2sfcgvy4</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "experiment:  71%|███████▏  | 5/7 [8:28:34<3:25:16, 6158.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.005, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish lr 0.05', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc27794096e74c4ab3cfb52c1936076c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_phylogeny_tripletloss_new_archi_full', 'modelName': 'models/db09c1fb1103170195903a5baafd469fc9dc48cfee2633db0d2fa6e9', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': 'adabce7dec9aebc1e0b2bd518e6ce6164a0f0705eb07215cf79b8983', 'trialHash': 'db09c1fb1103170195903a5baafd469fc9dc48cfee2633db0d2fa6e9', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 224, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.005, 'numOfTrials': 1, 'fc_layers': 1, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': True, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 0.2, 'phylogeny_loss': False, 'displayName': 'Fish lr 0.05', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">db09c1fb1103170195903a5baafd469fc9dc48cfee2633db0d2fa6e9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/173u6cdi\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/173u6cdi</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210511_093341-173u6cdi</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/500 [00:37<?, ?it/s, min_val_loss=inf, train=0.00696, val=0.00661, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [00:37<5:16:01, 38.00s/it, min_val_loss=inf, train=0.00696, val=0.00661, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 1/500 [03:26<5:16:01, 38.00s/it, min_val_loss=inf, train=0.0807, val=0.0934, val_loss=3.62]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [03:26<10:39:08, 77.00s/it, min_val_loss=inf, train=0.0807, val=0.0934, val_loss=3.62]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   0%|          | 2/500 [06:18<10:39:08, 77.00s/it, min_val_loss=10.7, train=0.194, val=0.162, val_loss=3.6]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [06:18<14:35:40, 105.72s/it, min_val_loss=10.7, train=0.194, val=0.162, val_loss=3.6]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 3/500 [09:12<14:35:40, 105.72s/it, min_val_loss=6.17, train=0.335, val=0.293, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [09:12<17:22:26, 126.10s/it, min_val_loss=6.17, train=0.335, val=0.293, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 4/500 [12:04<17:22:26, 126.10s/it, min_val_loss=3.42, train=0.395, val=0.331, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [12:04<19:15:25, 140.05s/it, min_val_loss=3.42, train=0.395, val=0.331, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 5/500 [14:57<19:15:25, 140.05s/it, min_val_loss=3.02, train=0.425, val=0.338, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [14:57<20:32:04, 149.65s/it, min_val_loss=3.02, train=0.425, val=0.338, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|          | 6/500 [17:50<20:32:04, 149.65s/it, min_val_loss=2.96, train=0.483, val=0.375, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [17:50<21:29:33, 156.94s/it, min_val_loss=2.96, train=0.483, val=0.375, val_loss=3.53]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   1%|▏         | 7/500 [20:42<21:29:33, 156.94s/it, min_val_loss=2.67, train=0.482, val=0.38, val_loss=3.51] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [20:42<22:03:56, 161.46s/it, min_val_loss=2.67, train=0.482, val=0.38, val_loss=3.51]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 8/500 [23:36<22:03:56, 161.46s/it, min_val_loss=2.63, train=0.503, val=0.407, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [23:36<22:30:27, 165.02s/it, min_val_loss=2.63, train=0.503, val=0.407, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 9/500 [26:28<22:30:27, 165.02s/it, min_val_loss=2.46, train=0.524, val=0.452, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [26:28<22:45:14, 167.17s/it, min_val_loss=2.46, train=0.524, val=0.452, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 10/500 [29:19<22:45:14, 167.17s/it, min_val_loss=2.21, train=0.528, val=0.459, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [29:19<22:52:00, 168.34s/it, min_val_loss=2.21, train=0.528, val=0.459, val_loss=3.49]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 11/500 [32:10<22:52:00, 168.34s/it, min_val_loss=2.18, train=0.552, val=0.483, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [32:10<22:54:21, 168.98s/it, min_val_loss=2.18, train=0.552, val=0.483, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▏         | 12/500 [35:01<22:54:21, 168.98s/it, min_val_loss=2.07, train=0.544, val=0.481, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [35:01<22:58:40, 169.86s/it, min_val_loss=2.07, train=0.544, val=0.481, val_loss=3.46]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 13/500 [37:51<22:58:40, 169.86s/it, min_val_loss=2.07, train=0.508, val=0.433, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [37:51<22:56:09, 169.90s/it, min_val_loss=2.07, train=0.508, val=0.433, val_loss=3.48]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 14/500 [40:41<22:56:09, 169.90s/it, min_val_loss=2.07, train=0.677, val=0.574, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [40:41<22:51:35, 169.68s/it, min_val_loss=2.07, train=0.677, val=0.574, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 15/500 [43:31<22:51:35, 169.68s/it, min_val_loss=1.74, train=0.614, val=0.528, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [43:31<22:49:44, 169.80s/it, min_val_loss=1.74, train=0.614, val=0.528, val_loss=3.44]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 16/500 [46:21<22:49:44, 169.80s/it, min_val_loss=1.74, train=0.677, val=0.597, val_loss=3.4] \u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [46:21<22:47:14, 169.84s/it, min_val_loss=1.74, train=0.677, val=0.597, val_loss=3.4]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   3%|▎         | 17/500 [49:12<22:47:14, 169.84s/it, min_val_loss=1.68, train=0.622, val=0.499, val_loss=3.45]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   4%|▎         | 18/500 [49:12<22:47:25, 170.22s/it, min_val_loss=1.68, train=0.622, val=0.499, val_loss=3.45]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    print('wandb not found')\n",
    "\n",
    "from myhelpers import config_plots, TrialStatistics\n",
    "from myhelpers.try_warning import try_running\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "config_plots.global_settings()\n",
    "\n",
    "experimetnsFileName = \"experiments.csv\"\n",
    "WANDB_message=\"wandb not working\"\n",
    "\n",
    "\n",
    "# For logging to server\n",
    "try_running(lambda : wandb.login(), WANDB_message)\n",
    "\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "# set cuda\n",
    "if device is not None:\n",
    "    print(\"using cuda\", device)\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# get experiment params\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "\n",
    "# init experiments file\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "\n",
    "paramsIterator = config_parser.getExperiments()  \n",
    "number_of_experiments = sum(1 for e in paramsIterator)  \n",
    "experiment_index = 0\n",
    "\n",
    "# Loop through experiments\n",
    "# with progressbar.ProgressBar(max_value=number_of_experiments) as bar:\n",
    "with tqdm(total=number_of_experiments, desc=\"experiment\") as bar:\n",
    "    for experiment_params in config_parser.getExperiments():\n",
    "        print(experiment_params)\n",
    "        experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "        # load images \n",
    "        if experiment_params['image_path'] == 'cifar-100-python':\n",
    "            datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        else:\n",
    "            datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "        train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "        architecture = {\n",
    "            \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "            \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "        }\n",
    "\n",
    "        # Loop through n trials\n",
    "        for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "            modelName = getModelName(experiment_params, i)\n",
    "            trialName = os.path.join(experimentPathAndName, modelName)\n",
    "            trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "            row_information = {\n",
    "                'experimentName': experimentName,\n",
    "                'modelName': modelName,\n",
    "                'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "                'experimentHash': experimentHash,\n",
    "                'trialHash': trialHash\n",
    "            }\n",
    "            row_information = {**row_information, **experiment_params} \n",
    "            print(row_information)\n",
    "\n",
    "            run = try_running(lambda : wandb.init(project='HGNN', group=experimentName+\"-\"+experimentHash, name=trialHash, config=row_information), WANDB_message) #, reinit=True\n",
    "\n",
    "            # Train/Load model\n",
    "            model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "\n",
    "#             try_running(lambda : wandb.watch(model, log=\"all\"), WANDB_message)\n",
    "\n",
    "            if os.path.exists(CNN.getModelFile(trialName)):\n",
    "                print(\"Model {0} found!\".format(trialName))\n",
    "            else:\n",
    "                initModelPath = CNN.getInitModelFile(experimentPathAndName)\n",
    "                if os.path.exists(initModelPath):\n",
    "                    model.load_state_dict(torch.load(initModelPath))\n",
    "                    print(\"Init Model {0} found!\".format(initModelPath))\n",
    "                CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=detailed_reporting)\n",
    "\n",
    "            # Add to experiments file\n",
    "            if os.path.exists(experimentsFileNameAndPath):\n",
    "                experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "            else:\n",
    "                experiments_df = pd.DataFrame()\n",
    "\n",
    "            record_exists = not (experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName]).empty if not experiments_df.empty else False\n",
    "            if record_exists:\n",
    "                experiments_df.drop(experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName].index, inplace = True) \n",
    "\n",
    "            experiments_df = experiments_df.append(pd.DataFrame(row_information, index=[0]), ignore_index = True)\n",
    "            experiments_df.to_csv(experimentsFileNameAndPath, header=True, index=False)\n",
    "\n",
    "            try_running(lambda : run.finish(), WANDB_message)\n",
    "\n",
    "        bar.update()\n",
    "\n",
    "        experiment_index = experiment_index + 1\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "#     import argparse\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--cuda', required=True, type=int)\n",
    "#     parser.add_argument('--experiments', required=True)\n",
    "#     parser.add_argument('--data', required=True)\n",
    "#     parser.add_argument('--name', required=True)\n",
    "#     parser.add_argument('--detailed', required=False, action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "#     main(experimentName=args.name, experimentsPath=args.experiments, dataPath=args.data, device=args.cuda, detailed_reporting=args.detailed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
