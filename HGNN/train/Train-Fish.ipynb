{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath = \"/raid/elhamod/Fish/experiments/\" #\"/raid/elhamod/CIFAR_HGNN/experiments/\" #\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath = \"/raid/elhamod/Fish/\" # \"/raid/elhamod/\" #\"/raid/elhamod/Fish/\"\n",
    "experimentName = \"Fish_s2_experiments\" #\"Fish_tripletloss_alpha_lr_experiments\"\n",
    "device = 0\n",
    "detailed_reporting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmndhamod\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "experiment:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 0\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.0001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 80, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-BBlonger', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8942cdfbe2340629667f5af87de3870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_s2_experiments', 'modelName': 'models/9597809d2f803c57e02d64f9a42d23d1822ccc6d0ad5ca00e6e4068f', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '0e17525006238fdd6bd48f93259409cda8382b3cd2ee4de330ad17d3', 'trialHash': '9597809d2f803c57e02d64f9a42d23d1822ccc6d0ad5ca00e6e4068f', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.0001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 80, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-BBlonger', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">9597809d2f803c57e02d64f9a42d23d1822ccc6d0ad5ca00e6e4068f</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/189hkoli\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/189hkoli</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125614-189hkoli</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /raid/elhamod/Fish/experiments/Fish_s2_experiments/models/9597809d2f803c57e02d64f9a42d23d1822ccc6d0ad5ca00e6e4068f found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/ipykernel_launcher.py:113: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "/home/elhamod/melhamodenv3/lib/python3.6/site-packages/ipykernel_launcher.py:115: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26674<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81499e07455348ecb7eef9415e0c001f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125614-189hkoli/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125614-189hkoli/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">9597809d2f803c57e02d64f9a42d23d1822ccc6d0ad5ca00e6e4068f</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/189hkoli\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/189hkoli</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  10%|█         | 1/10 [00:10<01:36, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 10, 'phylogeny_loss': False, 'displayName': 'Fish-s2-hier_margin10', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca92bdef28a468283271afebad8cf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_s2_experiments', 'modelName': 'models/c04fc1ad4011d9237677e11cc4a6f331616d59ac474c51ef3f8452e1', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '00238d12bbcb9a26bcc36612c1ee3cc20477f5481c44a2d6f5424cd3', 'trialHash': 'c04fc1ad4011d9237677e11cc4a6f331616d59ac474c51ef3f8452e1', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 10, 'phylogeny_loss': False, 'displayName': 'Fish-s2-hier_margin10', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">c04fc1ad4011d9237677e11cc4a6f331616d59ac474c51ef3f8452e1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/3pm0o3hl\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/3pm0o3hl</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125624-3pm0o3hl</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /raid/elhamod/Fish/experiments/Fish_s2_experiments/models/c04fc1ad4011d9237677e11cc4a6f331616d59ac474c51ef3f8452e1 found!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26888<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dbff196e5f414ea415d112885d3d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125624-3pm0o3hl/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125624-3pm0o3hl/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">c04fc1ad4011d9237677e11cc4a6f331616d59ac474c51ef3f8452e1</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/3pm0o3hl\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/3pm0o3hl</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  20%|██        | 2/10 [00:17<01:16,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': 'MSE', 'displayName': 'Fish-s2-hierMSE', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ace812dacce43a59f53a16cf21fce83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_s2_experiments', 'modelName': 'models/84afebe75e824b67ce1317b400057ea8cc84e9cb2a613100a6c8a524', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '31655405f541d9a8fc7e6faf1fcbb69c754e8325066ca5b81edaf642', 'trialHash': '84afebe75e824b67ce1317b400057ea8cc84e9cb2a613100a6c8a524', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': 'MSE', 'displayName': 'Fish-s2-hierMSE', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">84afebe75e824b67ce1317b400057ea8cc84e9cb2a613100a6c8a524</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/ddn4apgi\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/ddn4apgi</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125631-ddn4apgi</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /raid/elhamod/Fish/experiments/Fish_s2_experiments/models/84afebe75e824b67ce1317b400057ea8cc84e9cb2a613100a6c8a524 found!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27070<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c93a76f2da4c0bbacca91d62cc0ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125631-ddn4apgi/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125631-ddn4apgi/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">84afebe75e824b67ce1317b400057ea8cc84e9cb2a613100a6c8a524</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/ddn4apgi\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/ddn4apgi</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  30%|███       | 3/10 [00:24<01:01,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': 'KLDiv', 'displayName': 'Fish-s2-hierKLDiv', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2830d7f3d54d49aa9d7b4d2ae4d2c349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_s2_experiments', 'modelName': 'models/49c9e9af8e98f72e0a47d50cedb832db346b52951c4647c14259de68', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': 'faa3b6b8795a2cd76062199dd40c23a117a499a4e2942ff13142a1af', 'trialHash': '49c9e9af8e98f72e0a47d50cedb832db346b52951c4647c14259de68', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'BB', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': True, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': 'KLDiv', 'displayName': 'Fish-s2-hierKLDiv', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">49c9e9af8e98f72e0a47d50cedb832db346b52951c4647c14259de68</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2xivfw9i\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2xivfw9i</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125638-2xivfw9i</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /raid/elhamod/Fish/experiments/Fish_s2_experiments/models/49c9e9af8e98f72e0a47d50cedb832db346b52951c4647c14259de68 found!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27242<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2270c7ea3094302a6c9297eea0f288f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125638-2xivfw9i/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125638-2xivfw9i/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">49c9e9af8e98f72e0a47d50cedb832db346b52951c4647c14259de68</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2xivfw9i\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2xivfw9i</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  40%|████      | 4/10 [00:31<00:49,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'DISCO', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-DISCO', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172e9a83b49944de9323ecaadbba4fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_s2_experiments', 'modelName': 'models/3cf799ec684da6831701cf0ea1bfcbfd8dd79e05265fef4e09575980', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '5b2cc133e4eb601edd17ad15e64c541215319c3901a3dcd561ea8620', 'trialHash': '3cf799ec684da6831701cf0ea1bfcbfd8dd79e05265fef4e09575980', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'DISCO', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-DISCO', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">3cf799ec684da6831701cf0ea1bfcbfd8dd79e05265fef4e09575980</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/x3giufmx\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/x3giufmx</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125645-x3giufmx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /raid/elhamod/Fish/experiments/Fish_s2_experiments/models/3cf799ec684da6831701cf0ea1bfcbfd8dd79e05265fef4e09575980 found!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27417<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f44137413a4a3a9730eb9bc62b4b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125645-x3giufmx/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125645-x3giufmx/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">3cf799ec684da6831701cf0ea1bfcbfd8dd79e05265fef4e09575980</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/x3giufmx\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/x3giufmx</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  50%|█████     | 5/10 [00:38<00:39,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'DISCO', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-DISCO0.1', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c265312f1546b49db5ca7a4d657498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_s2_experiments', 'modelName': 'models/30c48681c072560bb8de81e1c94f0257d32cf43d7daa86dd9d5dfefa', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '7798f490b3c19f1a6f4de92668c1d315612dd1cd7a4cf13777be9695', 'trialHash': '30c48681c072560bb8de81e1c94f0257d32cf43d7daa86dd9d5dfefa', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'DISCO', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-DISCO0.1', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">30c48681c072560bb8de81e1c94f0257d32cf43d7daa86dd9d5dfefa</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1lok7k1a\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1lok7k1a</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125652-1lok7k1a</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /raid/elhamod/Fish/experiments/Fish_s2_experiments/models/30c48681c072560bb8de81e1c94f0257d32cf43d7daa86dd9d5dfefa found!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27665<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7581d1cd9b4e518886c2a5862f2725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125652-1lok7k1a/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125652-1lok7k1a/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">30c48681c072560bb8de81e1c94f0257d32cf43d7daa86dd9d5dfefa</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/1lok7k1a\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/1lok7k1a</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  60%|██████    | 6/10 [00:46<00:31,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'DISCO', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.9, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-DISCO0.9', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51e7d1b18e34932976123ba54b1817b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_s2_experiments', 'modelName': 'models/cee60fe0e51af04a2a315e16cd933d2478d78cc9710c2eb0fa6c4688', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '285b16c6ecb4be4b37a1b06d576cc311df81293afe4ac33cc15da7a2', 'trialHash': 'cee60fe0e51af04a2a315e16cd933d2478d78cc9710c2eb0fa6c4688', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'DISCO', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.9, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-DISCO0.9', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">cee60fe0e51af04a2a315e16cd933d2478d78cc9710c2eb0fa6c4688</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2rcsujzk\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2rcsujzk</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125700-2rcsujzk</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /raid/elhamod/Fish/experiments/Fish_s2_experiments/models/cee60fe0e51af04a2a315e16cd933d2478d78cc9710c2eb0fa6c4688 found!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28134<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c9cc404b8142308e6c37f14543b4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125700-2rcsujzk/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125700-2rcsujzk/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">cee60fe0e51af04a2a315e16cd933d2478d78cc9710c2eb0fa6c4688</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/2rcsujzk\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/2rcsujzk</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  70%|███████   | 7/10 [00:55<00:24,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'HGNN', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-HGNN', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10dc34fa8c134863b386f9d5ce210d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_s2_experiments', 'modelName': 'models/ff8e0e5240e74bcd086728196757fae51b027d26127d97ffff791476', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': 'f25606227c0255bb5f09f018d35a475f57bc18d563ac792fdcfbdd5c', 'trialHash': 'ff8e0e5240e74bcd086728196757fae51b027d26127d97ffff791476', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'HGNN', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-HGNN', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">ff8e0e5240e74bcd086728196757fae51b027d26127d97ffff791476</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/12vu059t\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/12vu059t</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125709-12vu059t</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:45<?, ?it/s, min_val_loss=inf, train=0.00454, val=0.00132, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [00:45<29:53, 45.97s/it, min_val_loss=inf, train=0.00454, val=0.00132, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [02:05<29:53, 45.97s/it, min_val_loss=inf, train=0.236, val=0.194, val_loss=3.55]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [02:05<35:32, 56.11s/it, min_val_loss=inf, train=0.236, val=0.194, val_loss=3.55]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [03:27<35:32, 56.11s/it, min_val_loss=5.16, train=0.528, val=0.448, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [03:27<39:20, 63.79s/it, min_val_loss=5.16, train=0.528, val=0.448, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [04:50<39:20, 63.79s/it, min_val_loss=2.23, train=0.731, val=0.653, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [04:50<41:49, 69.70s/it, min_val_loss=2.23, train=0.731, val=0.653, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [06:16<41:49, 69.70s/it, min_val_loss=1.53, train=0.773, val=0.619, val_loss=3.3] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [06:16<43:23, 74.39s/it, min_val_loss=1.53, train=0.773, val=0.619, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [07:20<43:23, 74.39s/it, min_val_loss=1.53, train=0.879, val=0.694, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [07:20<40:28, 71.43s/it, min_val_loss=1.53, train=0.879, val=0.694, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [08:18<40:28, 71.43s/it, min_val_loss=1.44, train=0.925, val=0.663, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [08:18<37:03, 67.37s/it, min_val_loss=1.44, train=0.925, val=0.663, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [09:15<37:03, 67.37s/it, min_val_loss=1.44, train=0.947, val=0.695, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [09:15<34:18, 64.32s/it, min_val_loss=1.44, train=0.947, val=0.695, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [10:16<34:18, 64.32s/it, min_val_loss=1.44, train=0.986, val=0.745, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [10:16<32:38, 63.17s/it, min_val_loss=1.44, train=0.986, val=0.745, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [11:13<32:38, 63.17s/it, min_val_loss=1.34, train=0.996, val=0.789, val_loss=3.05]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [11:13<30:40, 61.35s/it, min_val_loss=1.34, train=0.996, val=0.789, val_loss=3.05]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [12:13<30:40, 61.35s/it, min_val_loss=1.27, train=0.993, val=0.806, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [12:13<29:27, 60.96s/it, min_val_loss=1.27, train=0.993, val=0.806, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [13:11<29:27, 60.96s/it, min_val_loss=1.24, train=0.998, val=0.825, val_loss=3]   \u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [13:11<28:01, 60.04s/it, min_val_loss=1.24, train=0.998, val=0.825, val_loss=3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [14:09<28:01, 60.04s/it, min_val_loss=1.21, train=0.977, val=0.743, val_loss=3.04]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [14:09<26:41, 59.32s/it, min_val_loss=1.21, train=0.977, val=0.743, val_loss=3.04]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [15:04<26:41, 59.32s/it, min_val_loss=1.21, train=0.972, val=0.761, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [15:04<25:14, 58.26s/it, min_val_loss=1.21, train=0.972, val=0.761, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [16:02<25:14, 58.26s/it, min_val_loss=1.21, train=0.943, val=0.706, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [16:02<24:09, 57.97s/it, min_val_loss=1.21, train=0.943, val=0.706, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [16:57<24:09, 57.97s/it, min_val_loss=1.21, train=0.678, val=0.465, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [16:57<22:54, 57.25s/it, min_val_loss=1.21, train=0.678, val=0.465, val_loss=3.27]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [17:53<22:54, 57.25s/it, min_val_loss=1.21, train=0.961, val=0.692, val_loss=3.1] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [17:53<21:46, 56.80s/it, min_val_loss=1.21, train=0.961, val=0.692, val_loss=3.1]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [18:49<21:46, 56.80s/it, min_val_loss=1.21, train=0.845, val=0.652, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [18:49<20:43, 56.54s/it, min_val_loss=1.21, train=0.845, val=0.652, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [19:43<20:43, 56.54s/it, min_val_loss=1.21, train=0.899, val=0.61, val_loss=3.18] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [19:43<19:35, 55.96s/it, min_val_loss=1.21, train=0.899, val=0.61, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [20:39<19:35, 55.96s/it, min_val_loss=1.21, train=0.91, val=0.636, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [20:39<18:34, 55.72s/it, min_val_loss=1.21, train=0.91, val=0.636, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [21:35<18:34, 55.72s/it, min_val_loss=1.21, train=0.88, val=0.649, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [21:35<17:40, 55.84s/it, min_val_loss=1.21, train=0.88, val=0.649, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [22:30<17:40, 55.84s/it, min_val_loss=1.21, train=0.911, val=0.65, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [22:30<16:44, 55.80s/it, min_val_loss=1.21, train=0.911, val=0.65, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [23:33<16:44, 55.80s/it, min_val_loss=1.21, train=0.939, val=0.664, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [23:33<16:20, 57.70s/it, min_val_loss=1.21, train=0.939, val=0.664, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [24:49<16:20, 57.70s/it, min_val_loss=1.21, train=0.998, val=0.817, val_loss=2.93]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [24:49<16:52, 63.28s/it, min_val_loss=1.21, train=0.998, val=0.817, val_loss=2.93]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [26:08<16:52, 63.28s/it, min_val_loss=1.21, train=1, val=0.858, val_loss=2.88]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [26:08<17:02, 68.14s/it, min_val_loss=1.21, train=1, val=0.858, val_loss=2.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [27:32<17:02, 68.14s/it, min_val_loss=1.17, train=1, val=0.845, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [27:32<16:57, 72.70s/it, min_val_loss=1.17, train=1, val=0.845, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [28:54<16:57, 72.70s/it, min_val_loss=1.17, train=0.999, val=0.849, val_loss=2.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [28:54<16:21, 75.50s/it, min_val_loss=1.17, train=0.999, val=0.849, val_loss=2.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [30:14<16:21, 75.50s/it, min_val_loss=1.17, train=0.984, val=0.811, val_loss=2.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [30:14<15:21, 76.80s/it, min_val_loss=1.17, train=0.984, val=0.811, val_loss=2.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [31:37<15:21, 76.80s/it, min_val_loss=1.17, train=1, val=0.849, val_loss=2.89]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 29/40 [31:37<14:26, 78.75s/it, min_val_loss=1.17, train=1, val=0.849, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 29/40 [32:58<14:26, 78.75s/it, min_val_loss=1.17, train=1, val=0.86, val_loss=2.86] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 30/40 [32:58<13:14, 79.43s/it, min_val_loss=1.17, train=1, val=0.86, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 30/40 [34:23<13:14, 79.43s/it, min_val_loss=1.16, train=1, val=0.871, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 31/40 [34:23<12:10, 81.18s/it, min_val_loss=1.16, train=1, val=0.871, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 31/40 [35:47<12:10, 81.18s/it, min_val_loss=1.15, train=1, val=0.861, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 32/40 [35:47<10:55, 81.94s/it, min_val_loss=1.15, train=1, val=0.861, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 32/40 [37:11<10:55, 81.94s/it, min_val_loss=1.15, train=1, val=0.859, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 33/40 [37:11<09:37, 82.55s/it, min_val_loss=1.15, train=1, val=0.859, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 33/40 [38:32<09:37, 82.55s/it, min_val_loss=1.15, train=1, val=0.878, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 34/40 [38:32<08:12, 82.04s/it, min_val_loss=1.15, train=1, val=0.878, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 34/40 [39:58<08:12, 82.04s/it, min_val_loss=1.14, train=1, val=0.868, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 35/40 [39:58<06:55, 83.19s/it, min_val_loss=1.14, train=1, val=0.868, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 35/40 [41:20<06:55, 83.19s/it, min_val_loss=1.14, train=1, val=0.874, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 36/40 [41:20<05:32, 83.01s/it, min_val_loss=1.14, train=1, val=0.874, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 36/40 [42:44<05:32, 83.01s/it, min_val_loss=1.14, train=1, val=0.869, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 37/40 [42:44<04:10, 83.35s/it, min_val_loss=1.14, train=1, val=0.869, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 37/40 [44:07<04:10, 83.35s/it, min_val_loss=1.14, train=1, val=0.877, val_loss=2.85]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 38/40 [44:07<02:46, 83.13s/it, min_val_loss=1.14, train=1, val=0.877, val_loss=2.85]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 38/40 [45:11<02:46, 83.13s/it, min_val_loss=1.14, train=1, val=0.874, val_loss=2.85]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 39/40 [45:11<01:17, 77.42s/it, min_val_loss=1.14, train=1, val=0.874, val_loss=2.85]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 39/40 [46:08<01:17, 77.42s/it, min_val_loss=1.14, train=1, val=0.867, val_loss=2.85]\u001b[A\u001b[A\n",
      "\n",
      "iteration: 100%|██████████| 40/40 [46:27<00:00, 69.68s/it, min_val_loss=1.14, train=1, val=0.867, val_loss=2.85]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28310<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b008107184468ab421d788af0da127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125709-12vu059t/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_125709-12vu059t/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>39</td></tr><tr><td>learning rate</td><td>0.0005</td></tr><tr><td>validation_fine_f1</td><td>0.87769</td></tr><tr><td>training_fine_f1</td><td>1.0</td></tr><tr><td>training_fine_acc</td><td>1.0</td></tr><tr><td>test_fine_f1</td><td>0.87879</td></tr><tr><td>test_fine_acc</td><td>0.87368</td></tr><tr><td>validation_loss</td><td>2.85436</td></tr><tr><td>_runtime</td><td>2773</td></tr><tr><td>_timestamp</td><td>1622051002</td></tr><tr><td>_step</td><td>780</td></tr><tr><td>loss</td><td>0.00097</td></tr><tr><td>batch</td><td>759</td></tr><tr><td>loss_fine</td><td>0.00117</td></tr><tr><td>lambda_fine</td><td>0.74948</td></tr><tr><td>loss_coarse</td><td>0.00035</td></tr><tr><td>lambda_coarse</td><td>0.25052</td></tr><tr><td>train_fine_acc</td><td>0.87895</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>██████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁▃▅▆▆▇▆▇▇▇▇█▇▇▇▅▇▆▆▆▆▆▆████▇████████████</td></tr><tr><td>training_fine_f1</td><td>▁▃▅▆▆▇▇████████▆█▇▇▇▇▇██████████████████</td></tr><tr><td>training_fine_acc</td><td>▁▃▅▆▆▇█████████▆█▇▇▇▇▇██████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▃▄▆▆▆▇▆▇█▇█▇▇▆▅▇▆▆▆▆▇▆█████████████████</td></tr><tr><td>test_fine_acc</td><td>▁▃▅▆▆▇▇▇▇█▇█▇▇▇▅▇▆▆▆▆▇▆█████████████████</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▂▃▃▄▅▃▃▄▄▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▆▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▂▅▆▅▆▇▇█▇▆▇▆▆▇▇██▇█████████████████</td></tr><tr><td>loss_coarse</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>█████▇▄▃▄▃▂▂▁▂▃▂▃▃▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">ff8e0e5240e74bcd086728196757fae51b027d26127d97ffff791476</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/12vu059t\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/12vu059t</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  80%|████████  | 8/10 [47:31<28:09, 844.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'HGNN', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-HGNN0.1', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3f8f72b5ab4e3984f39712fbe8d3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_s2_experiments', 'modelName': 'models/4fed54c097ce833e9ab2bb09237dfc6b15ca5215cf8420dc3a921965', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': 'f1c23baaba98b272a16923939acbd9bfe71b45f416b7f04182e5c9f3', 'trialHash': '4fed54c097ce833e9ab2bb09237dfc6b15ca5215cf8420dc3a921965', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'HGNN', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.1, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-HGNN0.1', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">4fed54c097ce833e9ab2bb09237dfc6b15ca5215cf8420dc3a921965</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/164rl5s1\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/164rl5s1</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_134345-164rl5s1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:37<?, ?it/s, min_val_loss=inf, train=0.00605, val=0.00246, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [00:37<24:04, 37.04s/it, min_val_loss=inf, train=0.00605, val=0.00246, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [01:31<24:04, 37.04s/it, min_val_loss=inf, train=0.193, val=0.123, val_loss=3.56]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [01:31<26:50, 42.38s/it, min_val_loss=inf, train=0.193, val=0.123, val_loss=3.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [02:30<26:50, 42.38s/it, min_val_loss=8.1, train=0.469, val=0.407, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [02:30<29:05, 47.16s/it, min_val_loss=8.1, train=0.469, val=0.407, val_loss=3.41]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [03:27<29:05, 47.16s/it, min_val_loss=2.45, train=0.712, val=0.597, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [03:27<30:06, 50.17s/it, min_val_loss=2.45, train=0.712, val=0.597, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [04:25<30:06, 50.17s/it, min_val_loss=1.67, train=0.809, val=0.64, val_loss=3.33] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [04:25<30:37, 52.51s/it, min_val_loss=1.67, train=0.809, val=0.64, val_loss=3.33]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [05:23<30:37, 52.51s/it, min_val_loss=1.56, train=0.924, val=0.713, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [05:23<30:37, 54.05s/it, min_val_loss=1.56, train=0.924, val=0.713, val_loss=3.25]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [06:21<30:37, 54.05s/it, min_val_loss=1.4, train=0.968, val=0.75, val_loss=3.19]  \u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [06:21<30:31, 55.51s/it, min_val_loss=1.4, train=0.968, val=0.75, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [07:21<30:31, 55.51s/it, min_val_loss=1.33, train=0.968, val=0.717, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [07:21<30:10, 56.59s/it, min_val_loss=1.33, train=0.968, val=0.717, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [08:16<30:10, 56.59s/it, min_val_loss=1.33, train=0.916, val=0.637, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [08:16<29:03, 56.24s/it, min_val_loss=1.33, train=0.916, val=0.637, val_loss=3.16]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [09:13<29:03, 56.24s/it, min_val_loss=1.33, train=0.861, val=0.608, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [09:13<28:13, 56.43s/it, min_val_loss=1.33, train=0.861, val=0.608, val_loss=3.18]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [10:07<28:13, 56.43s/it, min_val_loss=1.33, train=0.929, val=0.689, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [10:07<27:00, 55.88s/it, min_val_loss=1.33, train=0.929, val=0.689, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [11:04<27:00, 55.88s/it, min_val_loss=1.33, train=0.778, val=0.575, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [11:04<26:08, 56.01s/it, min_val_loss=1.33, train=0.778, val=0.575, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [11:58<26:08, 56.01s/it, min_val_loss=1.33, train=0.726, val=0.523, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [11:58<25:01, 55.62s/it, min_val_loss=1.33, train=0.726, val=0.523, val_loss=3.19]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [12:54<25:01, 55.62s/it, min_val_loss=1.33, train=0.876, val=0.619, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [12:54<24:06, 55.63s/it, min_val_loss=1.33, train=0.876, val=0.619, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [13:51<24:06, 55.63s/it, min_val_loss=1.33, train=0.874, val=0.594, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [13:51<23:20, 56.02s/it, min_val_loss=1.33, train=0.874, val=0.594, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [14:47<23:20, 56.02s/it, min_val_loss=1.33, train=0.853, val=0.597, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [14:47<22:24, 56.02s/it, min_val_loss=1.33, train=0.853, val=0.597, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [15:42<22:24, 56.02s/it, min_val_loss=1.33, train=0.896, val=0.62, val_loss=3.12] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [15:42<21:19, 55.63s/it, min_val_loss=1.33, train=0.896, val=0.62, val_loss=3.12]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [16:37<21:19, 55.63s/it, min_val_loss=1.33, train=0.949, val=0.724, val_loss=3.02]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [16:37<20:20, 55.49s/it, min_val_loss=1.33, train=0.949, val=0.724, val_loss=3.02]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [17:32<20:20, 55.49s/it, min_val_loss=1.33, train=0.993, val=0.733, val_loss=2.98]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [17:32<19:20, 55.28s/it, min_val_loss=1.33, train=0.993, val=0.733, val_loss=2.98]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [18:28<19:20, 55.28s/it, min_val_loss=1.33, train=0.998, val=0.815, val_loss=2.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [18:28<18:28, 55.43s/it, min_val_loss=1.33, train=0.998, val=0.815, val_loss=2.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [19:27<18:28, 55.43s/it, min_val_loss=1.23, train=0.999, val=0.843, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [19:27<17:54, 56.53s/it, min_val_loss=1.23, train=0.999, val=0.843, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [20:25<17:54, 56.53s/it, min_val_loss=1.19, train=0.995, val=0.769, val_loss=2.94]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [20:25<17:09, 57.21s/it, min_val_loss=1.19, train=0.995, val=0.769, val_loss=2.94]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [21:21<17:09, 57.21s/it, min_val_loss=1.19, train=0.999, val=0.825, val_loss=2.9] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [21:21<16:01, 56.59s/it, min_val_loss=1.19, train=0.999, val=0.825, val_loss=2.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [22:17<16:01, 56.59s/it, min_val_loss=1.19, train=0.999, val=0.831, val_loss=2.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [22:17<15:04, 56.54s/it, min_val_loss=1.19, train=0.999, val=0.831, val_loss=2.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [23:12<15:04, 56.54s/it, min_val_loss=1.19, train=1, val=0.825, val_loss=2.9]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [23:12<14:02, 56.19s/it, min_val_loss=1.19, train=1, val=0.825, val_loss=2.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [24:10<14:02, 56.19s/it, min_val_loss=1.19, train=1, val=0.805, val_loss=2.92]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [24:10<13:12, 56.59s/it, min_val_loss=1.19, train=1, val=0.805, val_loss=2.92]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [25:05<13:12, 56.59s/it, min_val_loss=1.19, train=0.996, val=0.823, val_loss=2.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [25:05<12:08, 56.01s/it, min_val_loss=1.19, train=0.996, val=0.823, val_loss=2.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [26:01<12:08, 56.01s/it, min_val_loss=1.19, train=0.957, val=0.745, val_loss=2.98]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [26:01<11:14, 56.24s/it, min_val_loss=1.19, train=0.957, val=0.745, val_loss=2.98]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [26:58<11:14, 56.24s/it, min_val_loss=1.19, train=0.989, val=0.829, val_loss=2.9] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 29/40 [26:58<10:18, 56.25s/it, min_val_loss=1.19, train=0.989, val=0.829, val_loss=2.9]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 29/40 [27:53<10:18, 56.25s/it, min_val_loss=1.19, train=0.979, val=0.779, val_loss=2.97]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 30/40 [27:53<09:19, 55.91s/it, min_val_loss=1.19, train=0.979, val=0.779, val_loss=2.97]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 30/40 [28:49<09:19, 55.91s/it, min_val_loss=1.19, train=0.976, val=0.797, val_loss=2.96]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 31/40 [28:49<08:24, 56.03s/it, min_val_loss=1.19, train=0.976, val=0.797, val_loss=2.96]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 31/40 [29:45<08:24, 56.03s/it, min_val_loss=1.19, train=0.962, val=0.68, val_loss=3.05] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 32/40 [29:45<07:27, 55.88s/it, min_val_loss=1.19, train=0.962, val=0.68, val_loss=3.05]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 32/40 [30:40<07:27, 55.88s/it, min_val_loss=1.19, train=0.996, val=0.833, val_loss=2.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 33/40 [30:40<06:29, 55.71s/it, min_val_loss=1.19, train=0.996, val=0.833, val_loss=2.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 33/40 [31:36<06:29, 55.71s/it, min_val_loss=1.19, train=0.998, val=0.853, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 34/40 [31:36<05:34, 55.78s/it, min_val_loss=1.19, train=0.998, val=0.853, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 34/40 [32:35<05:34, 55.78s/it, min_val_loss=1.17, train=0.999, val=0.855, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 35/40 [32:35<04:44, 56.82s/it, min_val_loss=1.17, train=0.999, val=0.855, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 35/40 [33:33<04:44, 56.82s/it, min_val_loss=1.17, train=1, val=0.857, val_loss=2.87]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 36/40 [33:33<03:48, 57.21s/it, min_val_loss=1.17, train=1, val=0.857, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 36/40 [34:30<03:48, 57.21s/it, min_val_loss=1.17, train=0.999, val=0.843, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 37/40 [34:30<02:51, 57.06s/it, min_val_loss=1.17, train=0.999, val=0.843, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 37/40 [35:25<02:51, 57.06s/it, min_val_loss=1.17, train=1, val=0.857, val_loss=2.86]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 38/40 [35:25<01:53, 56.50s/it, min_val_loss=1.17, train=1, val=0.857, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 38/40 [36:22<01:53, 56.50s/it, min_val_loss=1.17, train=0.999, val=0.854, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 39/40 [36:22<00:56, 56.56s/it, min_val_loss=1.17, train=0.999, val=0.854, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 39/40 [37:15<00:56, 56.56s/it, min_val_loss=1.17, train=1, val=0.841, val_loss=2.87]    \u001b[A\u001b[A\n",
      "\n",
      "iteration: 100%|██████████| 40/40 [37:33<00:00, 56.33s/it, min_val_loss=1.17, train=1, val=0.841, val_loss=2.87]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 40851<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecc5c47f51444b1913443df1c4dfaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_134345-164rl5s1/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_134345-164rl5s1/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>39</td></tr><tr><td>learning rate</td><td>0.00025</td></tr><tr><td>validation_fine_f1</td><td>0.85711</td></tr><tr><td>training_fine_f1</td><td>1.0</td></tr><tr><td>training_fine_acc</td><td>1.0</td></tr><tr><td>test_fine_f1</td><td>0.86523</td></tr><tr><td>test_fine_acc</td><td>0.86579</td></tr><tr><td>validation_loss</td><td>2.87471</td></tr><tr><td>_runtime</td><td>2239</td></tr><tr><td>_timestamp</td><td>1622053264</td></tr><tr><td>_step</td><td>780</td></tr><tr><td>loss</td><td>0.00087</td></tr><tr><td>batch</td><td>759</td></tr><tr><td>loss_fine</td><td>0.00115</td></tr><tr><td>lambda_fine</td><td>0.54872</td></tr><tr><td>loss_coarse</td><td>0.00052</td></tr><tr><td>lambda_coarse</td><td>0.45128</td></tr><tr><td>train_fine_acc</td><td>0.86579</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>█████████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁▂▄▆▆▇▇▇▆▆▇▆▅▆▆▆▆▇▇██▇█████▇█▇█▇████████</td></tr><tr><td>training_fine_f1</td><td>▁▂▄▆▇▇██▇▇▇▆▆▇▇▇▇███████████████████████</td></tr><tr><td>training_fine_acc</td><td>▁▃▅▆▇▇██▇▇█▆▆▇▇▇▇███████████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▂▄▆▆▆▇▇▇▆▇▅▅▆▆▆▆▇▇▇███████▇██▇▇████████</td></tr><tr><td>test_fine_acc</td><td>▁▃▅▆▆▆▇▇▇▆▇▅▅▆▆▆▆▇▇████████▇██▇▇████████</td></tr><tr><td>validation_loss</td><td>█▇▆▆▅▅▄▄▄▄▃▄▄▃▄▃▃▂▂▁▁▂▁▁▁▂▁▂▁▂▂▃▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▆▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▁▁▁▁▁▁▂▃▃▄▅▄▆▆▇▆▄▅▇▇▇▇███▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>loss_coarse</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>██████▇▆▆▅▄▅▃▃▂▃▅▄▂▂▂▂▁▁▁▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">4fed54c097ce833e9ab2bb09237dfc6b15ca5215cf8420dc3a921965</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/164rl5s1\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/164rl5s1</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:  90%|█████████ | 9/10 [1:25:11<21:09, 1269.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'HGNN', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.9, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-HGNN0.9', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n",
      "Creating datasets...\n",
      "{'Alosa chrysochloris': 482298, 'Carassius auratus': 1005907, 'Cyprinus carpio': 429083, 'Esox americanus': 496115, 'Gambusia affinis': 617445, 'Lepisosteus osseus': 519445, 'Lepisosteus platostomus': 731608, 'Lepomis auritus': 1002718, 'Lepomis cyanellus': 476361, 'Lepomis gibbosus': 670266, 'Lepomis gulosus': 476359, 'Lepomis humilis': 892772, 'Lepomis macrochirus': 836783, 'Lepomis megalotis': 271249, 'Lepomis microlophus': 271244, 'Morone chrysops': 246133, 'Morone mississippiensis': 769290, 'Notropis atherinoides': 636312, 'Notropis blennius': 419165, 'Notropis boops': 443777, 'Notropis buccatus': 269524, 'Notropis buchanani': 555686, 'Notropis dorsalis': 419160, 'Notropis hudsonius': 135051, 'Notropis leuciodus': 338652, 'Notropis nubilus': 550199, 'Notropis percobromus': 403731, 'Notropis stramineus': 351741, 'Notropis telescopus': 550190, 'Notropis texanus': 550208, 'Notropis volucellus': 351735, 'Notropis wickliffi': 563834, 'Noturus exilis': 678206, 'Noturus flavus': 101864, 'Noturus gyrinus': 652777, 'Noturus miurus': 282530, 'Noturus nocturnus': 621586, 'Phenacobius mirabilis': 945111}\n",
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a346a59b0e43cd8114a7a77caa9387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='trial', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish_s2_experiments', 'modelName': 'models/22f3c58fbd082fdb22f0819fb4dea91d625e81471f8cfeb2cc8b42ad', 'datasetName': 'datasplits/7c7513bdfb4e5577fef1c3ec0fa6452d30d87dbc95f258a7c86dd76b', 'experimentHash': '45c7b416f10a0d0bbca735da8e631858ecca725e29dd5e322062d103', 'trialHash': '22f3c58fbd082fdb22f0819fb4dea91d625e81471f8cfeb2cc8b42ad', 'image_path': 'Curated4/Easy_50', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 1, 'fc_layers': 1, 'pretrained': True, 'epochs': 40, 'patience': -1, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_gamma': 0.5, 'scheduler_patience': 10, 'modelType': 'HGNN', 'lambda': 0, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': True, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.9, 'tripletEnabled': False, 'regularTripletLoss': False, 'tripletSamples': 2, 'tripletSelector': 'semihard', 'tripletMargin': 2, 'phylogeny_loss': False, 'displayName': 'Fish-s2-HGNN0.9', 'noSpeciesBackprop': False, 'phylogeny_loss_epsilon': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">22f3c58fbd082fdb22f0819fb4dea91d625e81471f8cfeb2cc8b42ad</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mndhamod/HGNN\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/3u10pi7v\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/3u10pi7v</a><br/>\n",
       "                Run data is saved locally in <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_142125-3u10pi7v</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:   0%|          | 0/40 [00:34<?, ?it/s, min_val_loss=inf, train=0.00251, val=0.00179, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [00:34<22:36, 34.77s/it, min_val_loss=inf, train=0.00251, val=0.00179, val_loss=3.64]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   2%|▎         | 1/40 [01:30<22:36, 34.77s/it, min_val_loss=inf, train=0.283, val=0.239, val_loss=3.56]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [01:30<26:00, 41.05s/it, min_val_loss=inf, train=0.283, val=0.239, val_loss=3.56]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   5%|▌         | 2/40 [02:28<26:00, 41.05s/it, min_val_loss=4.18, train=0.538, val=0.438, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [02:28<28:32, 46.27s/it, min_val_loss=4.18, train=0.538, val=0.438, val_loss=3.42]\u001b[A\u001b[A\n",
      "\n",
      "iteration:   8%|▊         | 3/40 [03:26<28:32, 46.27s/it, min_val_loss=2.29, train=0.831, val=0.675, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [03:26<29:43, 49.54s/it, min_val_loss=2.29, train=0.831, val=0.675, val_loss=3.36]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  10%|█         | 4/40 [04:24<29:43, 49.54s/it, min_val_loss=1.48, train=0.921, val=0.713, val_loss=3.3] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [04:24<30:27, 52.21s/it, min_val_loss=1.48, train=0.921, val=0.713, val_loss=3.3]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  12%|█▎        | 5/40 [05:23<30:27, 52.21s/it, min_val_loss=1.4, train=0.937, val=0.698, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [05:23<30:43, 54.23s/it, min_val_loss=1.4, train=0.937, val=0.698, val_loss=3.21]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  15%|█▌        | 6/40 [06:19<30:43, 54.23s/it, min_val_loss=1.4, train=0.985, val=0.757, val_loss=3.2] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [06:19<30:10, 54.85s/it, min_val_loss=1.4, train=0.985, val=0.757, val_loss=3.2]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  18%|█▊        | 7/40 [07:17<30:10, 54.85s/it, min_val_loss=1.32, train=0.996, val=0.792, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [07:17<29:41, 55.68s/it, min_val_loss=1.32, train=0.996, val=0.792, val_loss=3.11]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  20%|██        | 8/40 [08:14<29:41, 55.68s/it, min_val_loss=1.26, train=0.985, val=0.714, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [08:14<28:56, 56.01s/it, min_val_loss=1.26, train=0.985, val=0.714, val_loss=3.15]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  22%|██▎       | 9/40 [09:09<28:56, 56.01s/it, min_val_loss=1.26, train=0.994, val=0.788, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [09:09<27:51, 55.72s/it, min_val_loss=1.26, train=0.994, val=0.788, val_loss=3.08]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  25%|██▌       | 10/40 [10:05<27:51, 55.72s/it, min_val_loss=1.26, train=0.967, val=0.686, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [10:05<27:03, 55.98s/it, min_val_loss=1.26, train=0.967, val=0.686, val_loss=3.14]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  28%|██▊       | 11/40 [11:00<27:03, 55.98s/it, min_val_loss=1.26, train=0.998, val=0.815, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [11:00<25:57, 55.62s/it, min_val_loss=1.26, train=0.998, val=0.815, val_loss=3.06]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  30%|███       | 12/40 [11:59<25:57, 55.62s/it, min_val_loss=1.23, train=0.998, val=0.791, val_loss=3.02]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [11:59<25:25, 56.49s/it, min_val_loss=1.23, train=0.998, val=0.791, val_loss=3.02]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  32%|███▎      | 13/40 [12:54<25:25, 56.49s/it, min_val_loss=1.23, train=0.873, val=0.603, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [12:54<24:24, 56.31s/it, min_val_loss=1.23, train=0.873, val=0.603, val_loss=3.22]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  35%|███▌      | 14/40 [13:50<24:24, 56.31s/it, min_val_loss=1.23, train=0.806, val=0.547, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [13:50<23:24, 56.19s/it, min_val_loss=1.23, train=0.806, val=0.547, val_loss=3.28]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  38%|███▊      | 15/40 [14:45<23:24, 56.19s/it, min_val_loss=1.23, train=0.925, val=0.701, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [14:45<22:20, 55.85s/it, min_val_loss=1.23, train=0.925, val=0.701, val_loss=3.13]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  40%|████      | 16/40 [15:44<22:20, 55.85s/it, min_val_loss=1.23, train=0.852, val=0.611, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [15:44<21:46, 56.78s/it, min_val_loss=1.23, train=0.852, val=0.611, val_loss=3.17]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  42%|████▎     | 17/40 [16:44<21:46, 56.78s/it, min_val_loss=1.23, train=0.967, val=0.698, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [16:44<21:05, 57.51s/it, min_val_loss=1.23, train=0.967, val=0.698, val_loss=3.09]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  45%|████▌     | 18/40 [17:44<21:05, 57.51s/it, min_val_loss=1.23, train=0.985, val=0.765, val_loss=3.03]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [17:44<20:22, 58.23s/it, min_val_loss=1.23, train=0.985, val=0.765, val_loss=3.03]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  48%|████▊     | 19/40 [18:39<20:22, 58.23s/it, min_val_loss=1.23, train=0.967, val=0.705, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [18:39<19:09, 57.50s/it, min_val_loss=1.23, train=0.967, val=0.705, val_loss=3.07]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  50%|█████     | 20/40 [19:41<19:09, 57.50s/it, min_val_loss=1.23, train=0.999, val=0.808, val_loss=2.97]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [19:41<18:34, 58.64s/it, min_val_loss=1.23, train=0.999, val=0.808, val_loss=2.97]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  52%|█████▎    | 21/40 [20:41<18:34, 58.64s/it, min_val_loss=1.23, train=1, val=0.831, val_loss=2.92]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [20:41<17:42, 59.05s/it, min_val_loss=1.23, train=1, val=0.831, val_loss=2.92]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  55%|█████▌    | 22/40 [21:42<17:42, 59.05s/it, min_val_loss=1.2, train=1, val=0.849, val_loss=2.91] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [21:42<16:57, 59.85s/it, min_val_loss=1.2, train=1, val=0.849, val_loss=2.91]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  57%|█████▊    | 23/40 [22:48<16:57, 59.85s/it, min_val_loss=1.18, train=1, val=0.876, val_loss=2.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [22:48<16:26, 61.67s/it, min_val_loss=1.18, train=1, val=0.876, val_loss=2.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  60%|██████    | 24/40 [23:51<16:26, 61.67s/it, min_val_loss=1.14, train=1, val=0.867, val_loss=2.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [23:51<15:29, 61.98s/it, min_val_loss=1.14, train=1, val=0.867, val_loss=2.88]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  62%|██████▎   | 25/40 [24:51<15:29, 61.98s/it, min_val_loss=1.14, train=1, val=0.864, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [24:51<14:18, 61.29s/it, min_val_loss=1.14, train=1, val=0.864, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  65%|██████▌   | 26/40 [25:50<14:18, 61.29s/it, min_val_loss=1.14, train=0.999, val=0.852, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [25:50<13:09, 60.77s/it, min_val_loss=1.14, train=0.999, val=0.852, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  68%|██████▊   | 27/40 [26:49<13:09, 60.77s/it, min_val_loss=1.14, train=0.999, val=0.857, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [26:49<12:03, 60.32s/it, min_val_loss=1.14, train=0.999, val=0.857, val_loss=2.89]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  70%|███████   | 28/40 [27:48<12:03, 60.32s/it, min_val_loss=1.14, train=1, val=0.876, val_loss=2.87]    \u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 29/40 [27:48<10:57, 59.79s/it, min_val_loss=1.14, train=1, val=0.876, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  72%|███████▎  | 29/40 [28:50<10:57, 59.79s/it, min_val_loss=1.14, train=1, val=0.88, val_loss=2.87] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 30/40 [28:50<10:04, 60.40s/it, min_val_loss=1.14, train=1, val=0.88, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  75%|███████▌  | 30/40 [29:54<10:04, 60.40s/it, min_val_loss=1.14, train=1, val=0.87, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 31/40 [29:54<09:14, 61.58s/it, min_val_loss=1.14, train=1, val=0.87, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  78%|███████▊  | 31/40 [30:55<09:14, 61.58s/it, min_val_loss=1.14, train=1, val=0.877, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 32/40 [30:55<08:11, 61.46s/it, min_val_loss=1.14, train=1, val=0.877, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  80%|████████  | 32/40 [31:56<08:11, 61.46s/it, min_val_loss=1.14, train=1, val=0.867, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 33/40 [31:56<07:08, 61.21s/it, min_val_loss=1.14, train=1, val=0.867, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  82%|████████▎ | 33/40 [32:58<07:08, 61.21s/it, min_val_loss=1.14, train=1, val=0.864, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 34/40 [32:58<06:09, 61.56s/it, min_val_loss=1.14, train=1, val=0.864, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  85%|████████▌ | 34/40 [33:59<06:09, 61.56s/it, min_val_loss=1.14, train=1, val=0.86, val_loss=2.86] \u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 35/40 [33:59<05:05, 61.15s/it, min_val_loss=1.14, train=1, val=0.86, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  88%|████████▊ | 35/40 [35:01<05:05, 61.15s/it, min_val_loss=1.14, train=1, val=0.87, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 36/40 [35:01<04:06, 61.57s/it, min_val_loss=1.14, train=1, val=0.87, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  90%|█████████ | 36/40 [36:04<04:06, 61.57s/it, min_val_loss=1.14, train=1, val=0.87, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 37/40 [36:04<03:05, 61.82s/it, min_val_loss=1.14, train=1, val=0.87, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  92%|█████████▎| 37/40 [37:03<03:05, 61.82s/it, min_val_loss=1.14, train=1, val=0.883, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 38/40 [37:03<02:02, 61.03s/it, min_val_loss=1.14, train=1, val=0.883, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  95%|█████████▌| 38/40 [38:09<02:02, 61.03s/it, min_val_loss=1.13, train=1, val=0.867, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 39/40 [38:09<01:02, 62.55s/it, min_val_loss=1.13, train=1, val=0.867, val_loss=2.87]\u001b[A\u001b[A\n",
      "\n",
      "iteration:  98%|█████████▊| 39/40 [39:08<01:02, 62.55s/it, min_val_loss=1.13, train=1, val=0.884, val_loss=2.86]\u001b[A\u001b[A\n",
      "\n",
      "iteration: 100%|██████████| 40/40 [39:31<00:00, 59.30s/it, min_val_loss=1.13, train=1, val=0.884, val_loss=2.86]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 56013<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e9bc241e8d44bab37b5dc4587cf89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_142125-3u10pi7v/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/elhamod/projects/HGNN/code/HGNN/HGNN/train/wandb/run-20210526_142125-3u10pi7v/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>39</td></tr><tr><td>learning rate</td><td>0.001</td></tr><tr><td>validation_fine_f1</td><td>0.88384</td></tr><tr><td>training_fine_f1</td><td>1.0</td></tr><tr><td>training_fine_acc</td><td>1.0</td></tr><tr><td>test_fine_f1</td><td>0.89605</td></tr><tr><td>test_fine_acc</td><td>0.89737</td></tr><tr><td>validation_loss</td><td>2.85822</td></tr><tr><td>_runtime</td><td>2352</td></tr><tr><td>_timestamp</td><td>1622055637</td></tr><tr><td>_step</td><td>780</td></tr><tr><td>loss</td><td>0.00113</td></tr><tr><td>batch</td><td>759</td></tr><tr><td>loss_fine</td><td>0.00112</td></tr><tr><td>lambda_fine</td><td>0.95001</td></tr><tr><td>loss_coarse</td><td>0.00133</td></tr><tr><td>lambda_coarse</td><td>0.04999</td></tr><tr><td>train_fine_acc</td><td>0.89737</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>learning rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_fine_f1</td><td>▁▃▄▆▇▇▇▇▇▇▆▇▇▆▅▇▆▇▇▇▇███████████████████</td></tr><tr><td>training_fine_f1</td><td>▁▃▅▇▇████████▇▇▇▇███████████████████████</td></tr><tr><td>training_fine_acc</td><td>▁▃▅▇▇████████▇▇█▇███████████████████████</td></tr><tr><td>test_fine_f1</td><td>▁▃▄▆▆▆▇▇▇▇▇▇▇▅▅▆▆▇▇▇▇███████████████████</td></tr><tr><td>test_fine_acc</td><td>▁▃▅▆▆▆▇▇▇▇▇▇▇▅▅▆▆▇▇▇▇███████████████████</td></tr><tr><td>validation_loss</td><td>█▇▆▅▅▄▄▃▄▃▄▃▂▄▅▃▄▃▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss_fine</td><td>█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_fine</td><td>▃▁▃▄▅▇▇██████▇▇████▇████████████████████</td></tr><tr><td>loss_coarse</td><td>█▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lambda_coarse</td><td>▆█▆▅▄▂▂▁▁▁▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">22f3c58fbd082fdb22f0819fb4dea91d625e81471f8cfeb2cc8b42ad</strong>: <a href=\"https://wandb.ai/mndhamod/HGNN/runs/3u10pi7v\" target=\"_blank\">https://wandb.ai/mndhamod/HGNN/runs/3u10pi7v</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment: 100%|██████████| 10/10 [2:04:50<00:00, 749.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    print('wandb not found')\n",
    "\n",
    "from myhelpers import config_plots, TrialStatistics\n",
    "from myhelpers.try_warning import try_running\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers import cifar_dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "config_plots.global_settings()\n",
    "\n",
    "experimetnsFileName = \"experiments.csv\"\n",
    "WANDB_message=\"wandb not working\"\n",
    "\n",
    "\n",
    "# For logging to server\n",
    "try_running(lambda : wandb.login(), WANDB_message)\n",
    "\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "# set cuda\n",
    "if device is not None:\n",
    "    print(\"using cuda\", device)\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "else:\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# get experiment params\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "\n",
    "# init experiments file\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "\n",
    "paramsIterator = config_parser.getExperiments()  \n",
    "number_of_experiments = sum(1 for e in paramsIterator)  \n",
    "experiment_index = 0\n",
    "\n",
    "# Loop through experiments\n",
    "# with progressbar.ProgressBar(max_value=number_of_experiments) as bar:\n",
    "with tqdm(total=number_of_experiments, desc=\"experiment\") as bar:\n",
    "    for experiment_params in config_parser.getExperiments():\n",
    "        print(experiment_params)\n",
    "        experimentHash =TrialStatistics.getTrialName(experiment_params)\n",
    "\n",
    "        # load images \n",
    "        if experiment_params['image_path'] == 'cifar-100-python':\n",
    "            datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        else:\n",
    "            datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "        datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "        train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "        architecture = {\n",
    "            \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "            \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "        }\n",
    "\n",
    "        # Loop through n trials\n",
    "        for i in trange(experiment_params[\"numOfTrials\"], desc=\"trial\"):\n",
    "            modelName = getModelName(experiment_params, i)\n",
    "            trialName = os.path.join(experimentPathAndName, modelName)\n",
    "            trialHash = TrialStatistics.getTrialName(experiment_params, i)\n",
    "\n",
    "            row_information = {\n",
    "                'experimentName': experimentName,\n",
    "                'modelName': modelName,\n",
    "                'datasetName': getDatasetName(config_parser.fixPaths(experiment_params)),\n",
    "                'experimentHash': experimentHash,\n",
    "                'trialHash': trialHash\n",
    "            }\n",
    "            row_information = {**row_information, **experiment_params} \n",
    "            print(row_information)\n",
    "\n",
    "            run = try_running(lambda : wandb.init(project='HGNN', group=experimentName+\"-\"+experimentHash, name=trialHash, config=row_information), WANDB_message) #, reinit=True\n",
    "\n",
    "            # Train/Load model\n",
    "            model = CNN.create_model(architecture, experiment_params, device=device)\n",
    "            \n",
    "#             from torchsummary import summary\n",
    "\n",
    "#             summary(model, (3, 224, 224))\n",
    "\n",
    "#             try_running(lambda : wandb.watch(model, log=\"all\"), WANDB_message)\n",
    "\n",
    "            if os.path.exists(CNN.getModelFile(trialName)):\n",
    "                print(\"Model {0} found!\".format(trialName))\n",
    "            else:\n",
    "                initModelPath = CNN.getInitModelFile(experimentPathAndName)\n",
    "                if os.path.exists(initModelPath):\n",
    "                    model.load_state_dict(torch.load(initModelPath))\n",
    "                    print(\"Init Model {0} found!\".format(initModelPath))\n",
    "                CNN.trainModel(train_loader, validation_loader, experiment_params, model, trialName, test_loader, device=device, detailed_reporting=detailed_reporting)\n",
    "\n",
    "            # Add to experiments file\n",
    "            if os.path.exists(experimentsFileNameAndPath):\n",
    "                experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "            else:\n",
    "                experiments_df = pd.DataFrame()\n",
    "\n",
    "            record_exists = not (experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName]).empty if not experiments_df.empty else False\n",
    "            if record_exists:\n",
    "                experiments_df.drop(experiments_df[experiments_df['modelName'] == modelName][experiments_df['experimentName'] == experimentName].index, inplace = True) \n",
    "\n",
    "            experiments_df = experiments_df.append(pd.DataFrame(row_information, index=[0]), ignore_index = True)\n",
    "            experiments_df.to_csv(experimentsFileNameAndPath, header=True, index=False)\n",
    "\n",
    "            try_running(lambda : run.finish(), WANDB_message)\n",
    "\n",
    "        bar.update()\n",
    "\n",
    "        experiment_index = experiment_index + 1\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     torch.multiprocessing.set_start_method('spawn')\n",
    "    \n",
    "#     import argparse\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--cuda', required=True, type=int)\n",
    "#     parser.add_argument('--experiments', required=True)\n",
    "#     parser.add_argument('--data', required=True)\n",
    "#     parser.add_argument('--name', required=True)\n",
    "#     parser.add_argument('--detailed', required=False, action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "#     main(experimentName=args.name, experimentsPath=args.experiments, dataPath=args.data, device=args.cuda, detailed_reporting=args.detailed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
