{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_landscapes\n",
    "import torch\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from HGNN.train.configParser import ConfigParser\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 6\n"
     ]
    }
   ],
   "source": [
    "cuda=6\n",
    "\n",
    "# set cuda\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(cuda)\n",
    "    print(\"using cuda\", cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_params = {\n",
    "#             \"image_path\": \"INHS_cropped\",\n",
    "#             \"suffix\": \"biology_paper_cleaned_200max\",\n",
    "#             \"img_res\": 448,\n",
    "#             \"augmented\": True,\n",
    "#             \"batchSize\": 128,\n",
    "#             \"learning_rate\": 0.1,\n",
    "#             \"numOfTrials\": 1,\n",
    "#             \"fc_layers\": 1,\n",
    "#             \"modelType\": \"BB\",\n",
    "#             \"lambda\": 0.01,\n",
    "#             \"unsupervisedOnTest\": False,\n",
    "#             \"tl_model\": \"ResNet18\",\n",
    "#             \"link_layer\": \"avgpool\"\n",
    "#         }\n",
    "# model_path = \"/home/elhamod/HGNN/experiments/biology_paper_surfaceExperiments_BB_lr_largeBatch/models/8d50d7c9daa5d26371050c2e66f8cb6fb23a94d007db9dbc8bc48770\"\n",
    "# experimentsPath = \"/home/elhamod/HGNN/experiments/\" \n",
    "# experimentName = \"biology_paper_surfaceExperiments_BB_lr_largeBatch\"\n",
    "# dataPath = \"/data/BGNN_data\"\n",
    "\n",
    "\n",
    "# coord_Tracker_path = \"/home/elhamod/HGNN/coordinates/biology_paper_surfaceExperiments_BB_lr_largeBatch_01\"\n",
    "\n",
    "experiment_params = {\"image_path\": \"INHS_cropped\", \"suffix\": \"biology_paper_cleaned_15max\", \"img_res\": 448, \"augmented\": True, \"batchSize\": 64, \"learning_rate\": 0.0001, \"numOfTrials\": 1, \"fc_layers\": 1, \"modelType\": \"HGNN_add\", \"lambda\": 0.01, \"unsupervisedOnTest\": False, \"tl_model\": \"ResNet18\", \"link_layer\": \"avgpool\"}\n",
    "model_path = \"/home/elhamod/HGNN/experiments/biology_paper_surfaceExperiments_HGNN_datasets/models/af109a6b3598cfb56a4f87b4ab1167d63ec855654e268e5ebb3f2593\"\n",
    "experimentsPath = \"/home/elhamod/HGNN/experiments/\" \n",
    "experimentName = \"biology_paper_surfaceExperiments_HGNN_datasets\"\n",
    "dataPath = \"/data/BGNN_data\"\n",
    "\n",
    "\n",
    "coord_Tracker_path = \"/home/elhamod/HGNN/coordinates/testing3\"\n",
    "\n",
    "normalize = 'filter'\n",
    "Do_pca = True\n",
    "steps = 25\n",
    "# grid_batchSize = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38  models added\n"
     ]
    }
   ],
   "source": [
    "model_paths = []\n",
    "\n",
    "i=0\n",
    "while(True):\n",
    "    p = os.path.join(model_path, \"iterations\", \"iteration{0}.pt\".format(i))\n",
    "    if os.path.exists(p):\n",
    "        model_paths.append(p)\n",
    "        i = i+1\n",
    "    else:\n",
    "        model_paths.append(os.path.join(model_path, \"iterations\", \"finalModel.pt\"))\n",
    "        print(i, \" models added\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning files: 100%|██████████| 570/570 [00:02<00:00, 275.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets... Done.\n",
      "Creating loaders...\n",
      "Creating loaders... Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "train_loader, validation_loader, test_loader = datasetManager.getLoaders()\n",
    "architecture = {\n",
    "    \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "    \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cac90e5615444783855bfe63d00b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for model_path in tqdm(model_paths):\n",
    "    model = CNN.create_model(architecture, experiment_params)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_path))) # , map_location=torch.device('cpu')\n",
    "    model.eval()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapped_model_params = []\n",
    "# for i in models:\n",
    "#     wrapped_model_params.append(loss_landscapes.wrap_model(i).get_module_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = loss_landscapes.get_best_fit_orthogonal_bases_pca(wrapped_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.as_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.as_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "class BGNN_loss(loss_landscapes.Metric):\n",
    "    def __init__(self, loader, lambda_, isDSN):\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.isDSN = isDSN\n",
    "        self.lambda_ = lambda_\n",
    "        self.loader = loader\n",
    "        \n",
    "        self.inputs = []\n",
    "        self.target = []\n",
    "        self.meta_target = []\n",
    "        self.index = -1\n",
    "        \n",
    "    def get_next(self):\n",
    "        if len(self.inputs) == 0:\n",
    "            for batch in self.loader:\n",
    "                self.inputs.append(batch[\"image\"])\n",
    "                self.target.append(batch[\"fine\"])\n",
    "                self.meta_target.append(batch[\"coarse\"])\n",
    "        self.index = self.index+1\n",
    "        if self.index==len(self.inputs):\n",
    "            self.index=-1\n",
    "            return None, None, None\n",
    "        return self.inputs[self.index], self.target[self.index], self.meta_target[self.index]\n",
    "\n",
    "    def __call__(self, model_wrapper) -> float:\n",
    "#         s = time.time()\n",
    "        \n",
    "        inputs, target, meta_target = self.get_next()\n",
    "        while inputs is not None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.criterion = self.criterion.cuda()\n",
    "                inputs = inputs.cuda()\n",
    "                target = target.cuda()\n",
    "                meta_target = meta_target.cuda()\n",
    "            \n",
    "            batch_size = inputs.shape[0]\n",
    "                \n",
    "            output = model_wrapper.forward(inputs)\n",
    "            print(output)\n",
    "            \n",
    "            loss_coarse = 0\n",
    "            if output[\"coarse\"] is not None:\n",
    "                loss_coarse = self.criterion(output[\"coarse\"], meta_target if not self.isDSN else target)\n",
    "            loss_fine = self.criterion(output[\"fine\"], target)\n",
    "            loss = loss_fine + self.lambda_*loss_coarse\n",
    "            loss = batch_size*loss\n",
    "            \n",
    "            inputs, target, meta_target = self.get_next()\n",
    "            \n",
    "#             print(time.time()-s)\n",
    "#             s = time.time()\n",
    "\n",
    "        print(len(self.loader.dataset))\n",
    "        loss = loss/len(self.loader.dataset)\n",
    "        loss = loss.detach()\n",
    "        print(loss)\n",
    "        print(\"--\")\n",
    "        if torch.cuda.is_available():\n",
    "            loss = loss.cpu()\n",
    "        \n",
    "        return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class BGNN_loss3(loss_landscapes.Metric):\n",
    "    def __init__(self, loader, lambda_, isDSN):\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.isDSN = isDSN\n",
    "        self.lambda_ = lambda_\n",
    "        self.loader = loader\n",
    "        self.data_iter = iter(loader)\n",
    "\n",
    "    def __call__(self, model_wrapper) -> float:\n",
    "        s = time.time()\n",
    "        \n",
    "        next_batch = self.data_iter.next() # start loading the first batch\n",
    "#         print(next_batch)\n",
    "#         next_batch = [ _.cuda(non_blocking=True) for _ in next_batch ]  # with pin_memory=True and non_blocking=True, this will copy data to GPU non blockingly\n",
    "\n",
    "        for i in range(len(self.loader)):\n",
    "            batch = next_batch \n",
    "            if i + 1 != len(self.loader): \n",
    "                # start copying data of next batch\n",
    "                next_batch = self.data_iter.next()\n",
    "#                 next_batch = [ _.cuda(async=True) for _ in next_batch]\n",
    "\n",
    "            inputs = batch[\"image\"]\n",
    "            target = batch[\"fine\"]\n",
    "            meta_target = batch[\"coarse\"]\n",
    "            if torch.cuda.is_available():\n",
    "                self.criterion = self.criterion.cuda()\n",
    "                inputs = inputs.cuda()\n",
    "                target = target.cuda()\n",
    "                meta_target = meta_target.cuda()\n",
    "            \n",
    "            batch_size = inputs.shape[0]\n",
    "                \n",
    "            \n",
    "            output = model_wrapper.forward(inputs)\n",
    "            \n",
    "            \n",
    "            loss_coarse = 0\n",
    "            if output[\"coarse\"] is not None:\n",
    "                loss_coarse = self.criterion(output[\"coarse\"], meta_target if not self.isDSN else target)\n",
    "            loss_fine = self.criterion(output[\"fine\"], target)\n",
    "            loss = loss_fine + self.lambda_*loss_coarse\n",
    "            loss = batch_size*loss\n",
    "            \n",
    "            print(time.time()-s)\n",
    "            s = time.time()\n",
    "\n",
    "        loss = loss/len(self.loader.dataset)\n",
    "        loss = loss.detach()\n",
    "        if torch.cuda.is_available():\n",
    "            loss = loss.cpu()\n",
    "        \n",
    "        return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGNN_loss2(loss_landscapes.Loss):\n",
    "    def __init__(self, inputs, target, meta_target, lambda_, isDSN):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        if torch.cuda.is_available():\n",
    "            criterion = criterion.cuda()\n",
    "            inputs = inputs.cuda()\n",
    "            target = target.cuda()\n",
    "            meta_target = meta_target.cuda()\n",
    "        super().__init__(criterion, inputs, target)\n",
    "        self.isDSN = isDSN\n",
    "        self.lambda_ = lambda_\n",
    "        self.meta_target = meta_target\n",
    "\n",
    "    def __call__(self, model_wrapper) -> float:\n",
    "        output = model_wrapper.forward(self.inputs)\n",
    "        \n",
    "        loss_coarse = 0\n",
    "        if output[\"coarse\"] is not None:\n",
    "            loss_coarse = self.loss_fn(output[\"coarse\"], self.meta_target if not self.isDSN else self.target)\n",
    "        loss_fine = self.loss_fn(output[\"fine\"], self.target)\n",
    "        loss = loss_fine + self.lambda_*loss_coarse\n",
    "        \n",
    "        loss = loss.detach()\n",
    "        if torch.cuda.is_available():\n",
    "            loss = loss.cpu()\n",
    "        \n",
    "        return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_batchSize = len(test_loader.dataset)\n",
    "\n",
    "# num_of_workers = 8\n",
    "# test_loader = torch.utils.data.DataLoader(test_loader.dataset, batch_size=grid_batchSize, num_workers=num_of_workers, pin_memory=True)\n",
    "# batch = next(iter(test_loader))\n",
    "# inputs = batch[\"image\"]\n",
    "# target = batch[\"fine\"]\n",
    "# meta_target = batch[\"coarse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_loader.dataset, batch_size=len(test_loader.dataset))\n",
    "batch = next(iter(test_loader))\n",
    "inputs = batch[\"image\"]\n",
    "target = batch[\"fine\"]\n",
    "meta_target = batch[\"coarse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = BGNN_loss3(test_loader, experiment_params[\"lambda\"], False) #TODO: need to automate these params\n",
    "# metric = BGNN_loss(test_loader, experiment_params[\"lambda\"], False) #TODO: need to automate these params\n",
    "metric = BGNN_loss2(inputs, target, meta_target, experiment_params[\"lambda\"], False) #TODO: need to automate these params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_Tracker = loss_landscapes.Coordinates_tracker(path=coord_Tracker_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(coord_Tracker.dirs_[0].model_norm() )\n",
    "# print(coord_Tracker.scaled_dirs_[0].model_norm() )\n",
    "# print(loss_landscapes.wrap_model(models[-1]).get_module_parameters().model_norm())\n",
    "# print(coord_Tracker.dist_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for l in range(len(coord_Tracker.dirs_[0].parameters)):\n",
    "#     print(coord_Tracker.dirs_[0].parameters[l].shape)\n",
    "#     print(len(coord_Tracker.dirs_[0].parameters[l].size()))\n",
    "#     print(len(coord_Tracker.dirs_[0].parameters[l]))\n",
    "#     # normalize one-dimensional bias vectors\n",
    "#     if len(coord_Tracker.dirs_[0].parameters[l].size()) == 1:\n",
    "#         print('1-')\n",
    "# #         self.parameters[l] *= (ref_point.parameters[l].norm(order) / (self.parameters[l].norm(order)+ sys.float_info.epsilon))\n",
    "#     # normalize two-dimensional weight vectors\n",
    "#     for f in range(len(coord_Tracker.dirs_[0].parameters[l])):\n",
    "#         print('2-', f)\n",
    "# #         self.parameters[l][f] *= ref_point.filter_norm((l, f), order) / (self.filter_norm((l, f), order)+ sys.float_info.epsilon)\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA time:  57.033881425857544\n",
      "1.4936075e-08\n",
      "5.224507808685303\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "if Do_pca:\n",
    "    if not coord_Tracker.load():\n",
    "        coord_Tracker.update_directions(loss_landscapes.get_best_fit_orthogonal_bases_pca(models))\n",
    "        coord_Tracker.update_distance(loss_landscapes.get_optimal_distance(\n",
    "            models[:-1], \n",
    "            models[-1]))\n",
    "        coord_Tracker.save()\n",
    "        print(coord_Tracker.dist_)\n",
    "        print(\"saved\")\n",
    "    else:\n",
    "        print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.17232332e-05 -1.45999475e-05 -1.87032438e-05 ... -1.36305497e-03\n",
      " -1.27225055e-03 -1.44457351e-03]\n",
      "[-4.17232332e-05 -1.45999475e-05 -1.87032438e-05 ... -1.36305497e-03\n",
      " -1.27225055e-03 -1.44457351e-03]\n"
     ]
    }
   ],
   "source": [
    "# a = coord_Tracker.dirs_[0].as_numpy()\n",
    "# print(a)\n",
    "# b = loss_landscapes.numpy_to_ModelParameters(a, coord_Tracker.dirs_[0])\n",
    "# print(b.as_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-673a31ebc668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     print(\"loaded\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mloss_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_landscapes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_plane\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoord_Tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoord_Tracker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoord_Tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_data_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/loss-landscapes/loss_landscapes/main.py\u001b[0m in \u001b[0;36mrandom_plane\u001b[0;34m(model, metric, distance, steps, normalization, deepcopy_model, coord_Tracker)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mdir_two\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_normalize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'filter'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mdir_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_normalize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mdir_two\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_normalize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/loss-landscapes/loss_landscapes/model_interface/model_parameters.py\u001b[0m in \u001b[0;36mfilter_normalize_\u001b[0;34m(self, ref_point, order)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;31m# normalize two-dimensional weight vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mref_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmodel_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "loss_data_file = os.path.join(coord_Tracker_path, \"loss_surface_data\")\n",
    "# try:\n",
    "#     loss_data = np.load(loss_data_file)\n",
    "#     print(\"loaded\")\n",
    "# except:\n",
    "loss_data = loss_landscapes.random_plane(models[-1], metric, steps=steps, distance=coord_Tracker.dist_, normalization=normalize, deepcopy_model=True, coord_Tracker=coord_Tracker)\n",
    "np.save(loss_data_file, loss_data)\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric(loss_landscapes.wrap_model(models[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loss_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_model(model_params, center_model_params, dirs_):\n",
    "    dir_one = dirs_[0]\n",
    "    dir_two = dirs_[1]\n",
    "\n",
    "    model_params_one = model_params.dot(dir_one)\n",
    "    model_params_two = model_params.dot(dir_two)\n",
    "    \n",
    "    center_model_params_one = center_model_params.dot(dir_one)\n",
    "    center_model_params_two = center_model_params.dot(dir_two)\n",
    "\n",
    "    return model_params_one - center_model_params_one, model_params_two - center_model_params_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from myhelpers import config_plots\n",
    "config_plots.global_settings()\n",
    "\n",
    "num_rows=1\n",
    "num_columns=1\n",
    "STEPS = coord_Tracker.steps_\n",
    "DISTANCE = coord_Tracker.dist_\n",
    "\n",
    "f, axes = plt.subplots(num_rows, num_columns, figsize=(40, 15), dpi= 300,)\n",
    "\n",
    "X = np.array([(k - int(STEPS/2))*DISTANCE/STEPS for k in range(STEPS)])\n",
    "Y = X\n",
    "loss_data_fin = loss_data\n",
    "\n",
    "ax = plt.subplot(num_rows, num_columns, 1)\n",
    "contours = plt.contour(X, Y, loss_data_fin, 15, colors='black') # \n",
    "plt.clabel(contours, inline=True, fontsize=10)\n",
    "plt.imshow(loss_data_fin, extent=[X[0], X[-1], Y[0], Y[-1]], origin='lower') # , cmap='RdGy', alpha=0.5\n",
    "maxVal = np.amax([loss_data_fin])\n",
    "minVal = np.amin([loss_data_fin])\n",
    "plt.pcolor(X, Y, loss_data_fin, vmin=minVal, vmax=maxVal, cmap='Reds')\n",
    "plt.colorbar()\n",
    "\n",
    "ax.title.set_text('surface')\n",
    "plt.plot([0], [0], marker='o', markersize=4, color=\"blue\")\n",
    "\n",
    "wm_center = loss_landscapes.wrap_model(models[-1]).get_module_parameters()\n",
    "\n",
    "# for i in range(len(models)):\n",
    "#     wm = loss_landscapes.wrap_model(models[i]).get_module_parameters()\n",
    "#     scaled_model = scale_model(wm, wm_center, coord_Tracker.scaled_dirs_)\n",
    "#     print(scaled_model)\n",
    "#     plt.plot(scaled_model[0], scaled_model[1], marker='o', markersize=4, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.savefig(os.path.join(coord_Tracker_path, \"surface_fig.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
