{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms as torchvision_transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import ntpath\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# from HGNN.train import CNN, dataLoader\n",
    "from HGNN.train.configParser import ConfigParser, getModelName, getDatasetName\n",
    "from myhelpers import config_plots\n",
    "from HGNN.train import CNN, dataLoader\n",
    "\n",
    "# config_plots.global_settings()\n",
    "\n",
    "experimetnsFileName = \"experiments.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "experimentsPath=\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath=\"/raid/elhamod/Fish\"\n",
    "experimentName=\"Fish30-5run-PhyloNN6\" #\"Fish50_30-5run-BB-HGNN-crossvalidation\" \n",
    "trial_hash=\"0e8bc6eb6edfb88c5a419e14ab0b445d72ee1945bd474a26a7abcbd4\" #PhyloNN\n",
    "#\"9d6646b1d44b3034255f21a9d658fffe2f80e4f2180745e169abeb72\" #HGNN\n",
    "\n",
    "# image file:\n",
    "# fileName= '/raid/elhamod/Fish/Curated4/Easy_50/test/Notropis nubilus/INHS_FISH_81913.jpg'\n",
    "fileName= '/raid/elhamod/Fish/Curated4/Easy_30/test/Carassius auratus/INHS_FISH_4916.jpg'\n",
    "\n",
    "# MISC\n",
    "cuda=1\n",
    "SEED_INT=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project=\"Fish_activationMaximization_hyperp2\"\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'early_terminate': {\n",
    "       'type': 'hyperband',\n",
    "       'min_iter': 8   \n",
    "    }\n",
    "}\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "COUNT=1000\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"useRegularization\":{'values': [True, False]},\n",
    "    \"useRandomInitImage\":{'values': [True, False]},\n",
    "    \"L1_reg\":{'values': [True, False]},\n",
    "    \"iterations\":{\n",
    "        'distribution': 'q_uniform',\n",
    "        'min': 10,\n",
    "        'max': 10000,\n",
    "        'q':10\n",
    "    },\n",
    "    \"weight_decay\":{\n",
    "        'distribution': 'log_uniform',\n",
    "        'min': math.log(0.00001),\n",
    "        'max': math.log(10),\n",
    "    },\n",
    "    \"learning_rate\":{\n",
    "        'distribution': 'log_uniform',\n",
    "        'min': math.log(0.00001),\n",
    "        'max': math.log(10),\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda\n",
    "if cuda is not None:\n",
    "    print(\"using cuda\", cuda)\n",
    "    torch.cuda.set_device(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment parameters\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "if os.path.exists(experimentsFileNameAndPath):\n",
    "    experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "else:\n",
    "    raise Exception(\"Experiment not \" + trial_hash + \" found!\")\n",
    "experimentRecord = experiments_df[experiments_df[\"trialHash\"] == trial_hash]\n",
    "modelName = experimentRecord.iloc[0][\"modelName\"]\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "trialName = os.path.join(experimentPathAndName, modelName)\n",
    "experiment_params = experimentRecord.to_dict('records')[0]\n",
    "experiment_params = config_parser.fixExperimentParams(experiment_params)\n",
    "\n",
    "if math.isnan(experiment_params['suffix']):\n",
    "    experiment_params['suffix'] = None\n",
    "print(experiment_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetManager.updateParams(config_parser.fixPaths(experiment_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_loader, validation_loader, test_loader = datasetManager.getLoaders(SEED_INT)\n",
    "# architecture = {\n",
    "#     \"fine\": len(train_loader.dataset.csv_processor.getFineList()),\n",
    "#     \"coarse\" : len(train_loader.dataset.csv_processor.getCoarseList())\n",
    "# }\n",
    "architecture = CNN.get_architecture(experiment_params, train_loader.dataset.csv_processor)\n",
    "model = CNN.create_model(architecture, experiment_params, device=cuda)\n",
    "CNN.loadModel(model, trialName, device=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransformedImage(dataset, img, augmentation, normalization):\n",
    "    augmentation2, normalization2, pad2 = dataset.toggle_image_loading(augmentation=augmentation, normalization=normalization)\n",
    "    transforms = dataset.getTransforms()\n",
    "    composedTransforms = torchvision_transforms.Compose(transforms)\n",
    "    img_clone = composedTransforms(img)\n",
    "#     print(img_clone.shape)\n",
    "#     img_clone = img_clone.unsqueeze(0)\n",
    "#     print(img_clone.shape)\n",
    "    dataset.toggle_image_loading(augmentation2, normalization2, pad2)\n",
    "    return img_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(1, 2)\n",
    "\n",
    "\n",
    "title = ntpath.basename(fileName)\n",
    "original =  Image.open(fileName)\n",
    "image_non_normalized = getTransformedImage(test_loader.dataset, original, False, False)\n",
    "image_normalized = getTransformedImage(test_loader.dataset, original, False, True)\n",
    "\n",
    "axarr[0].imshow(np.transpose(image_non_normalized.detach().numpy(), (1, 2, 0)))\n",
    "axarr[1].imshow(np.transpose(image_normalized.detach().numpy(), (1, 2, 0)))\n",
    "axarr[0].axis('off')\n",
    "axarr[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "img = image_normalized.unsqueeze(0)\n",
    "if cuda is not None:\n",
    "    img = img.cuda()\n",
    "output = model(img)\n",
    "output_class = torch.argmax(output['fine'].squeeze())\n",
    "im = transforms.ToPILImage()(image_non_normalized).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchVisualizations.generate_class_specific_samples import ClassSpecificImageGeneration\n",
    "from pytorchVisualizations.generate_regularized_class_specific_samples import RegularizedClassSpecificImageGeneration\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(config=None):\n",
    "    run = wandb.init()\n",
    "\n",
    "    experiment_params=wandb.config if config is None else config\n",
    "    experiment_params = dict(experiment_params)\n",
    "    row_information = {\n",
    "        \"experimentsPath\":experimentsPath,\n",
    "        \"dataPath\":dataPath,\n",
    "        \"experimentName\":experimentName,\n",
    "        \"trial_hash\":trial_hash,\n",
    "        \"fileName\": fileName\n",
    "    }\n",
    "    row_information = {**row_information, **experiment_params} \n",
    "    print(row_information)\n",
    "\n",
    "    useRandomInitImage = experiment_params[\"useRandomInitImage\"]\n",
    "    iterations = experiment_params[\"iterations\"]\n",
    "    learning_rate = experiment_params[\"learning_rate\"]\n",
    "    weight_decay = experiment_params[\"weight_decay\"]\n",
    "    L1_reg = experiment_params[\"L1_reg\"]\n",
    "    img = image_non_normalized.unsqueeze(0) if not useRandomInitImage else None\n",
    "    if not experiment_params[\"useRegularization\"]:\n",
    "        csig = ClassSpecificImageGeneration(model, output_class.item(), img, cuda=cuda, normalizer=test_loader.dataset.normalizer)\n",
    "        im_generated, loss  = csig.generate(iterations=iterations, initial_learning_rate=learning_rate)\n",
    "    else:\n",
    "        csig = RegularizedClassSpecificImageGeneration(model, output_class.item(), img, cuda=cuda, normalizer=test_loader.dataset.normalizer)\n",
    "        im_generated, loss = csig.generate(iterations=iterations, initial_learning_rate=learning_rate, wd=weight_decay, L1_reg=L1_reg)\n",
    "    im_generated_normalized = test_loader.dataset.normalizer(im_generated)\n",
    "    \n",
    "    plt_img = im_generated.squeeze().permute(1,2,0).detach().numpy()\n",
    "    plt_img = (plt_img*255).astype(np.uint8)\n",
    "    plt_img = Image.fromarray(plt_img)\n",
    "    dist = torch.nn.Softmax()(model(im_generated_normalized.unsqueeze(0))['fine'])[0].tolist()\n",
    "    dist = list(map(lambda x: [x[0], x[1]], enumerate(dist)))\n",
    "    dist = wandb.Table(data=dist, columns=[\"class\", \"probability\"])\n",
    "    wandb.log({\"loss\": loss, \"distribution\": wandb.plot.bar(dist, 'class', 'probability'), \"output\": wandb.Image(plt_img)})\n",
    "    \n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=project)\n",
    "wandb.agent(sweep_id, function=train, count=COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
