{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from myhelpers import cifar_dataLoader\n",
    "\n",
    "from HGNN.train.configParser import ConfigParser\n",
    "from HGNN.train import CNN, dataLoader\n",
    "from myhelpers.memory import get_cuda_memory\n",
    "from myhelpers.image_show import showExample, getClosestImageFromDataloader\n",
    "from myhelpers.images_tsne import get_tsne\n",
    "from misc import get_classification_df\n",
    "\n",
    "experimetnsFileName = \"experiments.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsPath=\"/raid/elhamod/Fish/experiments/\"\n",
    "dataPath=\"/raid/elhamod/Fish/\"\n",
    "experimentName=\"Fish30-5run-PhyloNN6\"#\"Fish-L1-experiment2\"#\"CIFAR_phylogeny_HGNN_lambdaExperiment\"\n",
    "# trial_hash=\"27a4da8bf0c3bca7096cddf192ecf3069e035ad3fbba69bbd47bd157\" #MSE + L1\n",
    "#\"5922bad3c69f629daa0af24121e292a895f308e7cae3ec4f64536186\"#hier\n",
    "trial_hash=\"0e8bc6eb6edfb88c5a419e14ab0b445d72ee1945bd474a26a7abcbd4\" #PhyloNN\n",
    "SEED_INT=4 # order of the trial_hash when executed\n",
    "\n",
    "legends=['fine', 'coarse']\n",
    "use_submodel=False\n",
    "dataset_name=\"test\"\n",
    "cuda=0\n",
    "batch_size = 4\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda 0\n"
     ]
    }
   ],
   "source": [
    "# set cuda\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(cuda)\n",
    "    print(\"using cuda\", cuda)\n",
    "    \n",
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experimentName': 'Fish30-5run-PhyloNN6', 'modelName': 'models/0e8bc6eb6edfb88c5a419e14ab0b445d72ee1945bd474a26a7abcbd4', 'datasetName': 'datasplits/a30fade0855f8d4a9e28fdac4e65ae71ab21444a323ee6e3927d97f8', 'experimentHash': '2f4afd9af84811c61585f52e9e69fb2b8fc8e83ae71fa846e602b672', 'trialHash': '0e8bc6eb6edfb88c5a419e14ab0b445d72ee1945bd474a26a7abcbd4', 'image_path': 'Curated4/Easy_30', 'suffix': None, 'img_res': 448, 'augmented': True, 'batchSize': 64, 'learning_rate': 0.001, 'numOfTrials': 5, 'fc_layers': 1, 'modelType': 'PhyloNN', 'lambda': 1.0, 'unsupervisedOnTest': None, 'tl_model': 'ResNet18', 'link_layer': 'avgpool', 'adaptive_smoothing': False, 'adaptive_lambda': 0.1, 'adaptive_alpha': 0.5, 'noSpeciesBackprop': False, 'phylogeny_loss': 'False', 'phylogeny_loss_epsilon': 0.03, 'tripletEnabled': False, 'tripletSamples': 2.0, 'tripletSelector': 'semihard', 'tripletMargin': 2.0, 'displayName': 'Fish30-5run-PhyloNN-smalldistances-intraKorthogonality', 'pretrained': True, 'epochs': 120.0, 'patience': -1.0, 'optimizer': 'adabelief', 'scheduler': 'plateau', 'weightdecay': 0.01, 'scheduler_patience': 15.0, 'scheduler_gamma': 0.5, 'cosine_patience': None, 'regularTripletLoss': False, 'triplet_layers_dic': 'layer2,layer4', 'two_phase_lambda': False, 'L1reg': False, 'grayscale': False, 'tl_extralayer': False, 'random_fitting': False, 'useCrossValidation': True, 'phyloDistances': '0.5,0.3,0.1', 'addKernelOrthogonality': True, 'useImbalancedSampling': None, 'useImbalancedCriterion': None}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-29e2f0e3308c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mconfig_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperimentsPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimentName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mexperiment_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixExperimentParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mexperimentPathAndName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperimentsPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimentName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexperiment_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cifar-100-python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/HGNN/code/HGNN/HGNN/train/configParser.py\u001b[0m in \u001b[0;36mfixExperimentParams\u001b[0;34m(self, params_)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"phyloDistances\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"phyloDistances\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"phyloDistances\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"0.75,0.5,0.25\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"addKernelOrthogonality\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"addKernelOrthogonality\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"addKernelOrthogonality\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"useImbalancedSampling\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"useImbalancedSampling\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"useImbalancedSampling\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"useImbalancedCriterion\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"useImbalancedCriterion\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"useImbalancedCriterion\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/HGNN/code/HGNN/HGNN/train/configParser.py\u001b[0m in \u001b[0;36mcheck_valid\u001b[0;34m(params, key)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m      \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "# Get experiment parameters\n",
    "experimentsFileNameAndPath = os.path.join(experimentsPath, experimetnsFileName)\n",
    "if os.path.exists(experimentsFileNameAndPath):\n",
    "    experiments_df = pd.read_csv(experimentsFileNameAndPath)\n",
    "    experiments_df = experiments_df.where(pd.notnull(experiments_df), None)\n",
    "else:\n",
    "    raise Exception(\"Experiment not \" + trial_hash + \" found!\")\n",
    "experimentRecord = experiments_df[experiments_df[\"trialHash\"] == trial_hash]\n",
    "experiment_params = experimentRecord.to_dict('records')[0]\n",
    "print(experiment_params)\n",
    "\n",
    "config_parser = ConfigParser(experimentsPath, dataPath, experimentName)\n",
    "experiment_params = config_parser.fixExperimentParams(experiment_params)\n",
    "experimentPathAndName = os.path.join(experimentsPath, experimentName)\n",
    "if experiment_params['image_path'] == 'cifar-100-python':\n",
    "    datasetManager = cifar_dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "else:\n",
    "    datasetManager = dataLoader.datasetManager(experimentPathAndName, dataPath)\n",
    "datasetManager.updateParams(config_parser.fixPaths(experiment_params))\n",
    "train_loader, validation_loader, test_loader = datasetManager.getLoaders(SEED_INT)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(test_loader.dataset if dataset_name==\"test\" else train_loader.dataset, pin_memory=True, batch_size=batch_size, num_workers=num_workers)\n",
    "dataset = dataloader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# architecture = {\n",
    "#     \"fine\": len(dataset.csv_processor.getFineList()),\n",
    "#     \"coarse\" : len(dataset.csv_processor.getCoarseList())\n",
    "# }\n",
    "architecture = CNN.get_architecture(experiment_params, train_loader.dataset.csv_processor)\n",
    "model = CNN.create_model(architecture, experiment_params, cuda)\n",
    "\n",
    "# get the model and the parameters\n",
    "modelName = experimentRecord.iloc[0][\"modelName\"]\n",
    "trialName = os.path.join(experimentPathAndName, modelName)\n",
    "_ = CNN.loadModel(model, trialName)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show example and closest images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation_layer = 'layer2'#'fine'\n",
    "activation_layer ='01distance'\n",
    "\n",
    "model_sub = model\n",
    "if hasattr(model, 'network_fine') and use_submodel==True:\n",
    "    model_sub = model.network_fine\n",
    "\n",
    "get_tsne(dataloader, model_sub, activation_layer, experiment_params['img_res'], \n",
    "                os.path.join(experimentPathAndName, modelName), \n",
    "                dataset_name+\"_\"+activation_layer+(\"_submodule\" if use_submodel==True else \"\"), legends, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation_layer = 'layer4'#'fine'\n",
    "activation_layer ='03distance'\n",
    "\n",
    "get_tsne(dataloader, model_sub, activation_layer, experiment_params['img_res'], \n",
    "                os.path.join(experimentPathAndName, modelName), \n",
    "                dataset_name+\"_\"+activation_layer+(\"_submodule\" if use_submodel==True else \"\"), legends, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_layer ='05distance'\n",
    "\n",
    "get_tsne(dataloader, model_sub, activation_layer, experiment_params['img_res'], \n",
    "                os.path.join(experimentPathAndName, modelName), \n",
    "                dataset_name+\"_\"+activation_layer+(\"_submodule\" if use_submodel==True else \"\"), legends, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_layer = 'fine'#'fine'\n",
    "\n",
    "get_tsne(dataloader, model_sub, activation_layer, experiment_params['img_res'], \n",
    "                os.path.join(experimentPathAndName, modelName), \n",
    "                dataset_name+\"_\"+activation_layer+(\"_submodule\" if use_submodel==True else \"\"), legends, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_layer = 'gap_features'#'fine'\n",
    "\n",
    "get_tsne(dataloader, model_sub, activation_layer, experiment_params['img_res'], \n",
    "                os.path.join(experimentPathAndName, modelName), \n",
    "                dataset_name+\"_\"+activation_layer+(\"_submodule\" if use_submodel==True else \"\"), legends, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution and heat map analysis\n",
    "activation_layer = 'gap_features'#'gap_features'\n",
    "sub_vector_ratio = None # 0.25, 0.5,0.75,1.0 or None for default\n",
    "phylo_level = 1. #0.5, 0.7.0.9 1.0 # this should be changed with the number above when gap_features is used.\n",
    "\n",
    "# For heat map\n",
    "# vmax=None\n",
    "# vmin=None\n",
    "# ,vmax=vmax,vmin=vmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_keys(['input', 'gap_features', 'layer4_features', 'fine', '05distance', '03distance', '01distance'])\n",
    "# activation_layer ='gap_features'\n",
    "\n",
    "\n",
    "\n",
    "accumlated_features = None\n",
    "accumulated_labels= None\n",
    "accumulated_predictions= None\n",
    "a, n, _ = dataloader.dataset.toggle_image_loading(dataloader.dataset.augmentation_enabled, True)\n",
    "for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    image2 = batch['image'] \n",
    "    fine_label = batch['fine']\n",
    "    if cuda is not None:\n",
    "        image2 = image2.cuda()\n",
    "    activations = model.activations(image2)\n",
    "    features2 = activations[activation_layer].detach().cpu()\n",
    "    features2 = features2.reshape(features2.shape[0], -1)\n",
    "    pred, _ = CNN.getPredictions(activations['fine'], [])\n",
    "\n",
    "    # Calculate distance for each pair.\n",
    "    accumlated_features = features2 if accumlated_features is None else torch.cat([accumlated_features, features2]).detach()\n",
    "    accumulated_labels = fine_label.tolist() if accumulated_labels is None else accumulated_labels + fine_label.tolist()\n",
    "    accumulated_predictions = pred.tolist() if accumulated_predictions is None else accumulated_predictions + pred.tolist()\n",
    "dataloader.dataset.toggle_image_loading(a, n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.spatial as sp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "path_base = os.path.join(experimentPathAndName, 'models',trial_hash)\n",
    "\n",
    "\n",
    "if sub_vector_ratio is not None:\n",
    "    phylo_distance_cap = 1 - phylo_level\n",
    "    sub_vector= slice(int((sub_vector_ratio-0.25)*512), int(sub_vector_ratio*512))\n",
    "else:\n",
    "    phylo_distance_cap = 0.\n",
    "    sub_vector= slice(0, accumlated_features.shape[1])\n",
    "\n",
    "name_base = dataset_name,activation_layer,str(sub_vector)\n",
    "accumlated_features_sub = accumlated_features[:, sub_vector]\n",
    "\n",
    "x = accumlated_features_sub.reshape(1,-1)\n",
    "fig = plt.figure(0)\n",
    "plt.hist(x, bins=100, density=True, label=\"activations\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Activations\")\n",
    "plt.title(\"Distribution of values of activations\");\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'values')))\n",
    "\n",
    "fig = plt.figure(1)\n",
    "sqr = torch.sqrt(torch.sum(torch.pow(accumlated_features_sub, 2), 1).reshape(-1))\n",
    "plt.hist(sqr, density=True, bins=100, label=\"magnitude\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Magnitude\")\n",
    "plt.title(\"Magnitude of embeddings\");\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'magnitudes')))\n",
    "\n",
    "fig = plt.figure(2)\n",
    "sqrA = torch.sum(torch.pow(accumlated_features_sub, 2), 1, keepdim=True).expand(accumlated_features_sub.shape[0], x.shape[0])\n",
    "sqrB = torch.sum(torch.pow(accumlated_features_sub, 2), 1, keepdim=True).expand(accumlated_features_sub.shape[0], x.shape[0]).t()\n",
    "ans =  torch.nan_to_num(torch.sqrt(sqrA - 2*torch.mm(accumlated_features_sub, accumlated_features_sub.t()) + sqrB),0)\n",
    "ans2 = ans[torch.triu_indices(ans.shape[0],ans.shape[1])[0], torch.triu_indices(ans.shape[0],ans.shape[1])[1]]\n",
    "plt.hist(ans2, density=True, bins=100, label=\"distances\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.title(\"Distribution of distances between embeddings\");\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'distances')))\n",
    "distances_ = ans2\n",
    "\n",
    "fig = plt.figure(3)\n",
    "ax = sns.heatmap(ans)\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'distances_heat_map')))\n",
    "\n",
    "fig = fig = plt.figure(4)\n",
    "ans = sp.distance.cdist(accumlated_features_sub, accumlated_features_sub, 'cosine')\n",
    "ans2 = ans[torch.triu_indices(ans.shape[0],ans.shape[1])[0], torch.triu_indices(ans.shape[0],ans.shape[1])[1]]\n",
    "plt.hist(ans2, density=True, bins=100, label=\"cosine distance\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Cosine Distance\")\n",
    "plt.title(\"Distribution of cosine distance between embeddings\");\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base, 'cosine distance')))\n",
    "distances_normalized = ans\n",
    "\n",
    "fig = plt.figure(5)\n",
    "ax = sns.heatmap(ans)\n",
    "plt.title(\"Heat map of cosine distance between embeddings\");\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'cosine_distance_heat_map')))\n",
    "\n",
    "csv_processor = dataloader.dataset.csv_processor\n",
    "fig = fig = plt.figure(6)\n",
    "ans = torch.zeros(len(accumulated_labels),len(accumulated_labels))\n",
    "for indx, i in enumerate(accumulated_labels):\n",
    "    for indx2, j in enumerate(accumulated_labels[indx+1:]):\n",
    "#         print(csv_processor.getFineList()[i], csv_processor.getFineList()[j],csv_processor.tax.get_distance(csv_processor.getFineList()[i], csv_processor.getFineList()[j]))\n",
    "        dist = csv_processor.tax.get_distance(csv_processor.getFineList()[i], csv_processor.getFineList()[j])\n",
    "        ans[indx][indx2+1+indx] = ans[indx2+1+indx][indx] = dist\n",
    "        \n",
    "ans2 = ans[torch.triu_indices(ans.shape[0],ans.shape[1])[0], torch.triu_indices(ans.shape[0],ans.shape[1])[1]]\n",
    "ans = ans/torch.max(ans)\n",
    "plt.hist(ans2, density=True, bins=100, label=\"phylo distance\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Phylo distance\")\n",
    "plt.title(\"Distribution of phylo distance between embeddings\");\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'phylo_distance')))\n",
    "distances_phylo_ = ans2\n",
    "ans = torch.clip(ans, min = phylo_distance_cap)\n",
    "distances_phylo_normalized = ans\n",
    "\n",
    "fig = plt.figure(7)\n",
    "ax = sns.heatmap(ans)\n",
    "plt.title(\"Heat map of phylo distances between fishes\");\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'phylo_distance_heat_map'))\n",
    "\n",
    "fig = plt.figure(8)\n",
    "ax = sns.heatmap(abs(distances_phylo_normalized - distances_normalized))\n",
    "plt.title(\"Heat map of normalized distance difference\");\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base, 'normalized_distance_diff_heat_map'))\n",
    "\n",
    "\n",
    "len_species = csv_processor.getFineList()\n",
    "ans_phylo_dist = torch.zeros(len(len_species),len(len_species))\n",
    "ans_embedding_dist = torch.zeros(len(len_species),len(len_species))\n",
    "freq_phylo_dist = torch.zeros(len(len_species),len(len_species))\n",
    "for indx, i in enumerate(accumulated_labels):\n",
    "    for indx2, j in enumerate(accumulated_labels[indx+1:]):\n",
    "#         print(csv_processor.getFineList()[i], csv_processor.getFineList()[j],csv_processor.tax.get_distance(csv_processor.getFineList()[i], csv_processor.getFineList()[j]))\n",
    "        ans_phylo_dist[i][j] = ans_phylo_dist[j][i] = ans_phylo_dist[j][i]+distances_phylo_normalized[indx2+1+indx][indx]\n",
    "        freq_phylo_dist[i][j] = freq_phylo_dist[j][i] = freq_phylo_dist[j][i]+1\n",
    "        ans_embedding_dist[i][j] = ans_embedding_dist[j][i] = ans_embedding_dist[j][i]+distances_normalized[indx2+1+indx][indx]\n",
    "ans_phylo_dist = torch.div(ans_phylo_dist,freq_phylo_dist)\n",
    "ans_embedding_dist = torch.div(ans_embedding_dist,freq_phylo_dist)\n",
    "ans = abs(ans_phylo_dist - ans_embedding_dist)\n",
    "fig = plt.figure(9)\n",
    "ax = sns.heatmap(ans_phylo_dist)\n",
    "plt.title(\"Heat map of average phylo distances between fishes per species\");\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'avg_phylo_distance_per_species_heat_map'))\n",
    "fig = plt.figure(10)\n",
    "ax = sns.heatmap(ans_embedding_dist)\n",
    "plt.title(\"Heat map of average embedding distances between fishes per species\");\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'avg_embedding_distance_per_species_heat_map'))\n",
    "fig = plt.figure(11)\n",
    "ax = sns.heatmap(ans)\n",
    "plt.title(\"Heat map of error of average distances between fishes per species\");\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'avg_diff_distance_per_species_heat_map'))\n",
    "        \n",
    "        \n",
    "\n",
    "fig = plt.figure(12)\n",
    "cf_matrix = confusion_matrix(accumulated_labels, accumulated_predictions)\n",
    "ax = sns.heatmap(cf_matrix, cmap='Blues')\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumlated_features_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "nbrs = NearestNeighbors(n_neighbors=5, algorithm='brute', metric='cosine').fit(accumlated_features_sub)\n",
    "\n",
    "\n",
    "fig = plt.figure(13,figsize=(15,15))\n",
    "phylo_distance_per_example = []\n",
    "embedding_distance_per_example = []\n",
    "for indx, i in enumerate(range(accumlated_features_sub.shape[0])):\n",
    "    embedding_distances, indices = nbrs.kneighbors(accumlated_features_sub[indx, :].reshape(1, -1))\n",
    "    avg_embedding_distance = np.mean(embedding_distances)\n",
    "    embedding_distance_per_example.append(avg_embedding_distance)\n",
    "    \n",
    "    phylo_distances = distances_phylo_normalized[indx, indices]\n",
    "    avg_phylo_distance = torch.mean(phylo_distances).item()\n",
    "    phylo_distance_per_example.append(avg_phylo_distance)\n",
    "\n",
    "    \n",
    "X_axis = np.arange(accumlated_features_sub.shape[0])\n",
    "\n",
    "plt.bar(X_axis - 1, phylo_distance_per_example, 1, label = 'Phylo')\n",
    "plt.bar(X_axis , embedding_distance_per_example, 1, label = 'Embedding')\n",
    "  \n",
    "# plt.xticks(X_axis, X)\n",
    "plt.xlabel(\"Example\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.title(\"Average KNN distances of each example\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "from scipy.stats import pearsonr\n",
    "corr, _ = pearsonr(phylo_distance_per_example, embedding_distance_per_example)\n",
    "print('Pearsons correlation: %.3f' % corr)\n",
    "\n",
    "\n",
    "# tells how similar two distributions are (1 => very similar. 0=> not simialr)\n",
    "import scipy\n",
    "ks_stat, ks_p = scipy.stats.kstest(phylo_distance_per_example, embedding_distance_per_example)\n",
    "print('KS test: %.3f' % ks_stat) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sorted(sklearn.neighbors.VALID_METRICS['brute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr, _ = pearsonr(distances_phylo_, distances_)\n",
    "print('Pearsons correlation: %.3f' % corr)\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(os.path.join(path_base, 'phylo_embeddign_correlation_{}.csv'.format(name_base)), 'w+', newline='') as f:\n",
    "    thewriter = csv.writer(f)\n",
    "    thewriter.writerow([corr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#playing\n",
    "\n",
    "\n",
    "csv_processor = dataloader.dataset.csv_processor\n",
    "fig = fig = plt.figure(6)\n",
    "ans = torch.zeros(len(accumulated_labels),len(accumulated_labels))\n",
    "for indx, i in enumerate(accumulated_labels):\n",
    "    for indx2, j in enumerate(accumulated_labels[indx+1:]):\n",
    "#         print(csv_processor.getFineList()[i], csv_processor.getFineList()[j],csv_processor.tax.get_distance(csv_processor.getFineList()[i], csv_processor.getFineList()[j]))\n",
    "        dist = csv_processor.tax.get_distance(csv_processor.getFineList()[i], csv_processor.getFineList()[j])\n",
    "        ans[indx][indx2+1+indx] = ans[indx2+1+indx][indx] = dist\n",
    "        \n",
    "ans2 = ans[torch.triu_indices(ans.shape[0],ans.shape[1])[0], torch.triu_indices(ans.shape[0],ans.shape[1])[1]]\n",
    "ans = ans/torch.max(ans)\n",
    "plt.hist(ans2, density=True, bins=100, label=\"phylo distance\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Phylo distance\")\n",
    "plt.title(\"Distribution of phylo distance between embeddings\");\n",
    "fig.savefig(os.path.join(path_base, \"{}_{}.png\".format(name_base,'phylo_distance')))\n",
    "distances_phylo_ = ans2\n",
    "ans = torch.clip(ans, min = phylo_distance_cap)\n",
    "distances_phylo_normalized = ans\n",
    "\n",
    "fig = plt.figure(7)\n",
    "ax = sns.heatmap(ans)\n",
    "plt.title(\"Heat map of phylo distances between fishes\");\n",
    "plt.show()\n",
    "\n",
    "\n",
    "intervals = [0, int(0.25*512), int(0.5*512), int(0.75*512), 512]\n",
    "\n",
    "fig, axs = plt.subplots(2,4,figsize=(30,15))\n",
    "for i in range(len(intervals)-1):\n",
    "    accumlated_features_sub = accumlated_features[:, 0:intervals[i+1]]\n",
    "    \n",
    "    ax1 = axs[0, i]\n",
    "    ax2 = axs[1, i]\n",
    "    \n",
    "    ans = 1-sp.distance.cdist(accumlated_features_sub, accumlated_features_sub, 'cosine')\n",
    "    ans2 = ans[torch.triu_indices(ans.shape[0],ans.shape[1])[0], torch.triu_indices(ans.shape[0],ans.shape[1])[1]]\n",
    "    ax1.hist(ans2, density=True, bins=100, label=\"cosine distance\")\n",
    "    ax1.set_ylabel(\"Probability\")\n",
    "    ax1.set_xlabel(\"Cosine Distance\")\n",
    "    ax1.set_title(\"Distribution of cosine distance between embeddings\");\n",
    "\n",
    "    ax_ = sns.heatmap(ans, ax = ax2)\n",
    "    ax2.set_title(\"Heat map of cosine distance between embeddings\");\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
